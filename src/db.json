{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"source/assets/images/favicon.jpg","path":"assets/images/favicon.jpg","modified":0,"renderable":0}],"Cache":[{"_id":"source/_posts/Angular-4-Custom-Bootstrapping-Lazy-Bind-to-Designated-Container.md","hash":"4e5c18e35bbbab1f410459f8815b709a4ead0331","modified":1756765611285},{"_id":"source/_posts/Building-Linux-Workspace-on-Windows-10-via-WSL.md","hash":"95b369bd7ea2594276a7e7889923dd55f5ad681a","modified":1756765611285},{"_id":"source/_posts/It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO.md","hash":"4f482ecaa2ec0bff5455786ae3f2d645d2af5ff1","modified":1756765611286},{"_id":"source/_posts/Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul.md","hash":"b2c5c7ab0c703714ab47fb5664ec3d96817d9ac8","modified":1756765611287},{"_id":"source/_posts/PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-1-DHCP-Setup.md","hash":"91f24611de32ea6edd43bf40d90a462ab608cd93","modified":1756765611290},{"_id":"source/_posts/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray.md","hash":"e075bbea730a456f428aa9caf5461266c4b1f824","modified":1756765611289},{"_id":"source/_posts/PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-2-Config-TFTP-and-NFS-mounts.md","hash":"0aa613906fb7e719c48b7d8a901f8ca6574ae11d","modified":1756765611291},{"_id":"source/_posts/Why-you-should-ditch-browserify-and-commonjs-in-the-http-2-world.md","hash":"4f268134218ec3c653087804660a71e8e4d03eb4","modified":1756765611292},{"_id":"source/_posts/Why-Keeping-High-Standard-of-Shit-Is-Important.md","hash":"ff7771482253a6aa4fae490507e0e92422fbb736","modified":1756765611292},{"_id":"source/_posts/You-Don-t-Know-JS-Equal-or-Not-Equal.md","hash":"93327f57cc5361683bb4f574b839d9b29c44f790","modified":1756765611293},{"_id":"source/_posts/Workflow-Processing-Engine-Overview-2018-Airflow-vs-Azkaban-vs-Conductor-vs-Oozie-vs-Amazon-Step-Functions.md","hash":"4c66735a34da0939f88c506d7b1352b2a84a4423","modified":1756765611293},{"_id":"source/_drafts/Implementing-Regex-from-Scratch-1-The-Basics-of-Regex-and-Finite-Automata.md","hash":"3400addd205f853a8e724ad2bced147076466552","modified":1756765611284},{"_id":"source/_posts/get-started-with-hexo-blogging-system.md","hash":"ff2b1d61c87d06f5c1a27cd65ab54dbe66df5976","modified":1756765611294},{"_id":"source/all-categories/index.md","hash":"3d47d4e31035e3c8d960730f36bd3ad27f88e9be","modified":1756765611294},{"_id":"source/_posts/You-Don-t-Know-JS-Eval.md","hash":"7c662af494adc0555a9710fa0073a2b519008baf","modified":1756765611294},{"_id":"source/_posts/Building-Linux-Workspace-on-Windows-10-via-WSL/title.png","hash":"340a6d9c7bb6d45ec73cf4cb8df358cc4d0a1971","modified":1756765611286},{"_id":"source/all-archives/index.md","hash":"634c7e0b6841e3cdbb53b62fc5db80e1ad3a69f2","modified":1756765611294},{"_id":"source/all-tags/index.md","hash":"a9092fa374dd3f5b8b5c72f02fa0b128309f91ed","modified":1756765611295},{"_id":"source/_posts/It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO/non_zero_copy.png","hash":"727faecc1738291c60c4edfc857f20038ccf16e8","modified":1756765611287},{"_id":"source/_posts/It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO/mmap.png","hash":"2072f0ec0937bc3e140f8d53aeeeb86c91293658","modified":1756765611286},{"_id":"source/_posts/Building-Linux-Workspace-on-Windows-10-via-WSL/WSL.png","hash":"410f4bcbff3af97ae775ec3fd681b3d16d748473","modified":1756765611286},{"_id":"source/_posts/Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul/slowspeed.png","hash":"27641cd0bfbcd9f15f50acc7f83a26c6b8be5c7e","modified":1756765611288},{"_id":"source/_posts/Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul/speed.png","hash":"aec84c6fcf8764f264106ae31a7d1c9c5c4f8640","modified":1756765611288},{"_id":"source/_posts/It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO/scattergather.png","hash":"2d9cceb8ecc60e6c95bc7866e9fdc45ba90b212d","modified":1756765611287},{"_id":"source/_posts/It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO/zero_copy.png","hash":"b4e91d0123265db5d08a63e5219a510787a0e583","modified":1756765611287},{"_id":"source/_posts/Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul/telnet.png","hash":"83325fdd1fe9ca986a8b1a798585d0430e6fdf19","modified":1756765611288},{"_id":"source/_posts/Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul/RBK20.png","hash":"ba685949c5be97a1a3b44d740bdcca5770ed2b53","modified":1756765611288},{"_id":"source/_posts/PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-1-DHCP-Setup/net-topo.png","hash":"38010d7fe8501dd625804abd57be4c7272410f94","modified":1756765611291},{"_id":"source/_posts/PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-2-Config-TFTP-and-NFS-mounts/nfs-permission.png","hash":"b69313163397a0cbb021847f4be2e2c2025eebed","modified":1756765611291},{"_id":"source/assets/images/favicon.jpg","hash":"ed95ea56e130cfa08bb037abda44f7ca180971da","modified":1756765611295},{"_id":"source/_posts/PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-2-Config-TFTP-and-NFS-mounts/rainbow.jpg","hash":"ff457603b30a637eb7ca187685f7fbaa0e569278","modified":1756765611292},{"_id":"source/_drafts/Implementing-Regex-from-Scratch-1-The-Basics-of-Regex-and-Finite-Automata/sm.png","hash":"3ace0afce14335dad540b26c2f1066db41d0c9ca","modified":1756765611284},{"_id":"source/_drafts/Implementing-Regex-from-Scratch-1-The-Basics-of-Regex-and-Finite-Automata/example2.png","hash":"3e7d3e5e8a5040c4049f2bb39967a28d4a8bc2c2","modified":1756765611284},{"_id":"source/_posts/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray/tracing.png","hash":"f0f6f4e0dd32f1c4db3610863462c0a211714643","modified":1756765611290},{"_id":"source/_posts/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray/ms-failure.png","hash":"1605814da4df84527228b1c9921e4c6c07e24c88","modified":1756765611289},{"_id":"source/_posts/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray/xray.png","hash":"999302021354473cf2b135c0aee1319b68527896","modified":1756765611290},{"_id":"source/_posts/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray/iam.png","hash":"204c08fbf12d283c4693475746b42687f013adda","modified":1756765611289},{"_id":"source/_posts/Why-you-should-ditch-browserify-and-commonjs-in-the-http-2-world/comparison.gif","hash":"e84c1126667d35de343b64b7c0bfff9b01a10022","modified":1756765611293},{"_id":"themes/tranquilpeak/.gitignore","hash":"19e27dd778896662e6d604b69411d2dc3cf5fec6","modified":1756765611315},{"_id":"themes/tranquilpeak/.eslintignore","hash":"1f78f00553adf9ee374b343191ed809b0f8ba073","modified":1756765611313},{"_id":"themes/tranquilpeak/.eslintrc","hash":"ea1da9c0b0863a612ace47bd336cff95497bd878","modified":1756765611313},{"_id":"themes/tranquilpeak/DOCUMENTATION.md","hash":"daaa36cff0cf7aeb7682372d6697a55d6a0f2f5b","modified":1756765611315},{"_id":"themes/tranquilpeak/_config.yml","hash":"115dd6648606d9f64e4d34b7f1f7f6f9cecb31d6","modified":1756766618595},{"_id":"themes/tranquilpeak/Gruntfile.js","hash":"b2b078651cf8c5dcde2e847169ce242ad3319ca3","modified":1756765611315},{"_id":"themes/tranquilpeak/README.md","hash":"c18f2fea9ceeec6efa85d892df67a191d6c84ec5","modified":1756765611316},{"_id":"themes/tranquilpeak/package.json","hash":"8a2f463f5e9813f1c8d97228029543946325be32","modified":1756765611327},{"_id":"themes/tranquilpeak/.github/CONTRIBUTING.md","hash":"8873700e439b6fc8aa4804b70a90fd81bf310d6c","modified":1756765611314},{"_id":"themes/tranquilpeak/LICENSE","hash":"7a3b30ddb56760b35a6b7cd210aeb271cd292b60","modified":1756765611316},{"_id":"themes/tranquilpeak/.github/PULL_REQUEST_TEMPLATE.md","hash":"03cdb93ca72cd6a19043ea4bb1ec4f5b7479a0a9","modified":1756765611315},{"_id":"themes/tranquilpeak/renovate.json","hash":"a6e9bb0219577e715e333dabb0984b019728f596","modified":1756765611327},{"_id":"themes/tranquilpeak/.github/ISSUE_TEMPLATE.md","hash":"ec6bf3eb708f8eb62736ede6d2080f6610849802","modified":1756765611314},{"_id":"themes/tranquilpeak/layout/all-tags.ejs","hash":"aabb765f51a8d7582a5bafdbc8876b46ff1e6ea1","modified":1756765611325},{"_id":"themes/tranquilpeak/layout/all-archives.ejs","hash":"28a4e97d1cfe4730dbcc8fde27b079320b5eec35","modified":1756765611324},{"_id":"themes/tranquilpeak/layout/all-categories.ejs","hash":"c2257b8265a9a328e69c92f4a88cb2cdd99c047a","modified":1756765611325},{"_id":"themes/tranquilpeak/layout/layout.ejs","hash":"29c1291c7ca3f5da7bee6385b7f716d179199933","modified":1756765611325},{"_id":"themes/tranquilpeak/layout/index.ejs","hash":"36e89b37f520533bef85fb32b714214257fad1d0","modified":1756765611325},{"_id":"themes/tranquilpeak/layout/category.ejs","hash":"67902c418e299399133fe2a7ebc39cadd17de861","modified":1756765611325},{"_id":"themes/tranquilpeak/layout/archive.ejs","hash":"0df1edf4ed40b316715ef1949b09aa9c98d22359","modified":1756765611325},{"_id":"themes/tranquilpeak/layout/page.ejs","hash":"ae761feddc98f4d3260e7591ed9fc91d9a49d0ab","modified":1756765611326},{"_id":"themes/tranquilpeak/layout/post.ejs","hash":"ae761feddc98f4d3260e7591ed9fc91d9a49d0ab","modified":1756765611326},{"_id":"themes/tranquilpeak/languages/de-DE.yml","hash":"c9a7514af3c5c58023f8d3ed9503be4253146015","modified":1756765611316},{"_id":"themes/tranquilpeak/languages/fa.yml","hash":"ec86c6fd655ed0c6df017c17f62b784b916d3a4a","modified":1756765611317},{"_id":"themes/tranquilpeak/languages/it-IT.yml","hash":"a5655d08a311f3292df5805ea2d6d72fd68726ca","modified":1756765611317},{"_id":"themes/tranquilpeak/languages/es.yml","hash":"b5f37df87377d985339a21f343bf54db46926f1a","modified":1756765611317},{"_id":"themes/tranquilpeak/layout/tag.ejs","hash":"72bff1ae241fbe2ddca0537e8ed77c791a7d1766","modified":1756765611326},{"_id":"themes/tranquilpeak/languages/fr-FR.yml","hash":"c644bb587a5360bb70a12c2d83f8d5973fb0055c","modified":1756765611317},{"_id":"themes/tranquilpeak/languages/ja.yml","hash":"3c6f7cda21ac91abc9f728e3093b1e98b42d5295","modified":1756765611317},{"_id":"themes/tranquilpeak/languages/en.yml","hash":"35fbd79df7271b963d24c19bd7567bfe722918ad","modified":1756767096340},{"_id":"themes/tranquilpeak/languages/pt-br.yml","hash":"e2edf45fae5a5290a3dc2111ef38d5cd40d43771","modified":1756765611318},{"_id":"themes/tranquilpeak/languages/ru.yml","hash":"262f781591cf2dd1584411ed4a780369d4d2e30a","modified":1756765611318},{"_id":"themes/tranquilpeak/languages/ko.yml","hash":"b56376775c5bc7f8f1d1b356340597cc61aae392","modified":1756765611318},{"_id":"themes/tranquilpeak/languages/zh-cn.yml","hash":"fea995175f5aeb58f8a958b00556bc2336d37a07","modified":1756765611318},{"_id":"themes/tranquilpeak/scripts/.eslintrc.json","hash":"4cd7451f0bc15440bc8147912c63a11acd0e280f","modified":1756765611327},{"_id":"themes/tranquilpeak/languages/zh-tw.yml","hash":"f04e0e91edacb8f6cae7308007bd5e5158e6c77e","modified":1756765611319},{"_id":"themes/tranquilpeak/.github/ISSUE_TEMPLATE/bug-report.md","hash":"b9a9ffe556f1fca39009403f87bb8c8e840dc369","modified":1756765611314},{"_id":"themes/tranquilpeak/.github/ISSUE_TEMPLATE/feature-request---improvement.md","hash":"7d53b0c0d65dd28da17ff9d948e842883cab467f","modified":1756765611314},{"_id":"themes/tranquilpeak/tasks/pipeline.js","hash":"75b65cbd6d6ae471739ba80c9f792f602a0769d4","modified":1756765611351},{"_id":"themes/tranquilpeak/.github/ISSUE_TEMPLATE/question.md","hash":"443c4f27268d39e89390edb7dcffda407a070fd7","modified":1756765611314},{"_id":"themes/tranquilpeak/layout/_partial/about.ejs","hash":"c07cc11de098e81803edef47a2e98023571765ce","modified":1756767089575},{"_id":"themes/tranquilpeak/.github/workflows/nodejs.yml","hash":"595c32bcb8ba72cd0737c6009f24dabda12af834","modified":1756765611315},{"_id":"themes/tranquilpeak/layout/_partial/archive-post.ejs","hash":"e19455749c5c1bf773214d965f173613dd8f30f6","modified":1756765611319},{"_id":"themes/tranquilpeak/layout/_partial/footer.ejs","hash":"67962257d9e3cb6a4d56e3f0263f192ba9a413dd","modified":1756770663747},{"_id":"themes/tranquilpeak/layout/_partial/archive.ejs","hash":"bdd73d1f70bd91533863943f940f8193d44e0fdd","modified":1756765611319},{"_id":"themes/tranquilpeak/layout/_partial/cover.ejs","hash":"18b9a620fca9b648fd59467a66b3f4fdbba42613","modified":1756765611320},{"_id":"themes/tranquilpeak/layout/_partial/baidu-analytics.ejs","hash":"736eca2ecf6fdeea032d47c02c51688f006cafb4","modified":1756765611320},{"_id":"themes/tranquilpeak/layout/_partial/post.ejs","hash":"cdccf8f0ea1c01997fafdabb7cf044d3939cbb6c","modified":1756765611321},{"_id":"themes/tranquilpeak/layout/_partial/pagination.ejs","hash":"89d3be71ff2f5a847850d50e117896056f0ca1a6","modified":1756765611321},{"_id":"themes/tranquilpeak/layout/_partial/index.ejs","hash":"34d9260021e4b8b4bf232c2786d8e90e0db5bd20","modified":1756765611321},{"_id":"themes/tranquilpeak/layout/_partial/google-analytics.ejs","hash":"b756aa731678a37d22c97c7dc30767331b08d109","modified":1756765611320},{"_id":"themes/tranquilpeak/layout/_partial/search.ejs","hash":"0fd5f75117912cd1e6e7e6094079bccdc79c0000","modified":1756765611324},{"_id":"themes/tranquilpeak/layout/_partial/script.ejs","hash":"15ff7f6bff613b2db87f7e78ac98023e87bc0e5b","modified":1756765611324},{"_id":"themes/tranquilpeak/scripts/filters/excerpt.js","hash":"5e0e473d46f8c7aac988ebf0b25ab2eab0af73bb","modified":1756765611327},{"_id":"themes/tranquilpeak/layout/_partial/sidebar.ejs","hash":"39fb0c9f214cc36f4f96cbcacc832e542d723881","modified":1756765611324},{"_id":"themes/tranquilpeak/scripts/filters/thumbnail_image_url.js","hash":"0ad2d9304c991e2989708bdbfa6a59030ee691d3","modified":1756765611328},{"_id":"themes/tranquilpeak/scripts/tags/alert.js","hash":"5b86358dd53c9b0d44c385ec12cf9e76383e229a","modified":1756765611329},{"_id":"themes/tranquilpeak/layout/_partial/header.ejs","hash":"d6cbea4a9d3ae684d6afd5dfe40c02d8fd676f6e","modified":1756771332817},{"_id":"themes/tranquilpeak/scripts/tags/fancybox.js","hash":"55ce02f9ef084f43932eb9b0f10db1d5e9c3a250","modified":1756765611329},{"_id":"themes/tranquilpeak/layout/_partial/head.ejs","hash":"761536646720584acf6e70bd4b0a5fdba0e7522f","modified":1756765611320},{"_id":"themes/tranquilpeak/scripts/tags/video.js","hash":"497d64fd454cc6fd5c26a42cb8e38db90b83cf4f","modified":1756765611330},{"_id":"themes/tranquilpeak/scripts/tags/image.js","hash":"ab6b04db8fe2ad21ec52dac5e1c3ee76400c6a79","modified":1756765611330},{"_id":"themes/tranquilpeak/scripts/tags/tabbed_codeblock.js","hash":"7dccd36bdd85c96eb7af125b30d634f44125700c","modified":1756765611330},{"_id":"themes/tranquilpeak/scripts/tags/wide_image.js","hash":"d3596c62694548a3a684e8dfeb5372038ef6f450","modified":1756765611330},{"_id":"themes/tranquilpeak/scripts/tags/highlight_text.js","hash":"8e093f21e9a4b10c19be6f300dc90bcc39685ec9","modified":1756765611330},{"_id":"themes/tranquilpeak/scripts/helpers/generate_sharing_link.js","hash":"89069677e8acddef5fba0d3093f5f73fde4fa485","modified":1756765611328},{"_id":"themes/tranquilpeak/scripts/helpers/absolute_url.js","hash":"0d520a946b6f0abbac60529017ed3133265aac60","modified":1756765611328},{"_id":"themes/tranquilpeak/scripts/helpers/resolve_asset_url.js","hash":"c813e9daaed374b386b91876923f10c0cf1d624c","modified":1756765611329},{"_id":"themes/tranquilpeak/scripts/helpers/json_ld.js","hash":"4cd07766ed9436f0c4cb8a9ae22644272918b837","modified":1756765611329},{"_id":"themes/tranquilpeak/scripts/helpers/is_remote_url.js","hash":"471237ea295bcc9a392a5bb6738ef4c6ab673afb","modified":1756765611328},{"_id":"themes/tranquilpeak/source/_css/tranquilpeak.scss","hash":"e69c62b19ece4ab35d992e163b47f2a8374d7611","modified":1756765611339},{"_id":"themes/tranquilpeak/source/_fonts/merriweather-bold-italic.ttf","hash":"c1b8593dc9ddbf1a1a6268d31e08c4041448ed76","modified":1756765611342},{"_id":"themes/tranquilpeak/source/_fonts/merriweather-light.ttf","hash":"69bd3f15962cf91348a133d3d38571a5c67eca65","modified":1756765611343},{"_id":"themes/tranquilpeak/source/_fonts/merriweather-light-italic.ttf","hash":"c3835119c62e78a21cfb33e2de6686fb4a5929a0","modified":1756765611343},{"_id":"themes/tranquilpeak/source/_fonts/merriweather.ttf","hash":"d392237948e74246a2fb0d0d886498776a185e54","modified":1756765611343},{"_id":"themes/tranquilpeak/source/_fonts/open-sans-bold.ttf","hash":"3823779db9f51a34658809ce66bdd6ed1145ed8e","modified":1756765611343},{"_id":"themes/tranquilpeak/source/_images/logo-algolia-nebula-blue-full.svg","hash":"d5e4426b6bd2d8b8c3f477f83d348d8f3b3ec1ca","modified":1756765611346},{"_id":"themes/tranquilpeak/source/_fonts/merriweather-bold.ttf","hash":"e6d0ffb2d0348d9e22de97e57214b7db795e9513","modified":1756765611342},{"_id":"themes/tranquilpeak/source/_fonts/open-sans.ttf","hash":"618b78a8f6ffe7e1998eab67268859e2ab54be8e","modified":1756765611343},{"_id":"themes/tranquilpeak/source/_js/.eslintrc.json","hash":"27d4e7223eee3eca7eb717bea2d28d35714eccbc","modified":1756765611347},{"_id":"themes/tranquilpeak/source/_js/codeblock-resizer.js","hash":"391c2c9135579ea50c2060e0612e7105e06a8ff7","modified":1756765611347},{"_id":"themes/tranquilpeak/source/_js/archives-filter.js","hash":"8b8998b0e67c745a20450001c0739f6977b63309","modified":1756765611347},{"_id":"themes/tranquilpeak/source/_js/about.js","hash":"ce68bcc929192dfca6e699fe8e528990cd2d9590","modified":1756765611347},{"_id":"themes/tranquilpeak/source/_js/header.js","hash":"58ca619f851ebff35724c07dca7f1ed25e9cd5f2","modified":1756765611348},{"_id":"themes/tranquilpeak/source/_js/categories-filter.js","hash":"a5f3c4b9bbef1c8ec881eb39551060ca95bfbb33","modified":1756765611347},{"_id":"themes/tranquilpeak/source/_js/fancybox.js","hash":"659aaad375ecff9748a5d3b0ff1378a549236513","modified":1756765611347},{"_id":"themes/tranquilpeak/source/_js/image-gallery.js","hash":"90efc47246af69b7e17fab6e0bd1578043fcc2b0","modified":1756765611348},{"_id":"themes/tranquilpeak/source/_js/post-bottom-bar.js","hash":"64f78c44d29d3559d5e14e436b8cfe1d24b8c965","modified":1756765611348},{"_id":"themes/tranquilpeak/source/_js/search-modal.js","hash":"7ea9ab668bc3118c28e6212516f366ef764d3550","modified":1756765611348},{"_id":"themes/tranquilpeak/source/_js/share-options.js","hash":"c2046697540786721ff7c4754774f94fcaf32696","modified":1756765611348},{"_id":"themes/tranquilpeak/source/_js/smartresize.js","hash":"7342a21f94d7bb0525ccbd917d43a6824d7ae63e","modified":1756765611348},{"_id":"themes/tranquilpeak/source/_js/sidebar.js","hash":"18dc623dd237c519bb79ce93283a5446cc11a487","modified":1756765611348},{"_id":"themes/tranquilpeak/source/_js/tabbed-codeblocks.js","hash":"1282e11aacb036c8c9310cac49b75d7e2200125c","modified":1756765611349},{"_id":"themes/tranquilpeak/source/_js/tags-filter.js","hash":"d33625df51d7b9788eeea338626f04387c96f053","modified":1756765611349},{"_id":"themes/tranquilpeak/tasks/register/build.js","hash":"f64c77127a2d4bfdbb9866fcf368913334f44603","modified":1756765611351},{"_id":"themes/tranquilpeak/tasks/register/buildProd.js","hash":"450ff77a1bf631d1799206d218c6468592a59862","modified":1756765611351},{"_id":"themes/tranquilpeak/tasks/register/default.js","hash":"80174db9b8a729980770e639e70999ce5a816b78","modified":1756765611352},{"_id":"themes/tranquilpeak/tasks/register/eslint.js","hash":"718cd8a80d060e2587b84e0e20ceaa39c2cc9411","modified":1756765611352},{"_id":"themes/tranquilpeak/tasks/register/compileAssets.js","hash":"a65122ba0d1a1b6044e3b635a644882577b628ba","modified":1756765611352},{"_id":"themes/tranquilpeak/tasks/register/linkAssets.js","hash":"079cd274cb29f9984eb7150d88f61819d2ef6235","modified":1756765611352},{"_id":"themes/tranquilpeak/tasks/config/clean.js","hash":"ea70743eb4ed0d9ead68d6fc8b65b9f936229a00","modified":1756765611349},{"_id":"themes/tranquilpeak/tasks/register/linkAssetsProd.js","hash":"37f2fab80f25757be3e34b40bc5a26a82f088491","modified":1756765611352},{"_id":"themes/tranquilpeak/tasks/register/syncAssets.js","hash":"0813093c3e252a58371db53e914183a97e3186a1","modified":1756765611353},{"_id":"themes/tranquilpeak/tasks/config/concat.js","hash":"0c2c4dbbfac52f20a3035a2f8cd1c24f1759b17e","modified":1756765611349},{"_id":"themes/tranquilpeak/tasks/config/copy.js","hash":"4b9a4e724c65ec71cfec36b387a2625fc0f1074c","modified":1756765611350},{"_id":"themes/tranquilpeak/tasks/config/cssmin.js","hash":"352185864437728c9c19169f4fec6fc3308e8b6d","modified":1756765611350},{"_id":"themes/tranquilpeak/tasks/config/replace.js","hash":"984ba7ed966c16b1fadf47c62f76e99940d4b12d","modified":1756765611350},{"_id":"themes/tranquilpeak/tasks/config/sails-linker.js","hash":"c1103119ec7d1f04cfbb1cda83b40d11e8f2d218","modified":1756765611350},{"_id":"themes/tranquilpeak/tasks/config/sass.js","hash":"84ba6be74f99c502543b7f77a105ce16931fd38e","modified":1756765611350},{"_id":"themes/tranquilpeak/tasks/config/sync.js","hash":"fbc09d0ed201993bc1918223b8f8ad5b68f50490","modified":1756765611350},{"_id":"themes/tranquilpeak/tasks/config/watch.js","hash":"933eecd8a2506c627dbb9a1137305a16dd2d9f9a","modified":1756765611351},{"_id":"themes/tranquilpeak/tasks/config/uglify.js","hash":"57c7341a004b61539f5b3f02bd772f40494b8306","modified":1756765611351},{"_id":"themes/tranquilpeak/layout/_partial/post/category.ejs","hash":"d4b26aa8c66aa8dbfae7a4aafed874745efb2f8e","modified":1756765611322},{"_id":"themes/tranquilpeak/layout/_partial/post/actions.ejs","hash":"419223bff6146c46d4e993e4aa2e3636b051edc8","modified":1756765611321},{"_id":"themes/tranquilpeak/layout/_partial/post/disqus.ejs","hash":"9e0d238ed918e3138d5a9bb23a90408669e19169","modified":1756765611322},{"_id":"themes/tranquilpeak/layout/_partial/post/duoshuo.ejs","hash":"992368f8863f34d1dccf90c0cabc33d5dcdbe92a","modified":1756765611322},{"_id":"themes/tranquilpeak/layout/_partial/post/gallery.ejs","hash":"71b4ad77ab08c0980bf85b85c98a90c7de2be107","modified":1756765611322},{"_id":"themes/tranquilpeak/layout/_partial/post/gitalk.ejs","hash":"8c91c60ffeab780a8a628145c572456187142101","modified":1756765611323},{"_id":"themes/tranquilpeak/layout/_partial/post/gitment.ejs","hash":"1dcfdca4d20bab5a43bda44cf37df506ab3a0fb7","modified":1756765611323},{"_id":"themes/tranquilpeak/layout/_partial/post/header-cover.ejs","hash":"4d3a73d0c6f26769f778d02a559e5055ef1ee0ca","modified":1756765611323},{"_id":"themes/tranquilpeak/layout/_partial/post/header.ejs","hash":"19c5f0d680db08fe2880f41de3a085ef0298a386","modified":1756765611323},{"_id":"themes/tranquilpeak/layout/_partial/post/meta.ejs","hash":"f13cb5089b0d5898c9606519d1c872725a63a5f9","modified":1756765611323},{"_id":"themes/tranquilpeak/layout/_partial/post/share-options.ejs","hash":"896158d62beba2ecb7d6af6e0f01f8f1ea23e706","modified":1756765611323},{"_id":"themes/tranquilpeak/layout/_partial/post/tag.ejs","hash":"2a52f38fa359f8ec8ae776e44e92c48795e31272","modified":1756765611324},{"_id":"themes/tranquilpeak/source/_css/base/_base.scss","hash":"73210a9ecf0e5b3c8c5c2052a222209aa596dfbc","modified":1756765611331},{"_id":"themes/tranquilpeak/source/_css/components/_alert.scss","hash":"1e6bf40de34a341dd0228357e288153b9292977c","modified":1756765611331},{"_id":"themes/tranquilpeak/source/_css/components/_button.scss","hash":"ea9bd41800fe04860f7e892c17010588b2563a06","modified":1756765611332},{"_id":"themes/tranquilpeak/source/_css/components/_archive.scss","hash":"cfdf53475d228643d5c5e62689b3c6a593e4ccce","modified":1756765611331},{"_id":"themes/tranquilpeak/source/_css/components/_caption.scss","hash":"dad2683f5f3235b335eebcbda3db59a1cfbb2283","modified":1756765611332},{"_id":"themes/tranquilpeak/source/_css/components/_box.scss","hash":"eebaacf55234fdaa156edb7dc4d9a4f382eaf4d4","modified":1756765611331},{"_id":"themes/tranquilpeak/source/_css/components/_code.scss","hash":"0a14fe197c221a4d4d91ee9d41541ce7969043ca","modified":1756765611332},{"_id":"themes/tranquilpeak/source/_css/components/_fancybox.scss","hash":"e162e2ecafc2be70238a9885fd57a9a7ad642757","modified":1756765611332},{"_id":"themes/tranquilpeak/source/_css/components/_form.scss","hash":"368187e0c5f1a57d8c423f2ec23e897ee94a1a27","modified":1756765611332},{"_id":"themes/tranquilpeak/source/_css/components/_figure.scss","hash":"9828521c34d91f04813db23e174708f510fe30bf","modified":1756765611332},{"_id":"themes/tranquilpeak/source/_css/components/_hide.scss","hash":"c2f5ed51086b9ea5d94ff548bdc4653ea5cd7912","modified":1756765611333},{"_id":"themes/tranquilpeak/source/_css/components/_highlight-text.scss","hash":"7a7a8a78f75d6c43a46fd020e2d44546e716751b","modified":1756765611333},{"_id":"themes/tranquilpeak/source/_css/components/_icon.scss","hash":"60dc972bcd81b4f95ab2c067ca910a5bf324d907","modified":1756765611333},{"_id":"themes/tranquilpeak/source/_css/components/_image-gallery.scss","hash":"95ee5be04c523bf914596042574780ee527e6b38","modified":1756765611333},{"_id":"themes/tranquilpeak/source/_css/components/_link.scss","hash":"7460155ebf7ce0d3e9acd090b6b99673127fd0be","modified":1756765611333},{"_id":"themes/tranquilpeak/source/_css/components/_main-content.scss","hash":"90c46c87289a8b3cf7c14d36900f5ced9db36c25","modified":1756765611333},{"_id":"themes/tranquilpeak/source/_css/components/_markdown.scss","hash":"ba6d7e8f84b3b6dfef339f28b5c6ae372ead674f","modified":1756765611334},{"_id":"themes/tranquilpeak/source/_css/components/_media.scss","hash":"84aa03c01b741bbeb31c6b48122930f30ad40de3","modified":1756765611334},{"_id":"themes/tranquilpeak/source/_css/components/_modal.scss","hash":"bc56f823cd67b2ed65b5cf2c3e1851fa3e82f0f9","modified":1756765611334},{"_id":"themes/tranquilpeak/source/_css/components/_pagination.scss","hash":"809b9330a0e2d79c9285c1a452d61648248c8b70","modified":1756765611334},{"_id":"themes/tranquilpeak/source/_css/components/_post-actions.scss","hash":"1cc3ba46b1752043207bb820f4f886b0f982e445","modified":1756765611335},{"_id":"themes/tranquilpeak/source/_css/components/_post.scss","hash":"0457c281b7893f8fde8a0590e4c1ff2ffb4bc1a6","modified":1756765611335},{"_id":"themes/tranquilpeak/source/_css/components/_post-header-cover.scss","hash":"c4bf950602ccb3fbdad54938979651a7b488236e","modified":1756765611335},{"_id":"themes/tranquilpeak/source/_css/components/_pullquote.scss","hash":"86bc35ba358d1340debc459a344f5f6d34acaa42","modified":1756765611336},{"_id":"themes/tranquilpeak/source/_css/components/_postShorten.scss","hash":"a1b1ab16751fb99a3937cc0797426c658a2921f7","modified":1756765611336},{"_id":"themes/tranquilpeak/source/_css/components/_share-options-bar.scss","hash":"f6289a7cce6efe039614c77085945923c32d9673","modified":1756765611336},{"_id":"themes/tranquilpeak/source/_css/components/_tag.scss","hash":"ddd1c2ce17f0116655bdacda598e7f1d6fd18262","modified":1756765611336},{"_id":"themes/tranquilpeak/source/_css/components/_text.scss","hash":"7b38858248da2f73cf64c5949fd475a8e0a246ae","modified":1756765611336},{"_id":"themes/tranquilpeak/source/_css/components/_video.scss","hash":"f9727fd300d73e8844abf0e2575c45e6c7769333","modified":1756765611337},{"_id":"themes/tranquilpeak/source/_css/layouts/_about.scss","hash":"4a1c4cca6623449602208256dee6422173daf1ef","modified":1756765611337},{"_id":"themes/tranquilpeak/source/_css/layouts/_bottom-bar.scss","hash":"c7816c7ebc253d46466355860ead6669a3707ac6","modified":1756765611337},{"_id":"themes/tranquilpeak/source/_css/components/_tooltip.scss","hash":"f33ad3e677761af1ceb42d337691b8b354ff70bd","modified":1756765611336},{"_id":"themes/tranquilpeak/source/_css/layouts/_blog.scss","hash":"2450e07594a5e5186170fb5b5b157cc340808a9a","modified":1756765611337},{"_id":"themes/tranquilpeak/source/_css/layouts/_cover.scss","hash":"03a951c9b35dd4e35456caa36950e6d6bef77ac3","modified":1756765611337},{"_id":"themes/tranquilpeak/source/_css/layouts/_main.scss","hash":"ee4648502b355db51fc6cb7b05375003331f2f6e","modified":1756765611338},{"_id":"themes/tranquilpeak/source/_css/layouts/_footer.scss","hash":"645161942c66e6afbd666b4755337a6a6261e5e0","modified":1756765611337},{"_id":"themes/tranquilpeak/source/_css/layouts/_header.scss","hash":"d840a4be850cb258167ee1081e1b288dc64b3d2c","modified":1756765611338},{"_id":"themes/tranquilpeak/source/_css/layouts/_sidebar.scss","hash":"facaff0ca816a4ace67c7689e4286e88eb1010c2","modified":1756765611338},{"_id":"themes/tranquilpeak/source/_css/pages/_search.scss","hash":"5f7cde0b68156ea802c3caf6cd673cab948b09d9","modified":1756765611338},{"_id":"themes/tranquilpeak/source/_css/themes/_hljs-custom.scss","hash":"a0ffab53db35bbd4881aeb077400cba55551f3bd","modified":1756765611339},{"_id":"themes/tranquilpeak/source/_css/utils/_fonts.scss","hash":"4da4074668be0e0abe756aa537a406d14da7ceff","modified":1756765611339},{"_id":"themes/tranquilpeak/source/_css/utils/_variables.scss","hash":"fc62fcffcdb16fb823c3cb170bbcf34493f0467f","modified":1756765611339},{"_id":"themes/tranquilpeak/source/_css/utils/mixins/_bottom-bar.scss","hash":"ab28b4d66f021e44d7a46a76157c6a2b5fd23496","modified":1756765611340},{"_id":"themes/tranquilpeak/source/_css/utils/mixins/_category.scss","hash":"8cfc45772365e537949a4cd20cfa2a79e1a75dc8","modified":1756765611340},{"_id":"themes/tranquilpeak/source/_css/utils/mixins/_form.scss","hash":"14026f13b54ef9f2cd7109cf1bca43040816e542","modified":1756765611340},{"_id":"themes/tranquilpeak/source/_css/utils/mixins/_button.scss","hash":"70c64560144d7d6015bbedeca0531258f9bafa07","modified":1756765611340},{"_id":"themes/tranquilpeak/source/_css/utils/mixins/_header.scss","hash":"52de2cb5f49a3b5748557fe2d25791ce52457d62","modified":1756765611341},{"_id":"themes/tranquilpeak/source/_css/themes/_hljs-tranquilpeak.scss","hash":"3c1e0d8b6e2f37820d00572878d060080c5a6fe7","modified":1756765611339},{"_id":"themes/tranquilpeak/source/_css/utils/mixins/_main.scss","hash":"0ad55a6191d0a533917845006ceae329353c0b90","modified":1756765611341},{"_id":"themes/tranquilpeak/source/_css/utils/mixins/_opacity.scss","hash":"cfc9905b6df3b5100a30b5c47fd331068f238f97","modified":1756765611341},{"_id":"themes/tranquilpeak/source/_css/utils/mixins/_post-header-cover.scss","hash":"ecd9078372d8bd1b7f4979bae3251e8efb345a9b","modified":1756765611341},{"_id":"themes/tranquilpeak/source/_css/utils/mixins/_prefix.scss","hash":"5e04cd54ba375f69c97b62b7d4e535118ec78177","modified":1756765611341},{"_id":"themes/tranquilpeak/source/_css/utils/mixins/_sidebar.scss","hash":"6c3dc034c3d5fdf7f0c4c39a6913f5bae4fc4767","modified":1756765611342},{"_id":"themes/tranquilpeak/source/_css/utils/mixins/_share-options-bar.scss","hash":"06f84bceb82df4d3bda64144f6fe4ad8e3e45f89","modified":1756765611341},{"_id":"themes/tranquilpeak/source/_css/utils/mixins/_tag.scss","hash":"c8a87e586e27f999ffa46f273f78a812f1cf2f0a","modified":1756765611342},{"_id":"themes/tranquilpeak/package-lock.json","hash":"aca92fc78bbd8ffe5d4a63f96d0f786600ea0ee0","modified":1756765611326},{"_id":"themes/tranquilpeak/source/_images/cover.jpg","hash":"df915f5b3eeed6bad93a183f65e5c6f536e551fe","modified":1756765611346},{"_id":"themes/tranquilpeak/source/_images/cover-v1.2.0.jpg","hash":"a98b9bf4d6be3af0156c0b6e781f067e343a682d","modified":1756765611345},{"_id":"public/all-categories/index.html","hash":"2cef3bc4562d4937c3e0be0cf030badbf4181d3c","modified":1756774751980},{"_id":"public/all-archives/index.html","hash":"5dccb8184150363792518810a47260d22ca442c2","modified":1756774751980},{"_id":"public/all-tags/index.html","hash":"62d1fcd03ebbc335b8202464687808c9f37e6401","modified":1756774751980},{"_id":"public/2020/11/28/PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-2-Config-TFTP-and-NFS-mounts/index.html","hash":"05c386436e9a05e40b6664119e09a6e1ae49e697","modified":1756774751980},{"_id":"public/2020/11/28/PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-1-DHCP-Setup/index.html","hash":"d063638c1f0b113625a334e8361f5a26533879bb","modified":1756774751980},{"_id":"public/2020/05/23/Why-Keeping-High-Standard-of-Shit-Is-Important/index.html","hash":"49f35683cb5a71102b2533458e3e3ec66d0efc4c","modified":1756774751980},{"_id":"public/2020/04/04/Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul/index.html","hash":"9511f3dfaca615e46b7b91838bbedd07ef418d22","modified":1756774751980},{"_id":"public/2018/11/25/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray/index.html","hash":"0ff66ae2065ba32ffaab20e3c85854be61e23f85","modified":1756774751980},{"_id":"public/2018/05/14/Angular-4-Custom-Bootstrapping-Lazy-Bind-to-Designated-Container/index.html","hash":"45bb145e8fdb86f4b4214fe49bea833a8e25a30a","modified":1756774751980},{"_id":"public/2018/05/13/Building-Linux-Workspace-on-Windows-10-via-WSL/index.html","hash":"0dc39d829e3779cded7560640f8906cee4d682d8","modified":1756774751980},{"_id":"public/2018/04/13/Workflow-Processing-Engine-Overview-2018-Airflow-vs-Azkaban-vs-Conductor-vs-Oozie-vs-Amazon-Step-Functions/index.html","hash":"8269bf2c30dd9a0f5afe43d7423c9fc717c9b27e","modified":1756774751980},{"_id":"public/2016/10/03/You-Don-t-Know-JS-Equal-or-Not-Equal/index.html","hash":"38e115817c95805c4762d8a73537dc26833f0fce","modified":1756774751980},{"_id":"public/2016/10/01/You-Don-t-Know-JS-Eval/index.html","hash":"753fc15f4fb8f5787a220e56b83d0239ad1c61a1","modified":1756774751980},{"_id":"public/2016/09/18/Why-you-should-ditch-browserify-and-commonjs-in-the-http-2-world/index.html","hash":"f196bfe6257441e77b6fb427639536a8cff8873d","modified":1756774751980},{"_id":"public/2016/09/10/It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO/index.html","hash":"a7c0c0eb4abae5fcdc02f269fb3a7a9a077ee524","modified":1756774751980},{"_id":"public/2016/08/14/get-started-with-hexo-blogging-system/index.html","hash":"b8445dc52e71d99f9587c864f6427bc6c6939613","modified":1756774751980},{"_id":"public/categories/Frontend/index.html","hash":"90fdd2a4808662c13552e5f7d77df772c3b2733a","modified":1756774751980},{"_id":"public/categories/OS/index.html","hash":"5f79d57fd36f945dc07006350dcaa324da8b00e7","modified":1756774751980},{"_id":"public/categories/Operation/index.html","hash":"640beac9fbcddd04a40e80f9cced092d0f001521","modified":1756774751980},{"_id":"public/categories/Operation/Architecture/index.html","hash":"bc07aecfd97e43260a891211d0386a5d4aa09d2f","modified":1756774751980},{"_id":"public/categories/You-Don-t-Know-JS/index.html","hash":"7b26e9b10ceb82627b0407c7fbab83b80889a1aa","modified":1756774751980},{"_id":"public/categories/Hexo/index.html","hash":"686448d991b8c81e3b8e9730cf6bf031f3154764","modified":1756774751980},{"_id":"public/index.html","hash":"2f13201b49b97545053102b826b1b40abfd0c07a","modified":1756774751980},{"_id":"public/page/2/index.html","hash":"bdf5b7c6d57f7719ebb789656507f82e4c313c9e","modified":1756774751980},{"_id":"public/archives/index.html","hash":"cedb217e8456820c72e8a71be8ec2257e6ca747c","modified":1756774751980},{"_id":"public/archives/page/2/index.html","hash":"5789b6b46e888cd4406ff26883e2b17e9cfcede4","modified":1756774751980},{"_id":"public/archives/2016/index.html","hash":"e1fb4c3ea40b1462a753b632d3391320a2fca1f1","modified":1756774751980},{"_id":"public/archives/2016/08/index.html","hash":"24da3e8e6ba2a86b2166d952db255f0fcabf1f1d","modified":1756774751980},{"_id":"public/archives/2016/09/index.html","hash":"d707a0f721c0f506d529cb63ad06e5a7746ce3f8","modified":1756774751980},{"_id":"public/archives/2016/10/index.html","hash":"8929f15be12e75a3a373f0914802aaf13e68b2ac","modified":1756774751980},{"_id":"public/archives/2018/index.html","hash":"49e25f00fbda6ee96bed20be7a22a9973c6663ff","modified":1756774751980},{"_id":"public/archives/2018/04/index.html","hash":"82f21c457130c9096fadd01a9e4ddee3a9b6b9f7","modified":1756774751980},{"_id":"public/archives/2018/05/index.html","hash":"b74fb955643767b84c40cd637b9ff11c3598d6dd","modified":1756774751980},{"_id":"public/archives/2018/11/index.html","hash":"8f14b332a993531fc82bbcc9381af1b25341c3bc","modified":1756774751980},{"_id":"public/archives/2020/index.html","hash":"79f5f8caa60be6467b55f6ec6a2d7fa6f40b8ef0","modified":1756774751980},{"_id":"public/archives/2020/04/index.html","hash":"426908c5e19e7ef40faba7961e5b73e339960ac3","modified":1756774751980},{"_id":"public/archives/2020/05/index.html","hash":"55c5cc386a340e99cd8ff719012f0d3cfc3c5184","modified":1756774751980},{"_id":"public/archives/2020/11/index.html","hash":"2ced5c306d8b6f0dde4388ac88983f01c0eea43a","modified":1756774751980},{"_id":"public/tags/angular/index.html","hash":"615edb773203afced3316578d40eb48e4e07a763","modified":1756774751980},{"_id":"public/tags/linux/index.html","hash":"f4f58306cf46be1a357c8b4be330e6aa43b9e36b","modified":1756774751980},{"_id":"public/tags/windows/index.html","hash":"9780bdc3e951d545fe793d9fc1b9bb6e581b3c1c","modified":1756774751980},{"_id":"public/tags/wsl/index.html","hash":"324d8bc9a7a542b1a52eb127760deab1cbb03d99","modified":1756774751980},{"_id":"public/tags/io/index.html","hash":"f4e0b3507c6ceeb445ead38209fb65c0157376c2","modified":1756774751980},{"_id":"public/tags/wifi-openwrt/index.html","hash":"83606b2b033ef6033cc9fa240304f8d444d86153","modified":1756774751980},{"_id":"public/tags/os/index.html","hash":"06c5cfccdb56fb8ae0e4b1ba171631f344548cd0","modified":1756774751980},{"_id":"public/tags/java/index.html","hash":"64e47d060611145429d2efbd88a4e94aa960b635","modified":1756774751980},{"_id":"public/tags/unix/index.html","hash":"5443d60220ff324f3d212b3d81ee4f6e7a381484","modified":1756774751980},{"_id":"public/tags/grpc/index.html","hash":"9c0941c283b60b3a6f27f1cc07ad409aea455e5f","modified":1756774751980},{"_id":"public/tags/monitoring/index.html","hash":"62b294a0defdf8a3a3120673b98551fcaa6bc154","modified":1756774751980},{"_id":"public/tags/microservice/index.html","hash":"3c7b65f23851ce2e87e6b50154634649ba436065","modified":1756774751980},{"_id":"public/tags/kubernetes/index.html","hash":"aeee6c9dbc305c51041dd8ad1e7462e7296e1b70","modified":1756774751980},{"_id":"public/tags/xray/index.html","hash":"953bb6fa7d5beb185b00a0b620062e0f4dcbc8b0","modified":1756774751980},{"_id":"public/tags/aws/index.html","hash":"168f61094a76ddbbc600e69aa9f22b7dc31ddbae","modified":1756774751980},{"_id":"public/tags/raspberry/index.html","hash":"dffe2262c48c4ec86afe7c94337d8e0666c4337a","modified":1756774751980},{"_id":"public/tags/network/index.html","hash":"7e9feacb629eb1b9ca854453679f6a59be14189d","modified":1756774751980},{"_id":"public/tags/http2/index.html","hash":"ab1ef66f95bfd55fe83ce7238c4df6e75a5498e5","modified":1756774751980},{"_id":"public/tags/browserify/index.html","hash":"789dd18f0aeb6f8c415e167f3ed75cffa4cdfaa6","modified":1756774751980},{"_id":"public/tags/javascript/index.html","hash":"ac7e7e1d82a72c044097113b830877e69d905f82","modified":1756774751980},{"_id":"public/tags/dependency-management/index.html","hash":"cc00791d739c4e744e9d0e6ef3f9adee5444e211","modified":1756774751980},{"_id":"public/tags/modular-design/index.html","hash":"77e08a2cc2ea886ebe641aa311a858040f04ae12","modified":1756774751980},{"_id":"public/tags/es6/index.html","hash":"a188813aa027f82e0cde00f368a2d9d1c2ef7a40","modified":1756774751980},{"_id":"public/tags/opensource/index.html","hash":"b74e0f160f67398a8753878c931803625d0a7f63","modified":1756774751980},{"_id":"public/tags/software-engineering/index.html","hash":"a5edbd65f92ffe8e11ff996eb7a0bb110c6e534b","modified":1756774751980},{"_id":"public/tags/workflow/index.html","hash":"559e6fa54b3ca1b63cedafbd5792b072fb85d1f3","modified":1756774751980},{"_id":"public/tags/airflow/index.html","hash":"e8c34e8efb03ab8461329c6cbb0faf9a0136a64a","modified":1756774751980},{"_id":"public/tags/azkaban/index.html","hash":"4c8749fcddc3455cf870241486ab15d3770e0021","modified":1756774751980},{"_id":"public/tags/review/index.html","hash":"2650b17c7658fa554857f956a445630854ac34a6","modified":1756774751980},{"_id":"public/tags/frontend/index.html","hash":"fdfb80df5e017309105bc422bc587541eed93734","modified":1756774751980},{"_id":"public/tags/blog/index.html","hash":"ec5d159219f38d3d63812df6b3b899058c5d2ec6","modified":1756774751980},{"_id":"public/tags/hexo/index.html","hash":"a200b23d396c3d097d33359c6e6ee301e6e0b682","modified":1756774751980},{"_id":"source/_posts/post.md","hash":"e83d9e03aa979e5f6717111131b960fccb749f0a","modified":1756773562778},{"_id":"source/_posts/Journey-to-enable-M1-pro-single-thunderbolt.md","hash":"3862c3b24570c372bf591af9defa86bb7ebd8e48","modified":1756774649629},{"_id":"public/2025/09/01/Journey-to-enable-M1-pro-single-thunderbolt/index.html","hash":"109afbeeed269626c9c6ed7f7963c28cded21ca0","modified":1756774751980},{"_id":"public/tags/setup/index.html","hash":"588842ef158300c99551e542abcf7878c3e9b930","modified":1756774751980},{"_id":"public/tags/pcparts/index.html","hash":"2af51ab53bb34a7e705c0d8a7d82eacaab814680","modified":1756774751980},{"_id":"public/archives/2025/index.html","hash":"157511859df0dce6e44f0f0007527701b85150c5","modified":1756774751980},{"_id":"public/archives/2025/09/index.html","hash":"54a683369f34c4dc9022b3215ae7192830793b4f","modified":1756774751980}],"Category":[{"name":"Frontend","_id":"cmf1pfvs600048mmghto64uv5"},{"name":"OS","_id":"cmf1pfvs8000b8mmghjro4mrq"},{"name":"Operation","_id":"cmf1pfvs8000f8mmg61901fo3"},{"name":"Architecture","parent":"cmf1pfvs8000f8mmg61901fo3","_id":"cmf1pfvs9000i8mmg1fvk39uv"},{"name":"You Don't Know JS","_id":"cmf1pfvsg001l8mmgd7u0emmh"},{"name":"Hexo","_id":"cmf1pfvsh001u8mmgcsa0clxa"}],"Data":[],"Page":[{"title":"all-categories","layout":"all-categories","comments":0,"_content":"","source":"all-categories/index.md","raw":"---\ntitle: \"all-categories\"\nlayout: \"all-categories\"\ncomments: false\n---\n","date":"2025-09-01T22:26:51.294Z","updated":"2025-09-01T22:26:51.294Z","path":"all-categories/index.html","_id":"cmf1pfvs200008mmghwxz6drq","content":"","thumbnailImageUrl":null,"excerpt":"","more":""},{"title":"all-archives","layout":"all-archives","comments":0,"_content":"","source":"all-archives/index.md","raw":"---\ntitle: \"all-archives\"\nlayout: \"all-archives\"\ncomments: false\n---\n","date":"2025-09-01T22:26:51.294Z","updated":"2025-09-01T22:26:51.294Z","path":"all-archives/index.html","_id":"cmf1pfvs500028mmg832u9dnc","content":"","thumbnailImageUrl":null,"excerpt":"","more":""},{"title":"all-tags","layout":"all-tags","comments":0,"_content":"","source":"all-tags/index.md","raw":"---\ntitle: \"all-tags\"\nlayout: \"all-tags\"\ncomments: false\n---\n","date":"2025-09-01T22:26:51.295Z","updated":"2025-09-01T22:26:51.295Z","path":"all-tags/index.html","_id":"cmf1pfvs600068mmga7pv17y6","content":"","thumbnailImageUrl":null,"excerpt":"","more":""}],"Post":[{"title":"Angular 4+ Custom Bootstrapping: Lazy Bind to Designated Container","date":"2018-05-15T06:50:39.000Z","_content":"\n{% alert info no-icon %}\nThis works for Angular 4-6 so far.\n{% endalert %}\n\n<br>\nIf you have ever used Angular 1.x, you know there's a manual bootstrapping\noption which looks like:\n```js\nangular.bootstrap(document.querySelector('#myApp'), ['myModule'])`\n```\nThis used to be pretty handy until Angular 2 comes in and changes the life.\nFor some reason they decide to hide that option and ask people to just use\n`bootstrap` in `@NgModule`.\n\nI get that because for general users this is good enough,\nespecially if you are just building a general SPA.\nHowever if you want to build something advanced like lazy loading,\nor conditional rendering, then this seems a bit naive.\n\nThis is especially annoying when in React its counterpart is as simple as\n```js\nReactDOM.render(     \n  <MyApp />,\n  document.querySelector('#myApp')\n);\n```\n\nThis alone won't drive people away from Angular but it's just one of the examples\nthat shows Angular wants to force people into its model rather than thinking about\nuse cases in the real world.\n\nAlright enough whining and let's get to coding. After all, Angular seems excellent\nespecially it covers everything from development, testing, and packaging out of the box.\nLet's leave whining till next time.\n\n<!-- more -->\n\nI'll create a simple stackblitz app like this:\n\n<iframe style=\"border:none\" width=\"100%\" height=\"400px\" src=\"https://stackblitz.com/edit/angular-qrrjaz?embed=1&file=src/app/app.component.ts\"></iframe>\n\nIt's pretty simple. The module tells Angular to bootstrap `AppComponent`,\nwhich looks for an element with tag `<my-app>`. After that it loads the\n`HelloComponent` which renders the `greeting` message from input.\nThe button in `AppComponent` will switch the message to `it works` once clicked.\n\nBut what if we want to lazy load it into a div `#myApp` then?\n\nLooking at the document, it is not mentioned. However, if we carefully read it, we'll\nsee there's [something that reads](https://angular.io/guide/entry-components#a-bootstrapped-entry-component):\n\n> A component can also be bootstrapped imperatively in the module's ngDoBootstrap() method. The @NgModule.bootstrap property tells the compiler that this is an entry component and it should generate code to bootstrap the application with this component.\n\nAnd there's another chunk on that page that reads:\n\n> Though the @NgModule decorator has an entryComponents array, most of the time you won't have to explicitly set any entry components because Angular adds components listed in @NgModule.bootstrap and those in route definitions to entry components automatically. Though these two mechanisms account for most entry components, if your app happens to bootstrap or dynamically load a component by type imperatively, you must add it to entryComponents explicitly.\n\nAnd we are like:\n\n{% rage_face 'Are you fucking kidding me' style:width:200px %}\n\nWait there's another chunk on the [bootstrapping page](https://angular.io/guide/bootstrapping):\n\n> The application launches by bootstrapping the root AppModule, which is also referred to as an entryComponent. Among other things, the bootstrapping process creates the component(s) listed in the bootstrap array and inserts each one into the browser DOM.\n\nOk I get that. But still, WTF does that mean?\n\n{% rage_face 'Desk flip' style:width:200px %}\n\nNever mind. I figured out through reading ~~document~~ source code. After all, some say,\ndocumentation is for the weak.\n\n## And here are the steps (finally)\n\nFirst of all, replace `bootstrap` with `entryComponents` in `@NgModule`.\nThis will tell Angular not to preemptively initialize everything.\nIn addition to that, the `entryComponents` param will tell angular to prepare\nall `ComponentFactory` instances and load them into app's `ComponentFactoryResolver`.\nAnd if you re-read the document you'll see what it means.\n\nSo our example app now looks like this:\n\n```js\n@NgModule({\n  imports:      [ BrowserModule, FormsModule ],\n  declarations: [ AppComponent, HelloComponent ],\n  entryComponents: [ AppComponent ]\n})\n```\n\nNext, override the `ngDoBootstrap()` with an empty body.\nThis will prevent default bootstrapping action when `bootstrapModule()` is called\nin `main.ts` file.\n\n```js\nexport class AppModule {\n  public ngDoBootstrap(appRef: ApplicationRef) {\n\n  } \n}\n```\n\nAlright now we go back to the `main.ts` file to perform the core magic.\n\nThe source code (thanks to TypeScript) tells us that\n`platformBrowserDynamic().bootstrapModule(AppModule)`\nreturns a `NgModuleRef`.\n\nIn `NgModuleRef` we can grab the `injector`. As we all know, angular is all about\ndependency injection. So we can call `bootstrap` here then and in newer version of\nAngular, that would take an `rootSelectorOrNode`.\n\nGreat so let's do:\n\n```js\nplatformBrowserDynamic().bootstrapModule(AppModule)\n  .then((moduleRef: NgModuleRef<AppModule>) => {\n    const app: ApplicationRef = moduleRef.injector.get(ApplicationRef);\n    app.bootstrap(AppComponent, '#myApp');\n  })\n```\n\nAfter this our app would boot, but nothing happens if you click on \"Knock knock\".\n\nWTF?\n\nIn Angular 2+, there's the magic of `ngZone`. You can read more about it in their\n[zone.js repo](https://github.com/angular/zone.js/). In short, it provides an\n\"isolated\" execution context in which it hijacks the regular DOM methods to provide\nfeedback loop for Angular to handle events more performantly. So inside Angular zone,\nyour click is no longer a plain one but enhanced with magic to tell Angular something\nhas happened.\n\nSo how do we get the `ngZone` then? Remeber we have the omnipotent `injector` so we\ncan do\n\n```js\n.then((moduleRef: NgModuleRef<AppModule>) => {\n  const app: ApplicationRef = moduleRef.injector.get(ApplicationRef);\n  const ngZone: NgZone = moduleRef.injector.get(NgZone);\n  ngZone.run(() => {\n    app.bootstrap(AppComponent, '#myApp');\n  });\n})\n```\n\nAt the end, here's everything in a nutshell:\n\n<iframe style=\"border:none\" width=\"100%\" height=\"400px\" src=\"https://stackblitz.com/edit/angular-1cnxy4?embed=1&ctl=1&file=src/app/app.module.ts\"></iframe>\n\nEnjoy hacking, until they support this with a one-liner.\n","source":"_posts/Angular-4-Custom-Bootstrapping-Lazy-Bind-to-Designated-Container.md","raw":"---\ntitle: 'Angular 4+ Custom Bootstrapping: Lazy Bind to Designated Container'\ndate: 2018-05-14 23:50:39\ncategories:\n- Frontend\ntags:\n- angular\n---\n\n{% alert info no-icon %}\nThis works for Angular 4-6 so far.\n{% endalert %}\n\n<br>\nIf you have ever used Angular 1.x, you know there's a manual bootstrapping\noption which looks like:\n```js\nangular.bootstrap(document.querySelector('#myApp'), ['myModule'])`\n```\nThis used to be pretty handy until Angular 2 comes in and changes the life.\nFor some reason they decide to hide that option and ask people to just use\n`bootstrap` in `@NgModule`.\n\nI get that because for general users this is good enough,\nespecially if you are just building a general SPA.\nHowever if you want to build something advanced like lazy loading,\nor conditional rendering, then this seems a bit naive.\n\nThis is especially annoying when in React its counterpart is as simple as\n```js\nReactDOM.render(     \n  <MyApp />,\n  document.querySelector('#myApp')\n);\n```\n\nThis alone won't drive people away from Angular but it's just one of the examples\nthat shows Angular wants to force people into its model rather than thinking about\nuse cases in the real world.\n\nAlright enough whining and let's get to coding. After all, Angular seems excellent\nespecially it covers everything from development, testing, and packaging out of the box.\nLet's leave whining till next time.\n\n<!-- more -->\n\nI'll create a simple stackblitz app like this:\n\n<iframe style=\"border:none\" width=\"100%\" height=\"400px\" src=\"https://stackblitz.com/edit/angular-qrrjaz?embed=1&file=src/app/app.component.ts\"></iframe>\n\nIt's pretty simple. The module tells Angular to bootstrap `AppComponent`,\nwhich looks for an element with tag `<my-app>`. After that it loads the\n`HelloComponent` which renders the `greeting` message from input.\nThe button in `AppComponent` will switch the message to `it works` once clicked.\n\nBut what if we want to lazy load it into a div `#myApp` then?\n\nLooking at the document, it is not mentioned. However, if we carefully read it, we'll\nsee there's [something that reads](https://angular.io/guide/entry-components#a-bootstrapped-entry-component):\n\n> A component can also be bootstrapped imperatively in the module's ngDoBootstrap() method. The @NgModule.bootstrap property tells the compiler that this is an entry component and it should generate code to bootstrap the application with this component.\n\nAnd there's another chunk on that page that reads:\n\n> Though the @NgModule decorator has an entryComponents array, most of the time you won't have to explicitly set any entry components because Angular adds components listed in @NgModule.bootstrap and those in route definitions to entry components automatically. Though these two mechanisms account for most entry components, if your app happens to bootstrap or dynamically load a component by type imperatively, you must add it to entryComponents explicitly.\n\nAnd we are like:\n\n{% rage_face 'Are you fucking kidding me' style:width:200px %}\n\nWait there's another chunk on the [bootstrapping page](https://angular.io/guide/bootstrapping):\n\n> The application launches by bootstrapping the root AppModule, which is also referred to as an entryComponent. Among other things, the bootstrapping process creates the component(s) listed in the bootstrap array and inserts each one into the browser DOM.\n\nOk I get that. But still, WTF does that mean?\n\n{% rage_face 'Desk flip' style:width:200px %}\n\nNever mind. I figured out through reading ~~document~~ source code. After all, some say,\ndocumentation is for the weak.\n\n## And here are the steps (finally)\n\nFirst of all, replace `bootstrap` with `entryComponents` in `@NgModule`.\nThis will tell Angular not to preemptively initialize everything.\nIn addition to that, the `entryComponents` param will tell angular to prepare\nall `ComponentFactory` instances and load them into app's `ComponentFactoryResolver`.\nAnd if you re-read the document you'll see what it means.\n\nSo our example app now looks like this:\n\n```js\n@NgModule({\n  imports:      [ BrowserModule, FormsModule ],\n  declarations: [ AppComponent, HelloComponent ],\n  entryComponents: [ AppComponent ]\n})\n```\n\nNext, override the `ngDoBootstrap()` with an empty body.\nThis will prevent default bootstrapping action when `bootstrapModule()` is called\nin `main.ts` file.\n\n```js\nexport class AppModule {\n  public ngDoBootstrap(appRef: ApplicationRef) {\n\n  } \n}\n```\n\nAlright now we go back to the `main.ts` file to perform the core magic.\n\nThe source code (thanks to TypeScript) tells us that\n`platformBrowserDynamic().bootstrapModule(AppModule)`\nreturns a `NgModuleRef`.\n\nIn `NgModuleRef` we can grab the `injector`. As we all know, angular is all about\ndependency injection. So we can call `bootstrap` here then and in newer version of\nAngular, that would take an `rootSelectorOrNode`.\n\nGreat so let's do:\n\n```js\nplatformBrowserDynamic().bootstrapModule(AppModule)\n  .then((moduleRef: NgModuleRef<AppModule>) => {\n    const app: ApplicationRef = moduleRef.injector.get(ApplicationRef);\n    app.bootstrap(AppComponent, '#myApp');\n  })\n```\n\nAfter this our app would boot, but nothing happens if you click on \"Knock knock\".\n\nWTF?\n\nIn Angular 2+, there's the magic of `ngZone`. You can read more about it in their\n[zone.js repo](https://github.com/angular/zone.js/). In short, it provides an\n\"isolated\" execution context in which it hijacks the regular DOM methods to provide\nfeedback loop for Angular to handle events more performantly. So inside Angular zone,\nyour click is no longer a plain one but enhanced with magic to tell Angular something\nhas happened.\n\nSo how do we get the `ngZone` then? Remeber we have the omnipotent `injector` so we\ncan do\n\n```js\n.then((moduleRef: NgModuleRef<AppModule>) => {\n  const app: ApplicationRef = moduleRef.injector.get(ApplicationRef);\n  const ngZone: NgZone = moduleRef.injector.get(NgZone);\n  ngZone.run(() => {\n    app.bootstrap(AppComponent, '#myApp');\n  });\n})\n```\n\nAt the end, here's everything in a nutshell:\n\n<iframe style=\"border:none\" width=\"100%\" height=\"400px\" src=\"https://stackblitz.com/edit/angular-1cnxy4?embed=1&ctl=1&file=src/app/app.module.ts\"></iframe>\n\nEnjoy hacking, until they support this with a one-liner.\n","slug":"Angular-4-Custom-Bootstrapping-Lazy-Bind-to-Designated-Container","published":1,"updated":"2025-09-01T22:26:51.285Z","comments":1,"layout":"post","photos":[],"_id":"cmf1pfvs300018mmg9u4qb1pk","content":"<div class=\"alert info no-icon\"><p>This works for Angular 4-6 so far.</p>\n</div>\n\n<br>\nIf you have ever used Angular 1.x, you know there's a manual bootstrapping\noption which looks like:\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">angular.<span class=\"title function_\">bootstrap</span>(<span class=\"variable language_\">document</span>.<span class=\"title function_\">querySelector</span>(<span class=\"string\">&#x27;#myApp&#x27;</span>), [<span class=\"string\">&#x27;myModule&#x27;</span>])<span class=\"string\">`</span></span><br></pre></td></tr></table></figure>\nThis used to be pretty handy until Angular 2 comes in and changes the life.\nFor some reason they decide to hide that option and ask people to just use\n`bootstrap` in `@NgModule`.\n\n<p>I get that because for general users this is good enough,<br>especially if you are just building a general SPA.<br>However if you want to build something advanced like lazy loading,<br>or conditional rendering, then this seems a bit naive.</p>\n<p>This is especially annoying when in React its counterpart is as simple as</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"title class_\">ReactDOM</span>.<span class=\"title function_\">render</span>(     </span><br><span class=\"line\">  <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">MyApp</span> /&gt;</span></span>,</span><br><span class=\"line\">  <span class=\"variable language_\">document</span>.<span class=\"title function_\">querySelector</span>(<span class=\"string\">&#x27;#myApp&#x27;</span>)</span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure>\n\n<p>This alone won’t drive people away from Angular but it’s just one of the examples<br>that shows Angular wants to force people into its model rather than thinking about<br>use cases in the real world.</p>\n<p>Alright enough whining and let’s get to coding. After all, Angular seems excellent<br>especially it covers everything from development, testing, and packaging out of the box.<br>Let’s leave whining till next time.</p>\n<span id=\"more\"></span>\n\n<p>I’ll create a simple stackblitz app like this:</p>\n<iframe style=\"border:none\" width=\"100%\" height=\"400px\" src=\"https://stackblitz.com/edit/angular-qrrjaz?embed=1&file=src/app/app.component.ts\"></iframe>\n\n<p>It’s pretty simple. The module tells Angular to bootstrap <code>AppComponent</code>,<br>which looks for an element with tag <code>&lt;my-app&gt;</code>. After that it loads the<br><code>HelloComponent</code> which renders the <code>greeting</code> message from input.<br>The button in <code>AppComponent</code> will switch the message to <code>it works</code> once clicked.</p>\n<p>But what if we want to lazy load it into a div <code>#myApp</code> then?</p>\n<p>Looking at the document, it is not mentioned. However, if we carefully read it, we’ll<br>see there’s <a href=\"https://angular.io/guide/entry-components#a-bootstrapped-entry-component\">something that reads</a>:</p>\n<blockquote>\n<p>A component can also be bootstrapped imperatively in the module’s ngDoBootstrap() method. The @NgModule.bootstrap property tells the compiler that this is an entry component and it should generate code to bootstrap the application with this component.</p>\n</blockquote>\n<p>And there’s another chunk on that page that reads:</p>\n<blockquote>\n<p>Though the @NgModule decorator has an entryComponents array, most of the time you won’t have to explicitly set any entry components because Angular adds components listed in @NgModule.bootstrap and those in route definitions to entry components automatically. Though these two mechanisms account for most entry components, if your app happens to bootstrap or dynamically load a component by type imperatively, you must add it to entryComponents explicitly.</p>\n</blockquote>\n<p>And we are like:</p>\n<img src=\"http://www.memes.at/faces/are_you_fucking_kidding_me.jpg\" alt=\"Are you fucking kidding me\" style=\"width:200px\">\n\n<p>Wait there’s another chunk on the <a href=\"https://angular.io/guide/bootstrapping\">bootstrapping page</a>:</p>\n<blockquote>\n<p>The application launches by bootstrapping the root AppModule, which is also referred to as an entryComponent. Among other things, the bootstrapping process creates the component(s) listed in the bootstrap array and inserts each one into the browser DOM.</p>\n</blockquote>\n<p>Ok I get that. But still, WTF does that mean?</p>\n<img src=\"http://www.memes.at/faces/desk_flip.jpg\" alt=\"Desk flip\" style=\"width:200px\">\n\n<p>Never mind. I figured out through reading <del>document</del> source code. After all, some say,<br>documentation is for the weak.</p>\n<h2 id=\"And-here-are-the-steps-finally\"><a href=\"#And-here-are-the-steps-finally\" class=\"headerlink\" title=\"And here are the steps (finally)\"></a>And here are the steps (finally)</h2><p>First of all, replace <code>bootstrap</code> with <code>entryComponents</code> in <code>@NgModule</code>.<br>This will tell Angular not to preemptively initialize everything.<br>In addition to that, the <code>entryComponents</code> param will tell angular to prepare<br>all <code>ComponentFactory</code> instances and load them into app’s <code>ComponentFactoryResolver</code>.<br>And if you re-read the document you’ll see what it means.</p>\n<p>So our example app now looks like this:</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@<span class=\"title class_\">NgModule</span>(&#123;</span><br><span class=\"line\">  <span class=\"attr\">imports</span>:      [ <span class=\"title class_\">BrowserModule</span>, <span class=\"title class_\">FormsModule</span> ],</span><br><span class=\"line\">  <span class=\"attr\">declarations</span>: [ <span class=\"title class_\">AppComponent</span>, <span class=\"title class_\">HelloComponent</span> ],</span><br><span class=\"line\">  <span class=\"attr\">entryComponents</span>: [ <span class=\"title class_\">AppComponent</span> ]</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>Next, override the <code>ngDoBootstrap()</code> with an empty body.<br>This will prevent default bootstrapping action when <code>bootstrapModule()</code> is called<br>in <code>main.ts</code> file.</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"keyword\">class</span> <span class=\"title class_\">AppModule</span> &#123;</span><br><span class=\"line\">  public <span class=\"title function_\">ngDoBootstrap</span>(<span class=\"params\">appRef: ApplicationRef</span>) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125; </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Alright now we go back to the <code>main.ts</code> file to perform the core magic.</p>\n<p>The source code (thanks to TypeScript) tells us that<br><code>platformBrowserDynamic().bootstrapModule(AppModule)</code><br>returns a <code>NgModuleRef</code>.</p>\n<p>In <code>NgModuleRef</code> we can grab the <code>injector</code>. As we all know, angular is all about<br>dependency injection. So we can call <code>bootstrap</code> here then and in newer version of<br>Angular, that would take an <code>rootSelectorOrNode</code>.</p>\n<p>Great so let’s do:</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"title function_\">platformBrowserDynamic</span>().<span class=\"title function_\">bootstrapModule</span>(<span class=\"title class_\">AppModule</span>)</span><br><span class=\"line\">  .<span class=\"title function_\">then</span>(<span class=\"function\">(<span class=\"params\">moduleRef: NgModuleRef&lt;AppModule&gt;</span>) =&gt;</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">const</span> <span class=\"attr\">app</span>: <span class=\"title class_\">ApplicationRef</span> = moduleRef.<span class=\"property\">injector</span>.<span class=\"title function_\">get</span>(<span class=\"title class_\">ApplicationRef</span>);</span><br><span class=\"line\">    app.<span class=\"title function_\">bootstrap</span>(<span class=\"title class_\">AppComponent</span>, <span class=\"string\">&#x27;#myApp&#x27;</span>);</span><br><span class=\"line\">  &#125;)</span><br></pre></td></tr></table></figure>\n\n<p>After this our app would boot, but nothing happens if you click on “Knock knock”.</p>\n<p>WTF?</p>\n<p>In Angular 2+, there’s the magic of <code>ngZone</code>. You can read more about it in their<br><a href=\"https://github.com/angular/zone.js/\">zone.js repo</a>. In short, it provides an<br>“isolated” execution context in which it hijacks the regular DOM methods to provide<br>feedback loop for Angular to handle events more performantly. So inside Angular zone,<br>your click is no longer a plain one but enhanced with magic to tell Angular something<br>has happened.</p>\n<p>So how do we get the <code>ngZone</code> then? Remeber we have the omnipotent <code>injector</code> so we<br>can do</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.<span class=\"title function_\">then</span>(<span class=\"function\">(<span class=\"params\">moduleRef: NgModuleRef&lt;AppModule&gt;</span>) =&gt;</span> &#123;</span><br><span class=\"line\">  <span class=\"keyword\">const</span> <span class=\"attr\">app</span>: <span class=\"title class_\">ApplicationRef</span> = moduleRef.<span class=\"property\">injector</span>.<span class=\"title function_\">get</span>(<span class=\"title class_\">ApplicationRef</span>);</span><br><span class=\"line\">  <span class=\"keyword\">const</span> <span class=\"attr\">ngZone</span>: <span class=\"title class_\">NgZone</span> = moduleRef.<span class=\"property\">injector</span>.<span class=\"title function_\">get</span>(<span class=\"title class_\">NgZone</span>);</span><br><span class=\"line\">  ngZone.<span class=\"title function_\">run</span>(<span class=\"function\">() =&gt;</span> &#123;</span><br><span class=\"line\">    app.<span class=\"title function_\">bootstrap</span>(<span class=\"title class_\">AppComponent</span>, <span class=\"string\">&#x27;#myApp&#x27;</span>);</span><br><span class=\"line\">  &#125;);</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>At the end, here’s everything in a nutshell:</p>\n<iframe style=\"border:none\" width=\"100%\" height=\"400px\" src=\"https://stackblitz.com/edit/angular-1cnxy4?embed=1&ctl=1&file=src/app/app.module.ts\"></iframe>\n\n<p>Enjoy hacking, until they support this with a one-liner.</p>\n","thumbnailImageUrl":null,"excerpt":"<div class=\"alert info no-icon\"><p>This works for Angular 4-6 so far.</p>\n</div>\n\n<br>\nIf you have ever used Angular 1.x, you know there's a manual bootstrapping\noption which looks like:\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">angular.<span class=\"title function_\">bootstrap</span>(<span class=\"variable language_\">document</span>.<span class=\"title function_\">querySelector</span>(<span class=\"string\">&#x27;#myApp&#x27;</span>), [<span class=\"string\">&#x27;myModule&#x27;</span>])<span class=\"string\">`</span></span><br></pre></td></tr></table></figure>\nThis used to be pretty handy until Angular 2 comes in and changes the life.\nFor some reason they decide to hide that option and ask people to just use\n`bootstrap` in `@NgModule`.\n\n<p>I get that because for general users this is good enough,<br>especially if you are just building a general SPA.<br>However if you want to build something advanced like lazy loading,<br>or conditional rendering, then this seems a bit naive.</p>\n<p>This is especially annoying when in React its counterpart is as simple as</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"title class_\">ReactDOM</span>.<span class=\"title function_\">render</span>(     </span><br><span class=\"line\">  <span class=\"language-xml\"><span class=\"tag\">&lt;<span class=\"name\">MyApp</span> /&gt;</span></span>,</span><br><span class=\"line\">  <span class=\"variable language_\">document</span>.<span class=\"title function_\">querySelector</span>(<span class=\"string\">&#x27;#myApp&#x27;</span>)</span><br><span class=\"line\">);</span><br></pre></td></tr></table></figure>\n\n<p>This alone won’t drive people away from Angular but it’s just one of the examples<br>that shows Angular wants to force people into its model rather than thinking about<br>use cases in the real world.</p>\n<p>Alright enough whining and let’s get to coding. After all, Angular seems excellent<br>especially it covers everything from development, testing, and packaging out of the box.<br>Let’s leave whining till next time.</p>","more":"<p>I’ll create a simple stackblitz app like this:</p>\n<iframe style=\"border:none\" width=\"100%\" height=\"400px\" src=\"https://stackblitz.com/edit/angular-qrrjaz?embed=1&file=src/app/app.component.ts\"></iframe>\n\n<p>It’s pretty simple. The module tells Angular to bootstrap <code>AppComponent</code>,<br>which looks for an element with tag <code>&lt;my-app&gt;</code>. After that it loads the<br><code>HelloComponent</code> which renders the <code>greeting</code> message from input.<br>The button in <code>AppComponent</code> will switch the message to <code>it works</code> once clicked.</p>\n<p>But what if we want to lazy load it into a div <code>#myApp</code> then?</p>\n<p>Looking at the document, it is not mentioned. However, if we carefully read it, we’ll<br>see there’s <a href=\"https://angular.io/guide/entry-components#a-bootstrapped-entry-component\">something that reads</a>:</p>\n<blockquote>\n<p>A component can also be bootstrapped imperatively in the module’s ngDoBootstrap() method. The @NgModule.bootstrap property tells the compiler that this is an entry component and it should generate code to bootstrap the application with this component.</p>\n</blockquote>\n<p>And there’s another chunk on that page that reads:</p>\n<blockquote>\n<p>Though the @NgModule decorator has an entryComponents array, most of the time you won’t have to explicitly set any entry components because Angular adds components listed in @NgModule.bootstrap and those in route definitions to entry components automatically. Though these two mechanisms account for most entry components, if your app happens to bootstrap or dynamically load a component by type imperatively, you must add it to entryComponents explicitly.</p>\n</blockquote>\n<p>And we are like:</p>\n<img src=\"http://www.memes.at/faces/are_you_fucking_kidding_me.jpg\" alt=\"Are you fucking kidding me\" style=\"width:200px\">\n\n<p>Wait there’s another chunk on the <a href=\"https://angular.io/guide/bootstrapping\">bootstrapping page</a>:</p>\n<blockquote>\n<p>The application launches by bootstrapping the root AppModule, which is also referred to as an entryComponent. Among other things, the bootstrapping process creates the component(s) listed in the bootstrap array and inserts each one into the browser DOM.</p>\n</blockquote>\n<p>Ok I get that. But still, WTF does that mean?</p>\n<img src=\"http://www.memes.at/faces/desk_flip.jpg\" alt=\"Desk flip\" style=\"width:200px\">\n\n<p>Never mind. I figured out through reading <del>document</del> source code. After all, some say,<br>documentation is for the weak.</p>\n<h2 id=\"And-here-are-the-steps-finally\"><a href=\"#And-here-are-the-steps-finally\" class=\"headerlink\" title=\"And here are the steps (finally)\"></a>And here are the steps (finally)</h2><p>First of all, replace <code>bootstrap</code> with <code>entryComponents</code> in <code>@NgModule</code>.<br>This will tell Angular not to preemptively initialize everything.<br>In addition to that, the <code>entryComponents</code> param will tell angular to prepare<br>all <code>ComponentFactory</code> instances and load them into app’s <code>ComponentFactoryResolver</code>.<br>And if you re-read the document you’ll see what it means.</p>\n<p>So our example app now looks like this:</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@<span class=\"title class_\">NgModule</span>(&#123;</span><br><span class=\"line\">  <span class=\"attr\">imports</span>:      [ <span class=\"title class_\">BrowserModule</span>, <span class=\"title class_\">FormsModule</span> ],</span><br><span class=\"line\">  <span class=\"attr\">declarations</span>: [ <span class=\"title class_\">AppComponent</span>, <span class=\"title class_\">HelloComponent</span> ],</span><br><span class=\"line\">  <span class=\"attr\">entryComponents</span>: [ <span class=\"title class_\">AppComponent</span> ]</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>Next, override the <code>ngDoBootstrap()</code> with an empty body.<br>This will prevent default bootstrapping action when <code>bootstrapModule()</code> is called<br>in <code>main.ts</code> file.</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">export</span> <span class=\"keyword\">class</span> <span class=\"title class_\">AppModule</span> &#123;</span><br><span class=\"line\">  public <span class=\"title function_\">ngDoBootstrap</span>(<span class=\"params\">appRef: ApplicationRef</span>) &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125; </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Alright now we go back to the <code>main.ts</code> file to perform the core magic.</p>\n<p>The source code (thanks to TypeScript) tells us that<br><code>platformBrowserDynamic().bootstrapModule(AppModule)</code><br>returns a <code>NgModuleRef</code>.</p>\n<p>In <code>NgModuleRef</code> we can grab the <code>injector</code>. As we all know, angular is all about<br>dependency injection. So we can call <code>bootstrap</code> here then and in newer version of<br>Angular, that would take an <code>rootSelectorOrNode</code>.</p>\n<p>Great so let’s do:</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"title function_\">platformBrowserDynamic</span>().<span class=\"title function_\">bootstrapModule</span>(<span class=\"title class_\">AppModule</span>)</span><br><span class=\"line\">  .<span class=\"title function_\">then</span>(<span class=\"function\">(<span class=\"params\">moduleRef: NgModuleRef&lt;AppModule&gt;</span>) =&gt;</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">const</span> <span class=\"attr\">app</span>: <span class=\"title class_\">ApplicationRef</span> = moduleRef.<span class=\"property\">injector</span>.<span class=\"title function_\">get</span>(<span class=\"title class_\">ApplicationRef</span>);</span><br><span class=\"line\">    app.<span class=\"title function_\">bootstrap</span>(<span class=\"title class_\">AppComponent</span>, <span class=\"string\">&#x27;#myApp&#x27;</span>);</span><br><span class=\"line\">  &#125;)</span><br></pre></td></tr></table></figure>\n\n<p>After this our app would boot, but nothing happens if you click on “Knock knock”.</p>\n<p>WTF?</p>\n<p>In Angular 2+, there’s the magic of <code>ngZone</code>. You can read more about it in their<br><a href=\"https://github.com/angular/zone.js/\">zone.js repo</a>. In short, it provides an<br>“isolated” execution context in which it hijacks the regular DOM methods to provide<br>feedback loop for Angular to handle events more performantly. So inside Angular zone,<br>your click is no longer a plain one but enhanced with magic to tell Angular something<br>has happened.</p>\n<p>So how do we get the <code>ngZone</code> then? Remeber we have the omnipotent <code>injector</code> so we<br>can do</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.<span class=\"title function_\">then</span>(<span class=\"function\">(<span class=\"params\">moduleRef: NgModuleRef&lt;AppModule&gt;</span>) =&gt;</span> &#123;</span><br><span class=\"line\">  <span class=\"keyword\">const</span> <span class=\"attr\">app</span>: <span class=\"title class_\">ApplicationRef</span> = moduleRef.<span class=\"property\">injector</span>.<span class=\"title function_\">get</span>(<span class=\"title class_\">ApplicationRef</span>);</span><br><span class=\"line\">  <span class=\"keyword\">const</span> <span class=\"attr\">ngZone</span>: <span class=\"title class_\">NgZone</span> = moduleRef.<span class=\"property\">injector</span>.<span class=\"title function_\">get</span>(<span class=\"title class_\">NgZone</span>);</span><br><span class=\"line\">  ngZone.<span class=\"title function_\">run</span>(<span class=\"function\">() =&gt;</span> &#123;</span><br><span class=\"line\">    app.<span class=\"title function_\">bootstrap</span>(<span class=\"title class_\">AppComponent</span>, <span class=\"string\">&#x27;#myApp&#x27;</span>);</span><br><span class=\"line\">  &#125;);</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>At the end, here’s everything in a nutshell:</p>\n<iframe style=\"border:none\" width=\"100%\" height=\"400px\" src=\"https://stackblitz.com/edit/angular-1cnxy4?embed=1&ctl=1&file=src/app/app.module.ts\"></iframe>\n\n<p>Enjoy hacking, until they support this with a one-liner.</p>"},{"title":"Building Linux Workspace on Windows 10 via WSL","date":"2018-05-13T22:35:46.000Z","_content":"\n{% asset_img title.png %}\n\nI was not really a fan of Windows 10,\nlet alone Microsoft decided to ditch the most important feature I liked in Windows 7 - Aero.\nIn fact, I'd admit that in most cases I use Windows as an entertainment system\nrather than a working platform.\n\nDon't get me wrong, Windows is great, both in terms of the quality of the software\nand the design/usability of the system by itself. It's also particularly great of you are\na .NET developer, a webmaster using IIS, or a game developer heavily using DirectX.\nHowever, it's just cumbersome to use it as a daily OSS platform, namely there lacks the\ngeneral ecosystem and the tools are just different. Yes you can install node, java, maven,\ngradle, and you can probably use powershell to write shell scripts, but at the end of the day,\nthe overall configuration just feels different and since most people don't use Windows\nfor work on a day-to-day basis, it just takes too much time and effort to learn a set of\nrules with different flavor, just to get the environment set up.\n\nHowever, things have changed.\n\nThe release of WSL (Windows Subsystem on Linux) in Windows 10 was like silent bomb.\nIt wasn't really marketed to general public, but it implies the fundamental\nchange of attitude from Microsoft towards OSS community.\n\nWSL is not a virtual machine. In fact there's no real linux kernel running.\nInstead, there is a layer in between that translates linux system calls to\nsomething that windows kernel can handle. Technically, this is seriously phenomenal,\nas there's certain things that there's no direct equivalent in Windows.\n\nFor example:\n\nQuoted from [MSDN blog](https://blogs.msdn.microsoft.com/wsl/2016/06/08/wsl-system-calls/)\n\n> The Linux fork syscall has no documented equivalent for Windows.\n> When a fork syscall is made on WSL, lxss.sys does some of the initial work\n> to prepare for copying the process.\n> It then calls internal NT APIs to create the process with the correct semantics\n> and create a thread in the process with an identical register context.\n> Finally, it does some additional work to complete copying the process\n> and resumes the new process so it can begin executing.\n\nAnd another one regarding [WSL file system](https://blogs.msdn.microsoft.com/wsl/2016/06/15/wsl-file-system-support/):\n\n> The Windows Subsystem for Linux must translate various Linux file system operations\n> into NT kernel operations. WSL must provide a place where Linux system files can exist\n> with all the functionality required for that including Linux permissions,\n> symbolic links and other special files such as FIFOs;\n> it must provide access to the Windows volumes on your system;\n> and it must provide special file systems such as ProcFs.\n\nAnd now it even supports [interop](https://docs.microsoft.com/en-us/windows/wsl/interop)\nafter the Fall Creators update. This means if you type in `notepad.exe`,\nit would literally open notepad for you. Not very exciting but beyond that you could\ndo\n\n```shell\n# copy stuff to clipboard\necho 'foo bar' | clip.exe\n\n# open a file in windows using default associated program\ncmd.exe /C start image.png\n```\n\n**Awesome, but what's our original topic?**\n\n<!-- more -->\n\n## Enable WSL\n\nOk, to use WSL, you'd need to enable it first.\n\n*// Make sure you are using Windows 10 :)*\n\nIf you are running windows **earlier than 1709**, the setup would be like this:\n\n1. Press Windows key. Type in \"Turn windows features on or off\" and click on the option that shows up.\n2. Turn on WSL\n\n    {% asset_img WSL.png %}\n\n3. Reboot system. You will now have \"Bash on Ubuntu on Windows\" in your Start Menu.\n\nIf you are running version **>= 1709**, just search for \"Ubuntu\"\nor other available distributions like \"Debian\" or \"OpenSUSE\" in Microsoft Store.\n\n## Throw Away Cmd\n\nThe long lived cmd.exe in windows is not something people would generally like.\n\nIf you want to have something close to iTerm2 on Mac or Yakuake on KDE,\nyou can try [ConEmu](https://conemu.github.io/) (see title image).\n\nConEmu allows for creating tabs (Win + Shift + W),\nand splitting windows (Ctrl + Shift + O/E). Alternatively you can use tmux.\n\nI personally enable the Quake style which allows me to hit `Ctrl + ~` to bring it down.\n\nConEmu does not seem to have start up with Windows option but you can get around that\nby putting the shortcut into Startup folder in Start Menu and set the initial window\noption to be \"Minimized\".\n\n## Basic Setup\n\n### Upgrade Packages\n\n```shell\n$ sudo apt -y update\n$ sudo apt -y upgrade\n$ sudo apt -y dist-upgrade\n$ sudo apt -y autoremove\n```\n\n### Change to Favorite Shell\n\nBy default most distributions would use bash.\n\nIf you want to use zsh or other shell:\n\n```shell\n$ sudo chsh --shell /bin/zsh <username>\n```\n\nor if you want to install oh-my-zsh:\n\n```shell\n$ sh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"\n```\n\n### Mount Network Drives\n\nUbuntu on windows should have mounted all local disks for you under `/mnt`.\nIn case you need to mount a network drive like NAS:\n\n```shell\n$ sudo mount -t drvfs Z: /mnt/z\n```\n\n## Some Advanced Tweaks You Might Want\n\n### Disabling bell\n\nBy default you'll hear an annoying \"dang\" every time you tab complete.\nThat is associated with the \"Critical Stop\" sound config in windows.\nTo stop it is shell specific but generally:\n\nFor bash, put this into `.bashrc`\n\n```bash\nset bell-style none\n```\n\nFor zsh, this works for me in `.zshrc`\n\n```bash\nunsetopt beep\n```\n\n### Stop Windows From Sharing PATH\n\nBy default Windows would copy PATH variables to WSL. This could be quite annoying\nif you use tools like pyenv, rvm or nvm and you also have python, ruby, or node installed\nin Windows as typically the stuff from Windows will take priority.\n\nTo fix that you can create a DWORD `AppendNtPath` under\n`HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Lxss` and set the value to `0`.\n\n**Update:** The above trick seems to only work for legacy WSL (aka Bash on Ubuntu on Windows).\nFor new users, create a DWORD `DistributionFlags` with value `fffffffd` under `HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\LxssManager`.\nThen go to Services and restart the LxssManager service.\n\nThis is undocumented but described in [WSL#2048](https://github.com/Microsoft/WSL/issues/2048).\n\n### Allow Linux to Open File Using Preferred Application in Windows\n\nThis is quite useful if you just want to say check an image as WSL is not shipped with a desktop.\nBy default the `xdg-open` binary can be used in linux but WSL is not shipped with that either.\n\nLuckily as WSL supports interop, there's a trick:\n\n1. Create a file `/usr/bin/xdg-open` with content\n\n    ```bash\n    #!/usr/bin/env sh\n    /mnt/c/Windows/System32/cmd.exe /C start \"$1\"\n    ```\n\n2. Make it executable:\n\n    ```shell\n    $ sudo chmod +x /usr/bin/xdg-open\n    ```\n\n3. Use it:\n\n    ```shell\n    $ xdg-open a.png\n    ```\n\n### Call VSCode from Linux\n\nYou can technically get everything done with vi, but sometimes it's easier to just use a GUI.\n\nFor VSCode Mac version there's an option to install it to terminal, but it just doesn't exist in Windows.\n\nBut with WSL interop, just put this in shell rc file:\n\n```bash\nalias code /mnt/c/Program\\ Files/Microsoft\\ VS\\ Code/Code.exe\n```\n\nTo use it:\n\n```shell\n$ code path/to/my/file\n```\n\n### Install Python/Node/Ruby\n\nI use pyenv and it's the same as if you are in normal linux:\n\n```shell\n$ curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | zsh\npyenv install 3.6.5\npyenv shell 3.6.5\n```\n\nSimilar solution can be used for node and ruby using nvm and rvm respectively.\n","source":"_posts/Building-Linux-Workspace-on-Windows-10-via-WSL.md","raw":"---\ntitle: Building Linux Workspace on Windows 10 via WSL\ndate: 2018-05-13 15:35:46\ntags:\n- linux\n- windows\n- wsl\n---\n\n{% asset_img title.png %}\n\nI was not really a fan of Windows 10,\nlet alone Microsoft decided to ditch the most important feature I liked in Windows 7 - Aero.\nIn fact, I'd admit that in most cases I use Windows as an entertainment system\nrather than a working platform.\n\nDon't get me wrong, Windows is great, both in terms of the quality of the software\nand the design/usability of the system by itself. It's also particularly great of you are\na .NET developer, a webmaster using IIS, or a game developer heavily using DirectX.\nHowever, it's just cumbersome to use it as a daily OSS platform, namely there lacks the\ngeneral ecosystem and the tools are just different. Yes you can install node, java, maven,\ngradle, and you can probably use powershell to write shell scripts, but at the end of the day,\nthe overall configuration just feels different and since most people don't use Windows\nfor work on a day-to-day basis, it just takes too much time and effort to learn a set of\nrules with different flavor, just to get the environment set up.\n\nHowever, things have changed.\n\nThe release of WSL (Windows Subsystem on Linux) in Windows 10 was like silent bomb.\nIt wasn't really marketed to general public, but it implies the fundamental\nchange of attitude from Microsoft towards OSS community.\n\nWSL is not a virtual machine. In fact there's no real linux kernel running.\nInstead, there is a layer in between that translates linux system calls to\nsomething that windows kernel can handle. Technically, this is seriously phenomenal,\nas there's certain things that there's no direct equivalent in Windows.\n\nFor example:\n\nQuoted from [MSDN blog](https://blogs.msdn.microsoft.com/wsl/2016/06/08/wsl-system-calls/)\n\n> The Linux fork syscall has no documented equivalent for Windows.\n> When a fork syscall is made on WSL, lxss.sys does some of the initial work\n> to prepare for copying the process.\n> It then calls internal NT APIs to create the process with the correct semantics\n> and create a thread in the process with an identical register context.\n> Finally, it does some additional work to complete copying the process\n> and resumes the new process so it can begin executing.\n\nAnd another one regarding [WSL file system](https://blogs.msdn.microsoft.com/wsl/2016/06/15/wsl-file-system-support/):\n\n> The Windows Subsystem for Linux must translate various Linux file system operations\n> into NT kernel operations. WSL must provide a place where Linux system files can exist\n> with all the functionality required for that including Linux permissions,\n> symbolic links and other special files such as FIFOs;\n> it must provide access to the Windows volumes on your system;\n> and it must provide special file systems such as ProcFs.\n\nAnd now it even supports [interop](https://docs.microsoft.com/en-us/windows/wsl/interop)\nafter the Fall Creators update. This means if you type in `notepad.exe`,\nit would literally open notepad for you. Not very exciting but beyond that you could\ndo\n\n```shell\n# copy stuff to clipboard\necho 'foo bar' | clip.exe\n\n# open a file in windows using default associated program\ncmd.exe /C start image.png\n```\n\n**Awesome, but what's our original topic?**\n\n<!-- more -->\n\n## Enable WSL\n\nOk, to use WSL, you'd need to enable it first.\n\n*// Make sure you are using Windows 10 :)*\n\nIf you are running windows **earlier than 1709**, the setup would be like this:\n\n1. Press Windows key. Type in \"Turn windows features on or off\" and click on the option that shows up.\n2. Turn on WSL\n\n    {% asset_img WSL.png %}\n\n3. Reboot system. You will now have \"Bash on Ubuntu on Windows\" in your Start Menu.\n\nIf you are running version **>= 1709**, just search for \"Ubuntu\"\nor other available distributions like \"Debian\" or \"OpenSUSE\" in Microsoft Store.\n\n## Throw Away Cmd\n\nThe long lived cmd.exe in windows is not something people would generally like.\n\nIf you want to have something close to iTerm2 on Mac or Yakuake on KDE,\nyou can try [ConEmu](https://conemu.github.io/) (see title image).\n\nConEmu allows for creating tabs (Win + Shift + W),\nand splitting windows (Ctrl + Shift + O/E). Alternatively you can use tmux.\n\nI personally enable the Quake style which allows me to hit `Ctrl + ~` to bring it down.\n\nConEmu does not seem to have start up with Windows option but you can get around that\nby putting the shortcut into Startup folder in Start Menu and set the initial window\noption to be \"Minimized\".\n\n## Basic Setup\n\n### Upgrade Packages\n\n```shell\n$ sudo apt -y update\n$ sudo apt -y upgrade\n$ sudo apt -y dist-upgrade\n$ sudo apt -y autoremove\n```\n\n### Change to Favorite Shell\n\nBy default most distributions would use bash.\n\nIf you want to use zsh or other shell:\n\n```shell\n$ sudo chsh --shell /bin/zsh <username>\n```\n\nor if you want to install oh-my-zsh:\n\n```shell\n$ sh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"\n```\n\n### Mount Network Drives\n\nUbuntu on windows should have mounted all local disks for you under `/mnt`.\nIn case you need to mount a network drive like NAS:\n\n```shell\n$ sudo mount -t drvfs Z: /mnt/z\n```\n\n## Some Advanced Tweaks You Might Want\n\n### Disabling bell\n\nBy default you'll hear an annoying \"dang\" every time you tab complete.\nThat is associated with the \"Critical Stop\" sound config in windows.\nTo stop it is shell specific but generally:\n\nFor bash, put this into `.bashrc`\n\n```bash\nset bell-style none\n```\n\nFor zsh, this works for me in `.zshrc`\n\n```bash\nunsetopt beep\n```\n\n### Stop Windows From Sharing PATH\n\nBy default Windows would copy PATH variables to WSL. This could be quite annoying\nif you use tools like pyenv, rvm or nvm and you also have python, ruby, or node installed\nin Windows as typically the stuff from Windows will take priority.\n\nTo fix that you can create a DWORD `AppendNtPath` under\n`HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Lxss` and set the value to `0`.\n\n**Update:** The above trick seems to only work for legacy WSL (aka Bash on Ubuntu on Windows).\nFor new users, create a DWORD `DistributionFlags` with value `fffffffd` under `HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\LxssManager`.\nThen go to Services and restart the LxssManager service.\n\nThis is undocumented but described in [WSL#2048](https://github.com/Microsoft/WSL/issues/2048).\n\n### Allow Linux to Open File Using Preferred Application in Windows\n\nThis is quite useful if you just want to say check an image as WSL is not shipped with a desktop.\nBy default the `xdg-open` binary can be used in linux but WSL is not shipped with that either.\n\nLuckily as WSL supports interop, there's a trick:\n\n1. Create a file `/usr/bin/xdg-open` with content\n\n    ```bash\n    #!/usr/bin/env sh\n    /mnt/c/Windows/System32/cmd.exe /C start \"$1\"\n    ```\n\n2. Make it executable:\n\n    ```shell\n    $ sudo chmod +x /usr/bin/xdg-open\n    ```\n\n3. Use it:\n\n    ```shell\n    $ xdg-open a.png\n    ```\n\n### Call VSCode from Linux\n\nYou can technically get everything done with vi, but sometimes it's easier to just use a GUI.\n\nFor VSCode Mac version there's an option to install it to terminal, but it just doesn't exist in Windows.\n\nBut with WSL interop, just put this in shell rc file:\n\n```bash\nalias code /mnt/c/Program\\ Files/Microsoft\\ VS\\ Code/Code.exe\n```\n\nTo use it:\n\n```shell\n$ code path/to/my/file\n```\n\n### Install Python/Node/Ruby\n\nI use pyenv and it's the same as if you are in normal linux:\n\n```shell\n$ curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | zsh\npyenv install 3.6.5\npyenv shell 3.6.5\n```\n\nSimilar solution can be used for node and ruby using nvm and rvm respectively.\n","slug":"Building-Linux-Workspace-on-Windows-10-via-WSL","published":1,"updated":"2025-09-01T22:26:51.285Z","comments":1,"layout":"post","photos":[],"_id":"cmf1pfvs500038mmg08ibhuon","content":"<img src=\"/2018/05/13/Building-Linux-Workspace-on-Windows-10-via-WSL/title.png\" class=\"\">\n\n<p>I was not really a fan of Windows 10,<br>let alone Microsoft decided to ditch the most important feature I liked in Windows 7 - Aero.<br>In fact, I’d admit that in most cases I use Windows as an entertainment system<br>rather than a working platform.</p>\n<p>Don’t get me wrong, Windows is great, both in terms of the quality of the software<br>and the design/usability of the system by itself. It’s also particularly great of you are<br>a .NET developer, a webmaster using IIS, or a game developer heavily using DirectX.<br>However, it’s just cumbersome to use it as a daily OSS platform, namely there lacks the<br>general ecosystem and the tools are just different. Yes you can install node, java, maven,<br>gradle, and you can probably use powershell to write shell scripts, but at the end of the day,<br>the overall configuration just feels different and since most people don’t use Windows<br>for work on a day-to-day basis, it just takes too much time and effort to learn a set of<br>rules with different flavor, just to get the environment set up.</p>\n<p>However, things have changed.</p>\n<p>The release of WSL (Windows Subsystem on Linux) in Windows 10 was like silent bomb.<br>It wasn’t really marketed to general public, but it implies the fundamental<br>change of attitude from Microsoft towards OSS community.</p>\n<p>WSL is not a virtual machine. In fact there’s no real linux kernel running.<br>Instead, there is a layer in between that translates linux system calls to<br>something that windows kernel can handle. Technically, this is seriously phenomenal,<br>as there’s certain things that there’s no direct equivalent in Windows.</p>\n<p>For example:</p>\n<p>Quoted from <a href=\"https://blogs.msdn.microsoft.com/wsl/2016/06/08/wsl-system-calls/\">MSDN blog</a></p>\n<blockquote>\n<p>The Linux fork syscall has no documented equivalent for Windows.<br>When a fork syscall is made on WSL, lxss.sys does some of the initial work<br>to prepare for copying the process.<br>It then calls internal NT APIs to create the process with the correct semantics<br>and create a thread in the process with an identical register context.<br>Finally, it does some additional work to complete copying the process<br>and resumes the new process so it can begin executing.</p>\n</blockquote>\n<p>And another one regarding <a href=\"https://blogs.msdn.microsoft.com/wsl/2016/06/15/wsl-file-system-support/\">WSL file system</a>:</p>\n<blockquote>\n<p>The Windows Subsystem for Linux must translate various Linux file system operations<br>into NT kernel operations. WSL must provide a place where Linux system files can exist<br>with all the functionality required for that including Linux permissions,<br>symbolic links and other special files such as FIFOs;<br>it must provide access to the Windows volumes on your system;<br>and it must provide special file systems such as ProcFs.</p>\n</blockquote>\n<p>And now it even supports <a href=\"https://docs.microsoft.com/en-us/windows/wsl/interop\">interop</a><br>after the Fall Creators update. This means if you type in <code>notepad.exe</code>,<br>it would literally open notepad for you. Not very exciting but beyond that you could<br>do</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">copy stuff to clipboard</span></span><br><span class=\"line\">echo &#x27;foo bar&#x27; | clip.exe</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">open a file <span class=\"keyword\">in</span> windows using default associated program</span></span><br><span class=\"line\">cmd.exe /C start image.png</span><br></pre></td></tr></table></figure>\n\n<p><strong>Awesome, but what’s our original topic?</strong></p>\n<span id=\"more\"></span>\n\n<h2 id=\"Enable-WSL\"><a href=\"#Enable-WSL\" class=\"headerlink\" title=\"Enable WSL\"></a>Enable WSL</h2><p>Ok, to use WSL, you’d need to enable it first.</p>\n<p><em>// Make sure you are using Windows 10 :)</em></p>\n<p>If you are running windows <strong>earlier than 1709</strong>, the setup would be like this:</p>\n<ol>\n<li><p>Press Windows key. Type in “Turn windows features on or off” and click on the option that shows up.</p>\n</li>\n<li><p>Turn on WSL</p>\n <img src=\"/2018/05/13/Building-Linux-Workspace-on-Windows-10-via-WSL/WSL.png\" class=\"\">\n</li>\n<li><p>Reboot system. You will now have “Bash on Ubuntu on Windows” in your Start Menu.</p>\n</li>\n</ol>\n<p>If you are running version <strong>&gt;= 1709</strong>, just search for “Ubuntu”<br>or other available distributions like “Debian” or “OpenSUSE” in Microsoft Store.</p>\n<h2 id=\"Throw-Away-Cmd\"><a href=\"#Throw-Away-Cmd\" class=\"headerlink\" title=\"Throw Away Cmd\"></a>Throw Away Cmd</h2><p>The long lived cmd.exe in windows is not something people would generally like.</p>\n<p>If you want to have something close to iTerm2 on Mac or Yakuake on KDE,<br>you can try <a href=\"https://conemu.github.io/\">ConEmu</a> (see title image).</p>\n<p>ConEmu allows for creating tabs (Win + Shift + W),<br>and splitting windows (Ctrl + Shift + O/E). Alternatively you can use tmux.</p>\n<p>I personally enable the Quake style which allows me to hit <code>Ctrl + ~</code> to bring it down.</p>\n<p>ConEmu does not seem to have start up with Windows option but you can get around that<br>by putting the shortcut into Startup folder in Start Menu and set the initial window<br>option to be “Minimized”.</p>\n<h2 id=\"Basic-Setup\"><a href=\"#Basic-Setup\" class=\"headerlink\" title=\"Basic Setup\"></a>Basic Setup</h2><h3 id=\"Upgrade-Packages\"><a href=\"#Upgrade-Packages\" class=\"headerlink\" title=\"Upgrade Packages\"></a>Upgrade Packages</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\"><span class=\"built_in\">sudo</span> apt -y update</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\"><span class=\"built_in\">sudo</span> apt -y upgrade</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\"><span class=\"built_in\">sudo</span> apt -y dist-upgrade</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\"><span class=\"built_in\">sudo</span> apt -y autoremove</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Change-to-Favorite-Shell\"><a href=\"#Change-to-Favorite-Shell\" class=\"headerlink\" title=\"Change to Favorite Shell\"></a>Change to Favorite Shell</h3><p>By default most distributions would use bash.</p>\n<p>If you want to use zsh or other shell:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\"><span class=\"built_in\">sudo</span> chsh --shell /bin/zsh &lt;username&gt;</span></span><br></pre></td></tr></table></figure>\n\n<p>or if you want to install oh-my-zsh:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">sh -c <span class=\"string\">&quot;<span class=\"subst\">$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)</span>&quot;</span></span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Mount-Network-Drives\"><a href=\"#Mount-Network-Drives\" class=\"headerlink\" title=\"Mount Network Drives\"></a>Mount Network Drives</h3><p>Ubuntu on windows should have mounted all local disks for you under <code>/mnt</code>.<br>In case you need to mount a network drive like NAS:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\"><span class=\"built_in\">sudo</span> mount -t drvfs Z: /mnt/z</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Some-Advanced-Tweaks-You-Might-Want\"><a href=\"#Some-Advanced-Tweaks-You-Might-Want\" class=\"headerlink\" title=\"Some Advanced Tweaks You Might Want\"></a>Some Advanced Tweaks You Might Want</h2><h3 id=\"Disabling-bell\"><a href=\"#Disabling-bell\" class=\"headerlink\" title=\"Disabling bell\"></a>Disabling bell</h3><p>By default you’ll hear an annoying “dang” every time you tab complete.<br>That is associated with the “Critical Stop” sound config in windows.<br>To stop it is shell specific but generally:</p>\n<p>For bash, put this into <code>.bashrc</code></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">set</span> bell-style none</span><br></pre></td></tr></table></figure>\n\n<p>For zsh, this works for me in <code>.zshrc</code></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">unsetopt</span> beep</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Stop-Windows-From-Sharing-PATH\"><a href=\"#Stop-Windows-From-Sharing-PATH\" class=\"headerlink\" title=\"Stop Windows From Sharing PATH\"></a>Stop Windows From Sharing PATH</h3><p>By default Windows would copy PATH variables to WSL. This could be quite annoying<br>if you use tools like pyenv, rvm or nvm and you also have python, ruby, or node installed<br>in Windows as typically the stuff from Windows will take priority.</p>\n<p>To fix that you can create a DWORD <code>AppendNtPath</code> under<br><code>HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Lxss</code> and set the value to <code>0</code>.</p>\n<p><strong>Update:</strong> The above trick seems to only work for legacy WSL (aka Bash on Ubuntu on Windows).<br>For new users, create a DWORD <code>DistributionFlags</code> with value <code>fffffffd</code> under <code>HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\LxssManager</code>.<br>Then go to Services and restart the LxssManager service.</p>\n<p>This is undocumented but described in <a href=\"https://github.com/Microsoft/WSL/issues/2048\">WSL#2048</a>.</p>\n<h3 id=\"Allow-Linux-to-Open-File-Using-Preferred-Application-in-Windows\"><a href=\"#Allow-Linux-to-Open-File-Using-Preferred-Application-in-Windows\" class=\"headerlink\" title=\"Allow Linux to Open File Using Preferred Application in Windows\"></a>Allow Linux to Open File Using Preferred Application in Windows</h3><p>This is quite useful if you just want to say check an image as WSL is not shipped with a desktop.<br>By default the <code>xdg-open</code> binary can be used in linux but WSL is not shipped with that either.</p>\n<p>Luckily as WSL supports interop, there’s a trick:</p>\n<ol>\n<li><p>Create a file <code>/usr/bin/xdg-open</code> with content</p>\n <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/usr/bin/env sh</span></span><br><span class=\"line\">/mnt/c/Windows/System32/cmd.exe /C start <span class=\"string\">&quot;<span class=\"variable\">$1</span>&quot;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Make it executable:</p>\n <figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\"><span class=\"built_in\">sudo</span> <span class=\"built_in\">chmod</span> +x /usr/bin/xdg-open</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Use it:</p>\n <figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">xdg-open a.png</span></span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<h3 id=\"Call-VSCode-from-Linux\"><a href=\"#Call-VSCode-from-Linux\" class=\"headerlink\" title=\"Call VSCode from Linux\"></a>Call VSCode from Linux</h3><p>You can technically get everything done with vi, but sometimes it’s easier to just use a GUI.</p>\n<p>For VSCode Mac version there’s an option to install it to terminal, but it just doesn’t exist in Windows.</p>\n<p>But with WSL interop, just put this in shell rc file:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">alias</span> code /mnt/c/Program\\ Files/Microsoft\\ VS\\ Code/Code.exe</span><br></pre></td></tr></table></figure>\n\n<p>To use it:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">code path/to/my/file</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Install-Python-Node-Ruby\"><a href=\"#Install-Python-Node-Ruby\" class=\"headerlink\" title=\"Install Python/Node/Ruby\"></a>Install Python/Node/Ruby</h3><p>I use pyenv and it’s the same as if you are in normal linux:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | zsh</span></span><br><span class=\"line\">pyenv install 3.6.5</span><br><span class=\"line\">pyenv shell 3.6.5</span><br></pre></td></tr></table></figure>\n\n<p>Similar solution can be used for node and ruby using nvm and rvm respectively.</p>\n","thumbnailImageUrl":null,"excerpt":"<img src=\"/2018/05/13/Building-Linux-Workspace-on-Windows-10-via-WSL/title.png\" class=\"\">\n\n<p>I was not really a fan of Windows 10,<br>let alone Microsoft decided to ditch the most important feature I liked in Windows 7 - Aero.<br>In fact, I’d admit that in most cases I use Windows as an entertainment system<br>rather than a working platform.</p>\n<p>Don’t get me wrong, Windows is great, both in terms of the quality of the software<br>and the design/usability of the system by itself. It’s also particularly great of you are<br>a .NET developer, a webmaster using IIS, or a game developer heavily using DirectX.<br>However, it’s just cumbersome to use it as a daily OSS platform, namely there lacks the<br>general ecosystem and the tools are just different. Yes you can install node, java, maven,<br>gradle, and you can probably use powershell to write shell scripts, but at the end of the day,<br>the overall configuration just feels different and since most people don’t use Windows<br>for work on a day-to-day basis, it just takes too much time and effort to learn a set of<br>rules with different flavor, just to get the environment set up.</p>\n<p>However, things have changed.</p>\n<p>The release of WSL (Windows Subsystem on Linux) in Windows 10 was like silent bomb.<br>It wasn’t really marketed to general public, but it implies the fundamental<br>change of attitude from Microsoft towards OSS community.</p>\n<p>WSL is not a virtual machine. In fact there’s no real linux kernel running.<br>Instead, there is a layer in between that translates linux system calls to<br>something that windows kernel can handle. Technically, this is seriously phenomenal,<br>as there’s certain things that there’s no direct equivalent in Windows.</p>\n<p>For example:</p>\n<p>Quoted from <a href=\"https://blogs.msdn.microsoft.com/wsl/2016/06/08/wsl-system-calls/\">MSDN blog</a></p>\n<blockquote>\n<p>The Linux fork syscall has no documented equivalent for Windows.<br>When a fork syscall is made on WSL, lxss.sys does some of the initial work<br>to prepare for copying the process.<br>It then calls internal NT APIs to create the process with the correct semantics<br>and create a thread in the process with an identical register context.<br>Finally, it does some additional work to complete copying the process<br>and resumes the new process so it can begin executing.</p>\n</blockquote>\n<p>And another one regarding <a href=\"https://blogs.msdn.microsoft.com/wsl/2016/06/15/wsl-file-system-support/\">WSL file system</a>:</p>\n<blockquote>\n<p>The Windows Subsystem for Linux must translate various Linux file system operations<br>into NT kernel operations. WSL must provide a place where Linux system files can exist<br>with all the functionality required for that including Linux permissions,<br>symbolic links and other special files such as FIFOs;<br>it must provide access to the Windows volumes on your system;<br>and it must provide special file systems such as ProcFs.</p>\n</blockquote>\n<p>And now it even supports <a href=\"https://docs.microsoft.com/en-us/windows/wsl/interop\">interop</a><br>after the Fall Creators update. This means if you type in <code>notepad.exe</code>,<br>it would literally open notepad for you. Not very exciting but beyond that you could<br>do</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">copy stuff to clipboard</span></span><br><span class=\"line\">echo &#x27;foo bar&#x27; | clip.exe</span><br><span class=\"line\"><span class=\"meta prompt_\"></span></span><br><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">open a file <span class=\"keyword\">in</span> windows using default associated program</span></span><br><span class=\"line\">cmd.exe /C start image.png</span><br></pre></td></tr></table></figure>\n\n<p><strong>Awesome, but what’s our original topic?</strong></p>","more":"<h2 id=\"Enable-WSL\"><a href=\"#Enable-WSL\" class=\"headerlink\" title=\"Enable WSL\"></a>Enable WSL</h2><p>Ok, to use WSL, you’d need to enable it first.</p>\n<p><em>// Make sure you are using Windows 10 :)</em></p>\n<p>If you are running windows <strong>earlier than 1709</strong>, the setup would be like this:</p>\n<ol>\n<li><p>Press Windows key. Type in “Turn windows features on or off” and click on the option that shows up.</p>\n</li>\n<li><p>Turn on WSL</p>\n <img src=\"/2018/05/13/Building-Linux-Workspace-on-Windows-10-via-WSL/WSL.png\" class=\"\">\n</li>\n<li><p>Reboot system. You will now have “Bash on Ubuntu on Windows” in your Start Menu.</p>\n</li>\n</ol>\n<p>If you are running version <strong>&gt;= 1709</strong>, just search for “Ubuntu”<br>or other available distributions like “Debian” or “OpenSUSE” in Microsoft Store.</p>\n<h2 id=\"Throw-Away-Cmd\"><a href=\"#Throw-Away-Cmd\" class=\"headerlink\" title=\"Throw Away Cmd\"></a>Throw Away Cmd</h2><p>The long lived cmd.exe in windows is not something people would generally like.</p>\n<p>If you want to have something close to iTerm2 on Mac or Yakuake on KDE,<br>you can try <a href=\"https://conemu.github.io/\">ConEmu</a> (see title image).</p>\n<p>ConEmu allows for creating tabs (Win + Shift + W),<br>and splitting windows (Ctrl + Shift + O/E). Alternatively you can use tmux.</p>\n<p>I personally enable the Quake style which allows me to hit <code>Ctrl + ~</code> to bring it down.</p>\n<p>ConEmu does not seem to have start up with Windows option but you can get around that<br>by putting the shortcut into Startup folder in Start Menu and set the initial window<br>option to be “Minimized”.</p>\n<h2 id=\"Basic-Setup\"><a href=\"#Basic-Setup\" class=\"headerlink\" title=\"Basic Setup\"></a>Basic Setup</h2><h3 id=\"Upgrade-Packages\"><a href=\"#Upgrade-Packages\" class=\"headerlink\" title=\"Upgrade Packages\"></a>Upgrade Packages</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\"><span class=\"built_in\">sudo</span> apt -y update</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\"><span class=\"built_in\">sudo</span> apt -y upgrade</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\"><span class=\"built_in\">sudo</span> apt -y dist-upgrade</span></span><br><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\"><span class=\"built_in\">sudo</span> apt -y autoremove</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Change-to-Favorite-Shell\"><a href=\"#Change-to-Favorite-Shell\" class=\"headerlink\" title=\"Change to Favorite Shell\"></a>Change to Favorite Shell</h3><p>By default most distributions would use bash.</p>\n<p>If you want to use zsh or other shell:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\"><span class=\"built_in\">sudo</span> chsh --shell /bin/zsh &lt;username&gt;</span></span><br></pre></td></tr></table></figure>\n\n<p>or if you want to install oh-my-zsh:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">sh -c <span class=\"string\">&quot;<span class=\"subst\">$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)</span>&quot;</span></span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Mount-Network-Drives\"><a href=\"#Mount-Network-Drives\" class=\"headerlink\" title=\"Mount Network Drives\"></a>Mount Network Drives</h3><p>Ubuntu on windows should have mounted all local disks for you under <code>/mnt</code>.<br>In case you need to mount a network drive like NAS:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\"><span class=\"built_in\">sudo</span> mount -t drvfs Z: /mnt/z</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Some-Advanced-Tweaks-You-Might-Want\"><a href=\"#Some-Advanced-Tweaks-You-Might-Want\" class=\"headerlink\" title=\"Some Advanced Tweaks You Might Want\"></a>Some Advanced Tweaks You Might Want</h2><h3 id=\"Disabling-bell\"><a href=\"#Disabling-bell\" class=\"headerlink\" title=\"Disabling bell\"></a>Disabling bell</h3><p>By default you’ll hear an annoying “dang” every time you tab complete.<br>That is associated with the “Critical Stop” sound config in windows.<br>To stop it is shell specific but generally:</p>\n<p>For bash, put this into <code>.bashrc</code></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">set</span> bell-style none</span><br></pre></td></tr></table></figure>\n\n<p>For zsh, this works for me in <code>.zshrc</code></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">unsetopt</span> beep</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Stop-Windows-From-Sharing-PATH\"><a href=\"#Stop-Windows-From-Sharing-PATH\" class=\"headerlink\" title=\"Stop Windows From Sharing PATH\"></a>Stop Windows From Sharing PATH</h3><p>By default Windows would copy PATH variables to WSL. This could be quite annoying<br>if you use tools like pyenv, rvm or nvm and you also have python, ruby, or node installed<br>in Windows as typically the stuff from Windows will take priority.</p>\n<p>To fix that you can create a DWORD <code>AppendNtPath</code> under<br><code>HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Lxss</code> and set the value to <code>0</code>.</p>\n<p><strong>Update:</strong> The above trick seems to only work for legacy WSL (aka Bash on Ubuntu on Windows).<br>For new users, create a DWORD <code>DistributionFlags</code> with value <code>fffffffd</code> under <code>HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\LxssManager</code>.<br>Then go to Services and restart the LxssManager service.</p>\n<p>This is undocumented but described in <a href=\"https://github.com/Microsoft/WSL/issues/2048\">WSL#2048</a>.</p>\n<h3 id=\"Allow-Linux-to-Open-File-Using-Preferred-Application-in-Windows\"><a href=\"#Allow-Linux-to-Open-File-Using-Preferred-Application-in-Windows\" class=\"headerlink\" title=\"Allow Linux to Open File Using Preferred Application in Windows\"></a>Allow Linux to Open File Using Preferred Application in Windows</h3><p>This is quite useful if you just want to say check an image as WSL is not shipped with a desktop.<br>By default the <code>xdg-open</code> binary can be used in linux but WSL is not shipped with that either.</p>\n<p>Luckily as WSL supports interop, there’s a trick:</p>\n<ol>\n<li><p>Create a file <code>/usr/bin/xdg-open</code> with content</p>\n <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/usr/bin/env sh</span></span><br><span class=\"line\">/mnt/c/Windows/System32/cmd.exe /C start <span class=\"string\">&quot;<span class=\"variable\">$1</span>&quot;</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Make it executable:</p>\n <figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\"><span class=\"built_in\">sudo</span> <span class=\"built_in\">chmod</span> +x /usr/bin/xdg-open</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>Use it:</p>\n <figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">xdg-open a.png</span></span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<h3 id=\"Call-VSCode-from-Linux\"><a href=\"#Call-VSCode-from-Linux\" class=\"headerlink\" title=\"Call VSCode from Linux\"></a>Call VSCode from Linux</h3><p>You can technically get everything done with vi, but sometimes it’s easier to just use a GUI.</p>\n<p>For VSCode Mac version there’s an option to install it to terminal, but it just doesn’t exist in Windows.</p>\n<p>But with WSL interop, just put this in shell rc file:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">alias</span> code /mnt/c/Program\\ Files/Microsoft\\ VS\\ Code/Code.exe</span><br></pre></td></tr></table></figure>\n\n<p>To use it:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">code path/to/my/file</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Install-Python-Node-Ruby\"><a href=\"#Install-Python-Node-Ruby\" class=\"headerlink\" title=\"Install Python/Node/Ruby\"></a>Install Python/Node/Ruby</h3><p>I use pyenv and it’s the same as if you are in normal linux:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\">$ </span><span class=\"language-bash\">curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | zsh</span></span><br><span class=\"line\">pyenv install 3.6.5</span><br><span class=\"line\">pyenv shell 3.6.5</span><br></pre></td></tr></table></figure>\n\n<p>Similar solution can be used for node and ruby using nvm and rvm respectively.</p>"},{"title":"Make Wireless BackHaul Great Again: Disable Orbi 2.4G Backhaul","date":"2020-04-05T04:14:57.000Z","_content":"\n## TL;DR;\nI was able to \"break\" Orbi's 2.4G backhaul fallback\nand hence force it onto 5G.\n\nSo instead of this\n\n<img src=\"{% asset_path slowspeed.png %}\" style=\"width: 400px\" />\n\nI got this:\n\n<img src=\"{% asset_path speed.png %}\" style=\"width: 400px\" />\n\nThe solution isn't clean. Basically telnet to the satellite and then\n```console\nroot@RBS20:/# config set wlg_sta_ssid=<original ssid>_disabled\nroot@RBS20:/# config commit \n```\n\nThen reboot.\n\nMore details and investigation<br>\n↓\n\n<!-- more -->\n\n## Background\n\nA while back, I got the Netgear Orbi [RBK20](https://www.netgear.com/support/product/RBK20.aspx)\nto replace my original not so reliable powerline setup.\n\n{% asset_img RBK20.png %}\n\nIt's a wifi mesh solution that aims to solve the problem\nwhere traditional single pointer router fails -\nto provide good signal coverage\nin complex indoor environment with walls in between rooms.\nThis is esp. important in the AC world where\nthe 5G signal falls short as it tends to be more easily blocked/\nthan 2.4G.\n\nOne thing worth mentioning though is that Orbi isn't\na true mesh system, that is, to support 802.11s mesh standard.\nA true mesh system allows satellites to cooperate together\nthrough a routing algorithm to find the optimal data transfer path\nbut Orbi basically supports only star topology or daisy chaining.\nThat said though, in practice, some of the \"advanced\" true mesh systems\non the other hand suffer from lack of dedicated backhaul channel\nfor inter satellite uplink (e.g. Google Wifi) and hence actually\nperform much worse in reality as they essentially just become\nmore advanced repeaters.\n\n## Problem\n\nIn order for Orbi to have max power, we need to\nensure that Orbi has the dedicated backhaul operate in 5G mode,\notherwise anything connected to satellites would be bound to\n2.4G speed which is 192Mbps.\n\nIt's not unusable technically but it sort of defeats the purpose.\nWhat makes things worse is that Netgear decides on a strategy\nthat if 5G backhaul isn't stable, it would fall back onto 2.4G,\nwhich isn't dedicated but shared with regular 2.4G radio.\nThis isn't a bad idea per se as home layout may be complex so\nthey sacrifice throughtput in favor of usability in certain\nscenarios.\n\nHowever this setup doesn't work well in practice.\nFor whatever reason, the system would fall back onto 2.4G\nrandomly even if 5G signal is perfectly fine. And the only\nway to fix that would be to power cycle the satellite.\n\n## Investigation\n\nSince Orbi is OpenWRT based, I took a look at the configs\nthat potentially control the behavior and here they are:\n\n```console\nroot@RBS20:/# uci show | grep WiFiLink\n...\nrepacd.WiFiLink.RSSIThresholdFar='-75'\nrepacd.WiFiLink.RSSIThresholdFar5g='-82'\nrepacd.WiFiLink.RSSIThresholdFar24g='-76'\nrepacd.WiFiLink.RSSIThresholdNear='-60'\nrepacd.WiFiLink.RSSIThresholdMin='-75'\nrepacd.WiFiLink.RSSIThresholdPrefer2GBackhaul='-82'\nrepacd.WiFiLink.2GBackhaulSwitchDownTime='10'\nrepacd.WiFiLink.MaxMeasuringStateAttempts='30'\nrepacd.WiFiLink.DaisyChain='1'\nrepacd.WiFiLink.RateNumMeasurements='5'\nrepacd.WiFiLink.RateThresholdMin5GInPercent='35'\nrepacd.WiFiLink.RateThresholdMax5GInPercent='95'\nrepacd.WiFiLink.RateThresholdPrefer2GBackhaulInPercent='5'\nrepacd.WiFiLink.5GBackhaulBadlinkTimeout='60'\nrepacd.WiFiLink.BSSIDAssociationTimeout='170'\nrepacd.WiFiLink.RateScalingFactor='85'\nrepacd.WiFiLink.5GBackhaulEvalTimeShort='330'\nrepacd.WiFiLink.5GBackhaulEvalTimeLong='1800'\nrepacd.WiFiLink.2GBackhaulEvalTime='1800'\n...\n```\n\nSo in theory 2G backhaul should only be preferred\nafter signal drops below -82dbm or rate is below 5%/35%.\nHowever in practice this almost is never the case.\n\n```console\n# From router side:\nroot@RBR20:/# wlanconfig ath11 list\nADDR               AID CHAN TXRATE RXRATE RSSI MINRSSI MAXRSSI IDLE  TXSEQ  RXSEQ  CAPS        ACAPS\n ERP    STATE MAXRATE(DOT11) HTCAPS ASSOCTIME    IEs   MODE                   PSMODE\n                     1  157 866M    780M   51      44      58    0      0   65535   EPs         0\n   b              0           AWPSM 18:46:39     RSN WME IEEE80211_MODE_11AC_VHT80   0\n```\n\nRSSI here is 51 which indicates roughly -44dbm\nwhich is perfectly fine. It's possible that the number would\nfluctuate during operation but as you see the min value\nis still way above the threshold. Also in practice,\nonce Orbi picks 2.4G it will stick there which doesn't make sense\nbut since Orbi's software isn't open sourced there's no way to know\nexactly how it messes things up (looking at you Netgear developers).\n\n## A Hacky Solution\n\n### Enable Telnet\nGo to http://&lt;satellite ip&gt;/debug.htm, log in with router username & password\nand check \"Enable Telnet\".\n\n{% asset_img telnet.png %}\n\n### Initial Attempt\nSince we cannot alter Netgear's software behavior directly,\nwe have to somehow break the 2.4G connection to force it onto 5G.\n\nMy initial thought was to disable the 2.4G wifi interface.\n(Spoiler: this does not work somehow).\n\nThis is doable via OpenWRT's config system.\n\n```console\n# Show all wifi interfaces. The 2.4G is already disabled here but in normal operation it will show something.\nroot@RBS20:/# iwconfig\nath01     IEEE 802.11b  ESSID:\"NETGEAR_ORBI_hidden52\"\n          Mode:Managed  Frequency:2.412 GHz  Access Point: Not-Associated\n          Bit Rate:0 kb/s   Tx-Power:25 dBm\n          RTS thr:off   Fragment thr:off\n          Power Management:off\n          Link Quality=0/94  Signal level=-95 dBm  Noise level=-95 dBm\n          Rx invalid nwid:0  Rx invalid crypt:0  Rx invalid frag:0\n          Tx excessive retries:0  Invalid misc:0   Missed beacon:0\n\nroot@RBS20:/# vi /etc/config/wireless\nconfig wifi-iface\n        option device 'wifi0'\n        option network '0'\n        option bridge 'br0'\n        option mode 'sta'\n        option wds '1'\n        option athnewind '1'\n        option vap_ind '0'\n        option backhaul '1'\n        option wsplcd_unmanaged '1'\n        option repacd_security_unmanaged '1'\n        option dropmdns '0'\n        option ssid 'NETGEAR_ORBI_hidden52'\n        option encryption 'psk2+ccmp'\n        option ifname 'ath01'\n        option wps_config 'virtual_push_button physical_push_button'\n        option wps_pbc '1'\n        option dyn_bw_rts '0'\n        option disabled '0'\n```\n\n^ From there we can see `ath01` is bound to physical interface `wifi0`.\nWe can just change the disabled value from 0 to 1.\nThen `uci commit` will persist the value.\n\nHowever for some reason, Orbi will restore values in OpenWRT config,\npresumably for system integrity reason. YMMV but at least this does\nnot work for RBK20 system.\n\n### The Hacky Attempt\nWhile Orbi restores uci configs, it does keep its own configs\nthat are alterable via `config`.\n\nThen essentially the solution becomes that we can change the 2.4G\nbackhaul SSID such that it cannot find the right one.\n\nIn my system the original SSID is `NETGEAR_ORBI_hidden52` from\n`iwconfig`.\nWe can grab all related configs from\n\n```console\nroot@RBS20:/# config show | grep hidden52\nwlg_ap_bh_ssid=NETGEAR_ORBI_hidden52\nwla_ap_bh_ssid=NETGEAR_ORBI_hidden52\nwla_sta_ssid=NETGEAR_ORBI_hidden52\nwlg_sta_ssid=NETGEAR_ORBI_hidden52\n```\n\nHere anything starts with `wla` is 5G and `wlg` is 2.4G.\n\nThen we can do\n\n```console\nroot@RBS20:/# config set wlg_sta_ssid=NETGEAR_ORBI_hidden52_disabled\nroot@RBS20:/# config commit\n```\n\nThe `_disabled` suffix is arbitrary with the idea to prevent\nOrbi from connecting to router via 2.4G.\n\n`wlg_ap_bh_ssid` does not need to be set. The `ap` setup will\nautomatically follow the `sta` setup.\n\n`nvram show | grep ssid` should now show the persisted value.\n\nIn RBS20, if you use `reboot` to restart the system,\nit will somehow get stuck with pink led light.\nSo I always just power cycle it, which works just fine.\n\nTo confirm this works, on satellite side, once restarted,\n`iwconfig` should show the new SSID and signal strength should be\n-95dbm indicating it's disconnected.\n\nEnable Telnet in router and log into it. \n```\n# ath01 is 2.4G wifi interface. It should have nothing connected now.\nroot@RBR20:/# wlanconfig ath01 list\n# ath11 is 5G. It should have satellite(s) connected.\nroot@RBR20:/# wlanconfig ath11 list\nADDR               AID CHAN TXRATE RXRATE RSSI MINRSSI MAXRSSI IDLE  TXSEQ  RXSEQ  CAPS        ACAPS\n ERP    STATE MAXRATE(DOT11) HTCAPS ASSOCTIME    IEs   MODE                   PSMODE\n                     1  157 866M    780M   51      44      58    0      0   65535   EPs         0\n   b              0           AWPSM 19:26:27     RSN WME IEEE80211_MODE_11AC_VHT80   0\n```\n","source":"_posts/Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul.md","raw":"---\ntitle: 'Make Wireless BackHaul Great Again: Disable Orbi 2.4G Backhaul'\ndate: 2020-04-04 21:14:57\ntags: wifi, openwrt\n---\n\n## TL;DR;\nI was able to \"break\" Orbi's 2.4G backhaul fallback\nand hence force it onto 5G.\n\nSo instead of this\n\n<img src=\"{% asset_path slowspeed.png %}\" style=\"width: 400px\" />\n\nI got this:\n\n<img src=\"{% asset_path speed.png %}\" style=\"width: 400px\" />\n\nThe solution isn't clean. Basically telnet to the satellite and then\n```console\nroot@RBS20:/# config set wlg_sta_ssid=<original ssid>_disabled\nroot@RBS20:/# config commit \n```\n\nThen reboot.\n\nMore details and investigation<br>\n↓\n\n<!-- more -->\n\n## Background\n\nA while back, I got the Netgear Orbi [RBK20](https://www.netgear.com/support/product/RBK20.aspx)\nto replace my original not so reliable powerline setup.\n\n{% asset_img RBK20.png %}\n\nIt's a wifi mesh solution that aims to solve the problem\nwhere traditional single pointer router fails -\nto provide good signal coverage\nin complex indoor environment with walls in between rooms.\nThis is esp. important in the AC world where\nthe 5G signal falls short as it tends to be more easily blocked/\nthan 2.4G.\n\nOne thing worth mentioning though is that Orbi isn't\na true mesh system, that is, to support 802.11s mesh standard.\nA true mesh system allows satellites to cooperate together\nthrough a routing algorithm to find the optimal data transfer path\nbut Orbi basically supports only star topology or daisy chaining.\nThat said though, in practice, some of the \"advanced\" true mesh systems\non the other hand suffer from lack of dedicated backhaul channel\nfor inter satellite uplink (e.g. Google Wifi) and hence actually\nperform much worse in reality as they essentially just become\nmore advanced repeaters.\n\n## Problem\n\nIn order for Orbi to have max power, we need to\nensure that Orbi has the dedicated backhaul operate in 5G mode,\notherwise anything connected to satellites would be bound to\n2.4G speed which is 192Mbps.\n\nIt's not unusable technically but it sort of defeats the purpose.\nWhat makes things worse is that Netgear decides on a strategy\nthat if 5G backhaul isn't stable, it would fall back onto 2.4G,\nwhich isn't dedicated but shared with regular 2.4G radio.\nThis isn't a bad idea per se as home layout may be complex so\nthey sacrifice throughtput in favor of usability in certain\nscenarios.\n\nHowever this setup doesn't work well in practice.\nFor whatever reason, the system would fall back onto 2.4G\nrandomly even if 5G signal is perfectly fine. And the only\nway to fix that would be to power cycle the satellite.\n\n## Investigation\n\nSince Orbi is OpenWRT based, I took a look at the configs\nthat potentially control the behavior and here they are:\n\n```console\nroot@RBS20:/# uci show | grep WiFiLink\n...\nrepacd.WiFiLink.RSSIThresholdFar='-75'\nrepacd.WiFiLink.RSSIThresholdFar5g='-82'\nrepacd.WiFiLink.RSSIThresholdFar24g='-76'\nrepacd.WiFiLink.RSSIThresholdNear='-60'\nrepacd.WiFiLink.RSSIThresholdMin='-75'\nrepacd.WiFiLink.RSSIThresholdPrefer2GBackhaul='-82'\nrepacd.WiFiLink.2GBackhaulSwitchDownTime='10'\nrepacd.WiFiLink.MaxMeasuringStateAttempts='30'\nrepacd.WiFiLink.DaisyChain='1'\nrepacd.WiFiLink.RateNumMeasurements='5'\nrepacd.WiFiLink.RateThresholdMin5GInPercent='35'\nrepacd.WiFiLink.RateThresholdMax5GInPercent='95'\nrepacd.WiFiLink.RateThresholdPrefer2GBackhaulInPercent='5'\nrepacd.WiFiLink.5GBackhaulBadlinkTimeout='60'\nrepacd.WiFiLink.BSSIDAssociationTimeout='170'\nrepacd.WiFiLink.RateScalingFactor='85'\nrepacd.WiFiLink.5GBackhaulEvalTimeShort='330'\nrepacd.WiFiLink.5GBackhaulEvalTimeLong='1800'\nrepacd.WiFiLink.2GBackhaulEvalTime='1800'\n...\n```\n\nSo in theory 2G backhaul should only be preferred\nafter signal drops below -82dbm or rate is below 5%/35%.\nHowever in practice this almost is never the case.\n\n```console\n# From router side:\nroot@RBR20:/# wlanconfig ath11 list\nADDR               AID CHAN TXRATE RXRATE RSSI MINRSSI MAXRSSI IDLE  TXSEQ  RXSEQ  CAPS        ACAPS\n ERP    STATE MAXRATE(DOT11) HTCAPS ASSOCTIME    IEs   MODE                   PSMODE\n                     1  157 866M    780M   51      44      58    0      0   65535   EPs         0\n   b              0           AWPSM 18:46:39     RSN WME IEEE80211_MODE_11AC_VHT80   0\n```\n\nRSSI here is 51 which indicates roughly -44dbm\nwhich is perfectly fine. It's possible that the number would\nfluctuate during operation but as you see the min value\nis still way above the threshold. Also in practice,\nonce Orbi picks 2.4G it will stick there which doesn't make sense\nbut since Orbi's software isn't open sourced there's no way to know\nexactly how it messes things up (looking at you Netgear developers).\n\n## A Hacky Solution\n\n### Enable Telnet\nGo to http://&lt;satellite ip&gt;/debug.htm, log in with router username & password\nand check \"Enable Telnet\".\n\n{% asset_img telnet.png %}\n\n### Initial Attempt\nSince we cannot alter Netgear's software behavior directly,\nwe have to somehow break the 2.4G connection to force it onto 5G.\n\nMy initial thought was to disable the 2.4G wifi interface.\n(Spoiler: this does not work somehow).\n\nThis is doable via OpenWRT's config system.\n\n```console\n# Show all wifi interfaces. The 2.4G is already disabled here but in normal operation it will show something.\nroot@RBS20:/# iwconfig\nath01     IEEE 802.11b  ESSID:\"NETGEAR_ORBI_hidden52\"\n          Mode:Managed  Frequency:2.412 GHz  Access Point: Not-Associated\n          Bit Rate:0 kb/s   Tx-Power:25 dBm\n          RTS thr:off   Fragment thr:off\n          Power Management:off\n          Link Quality=0/94  Signal level=-95 dBm  Noise level=-95 dBm\n          Rx invalid nwid:0  Rx invalid crypt:0  Rx invalid frag:0\n          Tx excessive retries:0  Invalid misc:0   Missed beacon:0\n\nroot@RBS20:/# vi /etc/config/wireless\nconfig wifi-iface\n        option device 'wifi0'\n        option network '0'\n        option bridge 'br0'\n        option mode 'sta'\n        option wds '1'\n        option athnewind '1'\n        option vap_ind '0'\n        option backhaul '1'\n        option wsplcd_unmanaged '1'\n        option repacd_security_unmanaged '1'\n        option dropmdns '0'\n        option ssid 'NETGEAR_ORBI_hidden52'\n        option encryption 'psk2+ccmp'\n        option ifname 'ath01'\n        option wps_config 'virtual_push_button physical_push_button'\n        option wps_pbc '1'\n        option dyn_bw_rts '0'\n        option disabled '0'\n```\n\n^ From there we can see `ath01` is bound to physical interface `wifi0`.\nWe can just change the disabled value from 0 to 1.\nThen `uci commit` will persist the value.\n\nHowever for some reason, Orbi will restore values in OpenWRT config,\npresumably for system integrity reason. YMMV but at least this does\nnot work for RBK20 system.\n\n### The Hacky Attempt\nWhile Orbi restores uci configs, it does keep its own configs\nthat are alterable via `config`.\n\nThen essentially the solution becomes that we can change the 2.4G\nbackhaul SSID such that it cannot find the right one.\n\nIn my system the original SSID is `NETGEAR_ORBI_hidden52` from\n`iwconfig`.\nWe can grab all related configs from\n\n```console\nroot@RBS20:/# config show | grep hidden52\nwlg_ap_bh_ssid=NETGEAR_ORBI_hidden52\nwla_ap_bh_ssid=NETGEAR_ORBI_hidden52\nwla_sta_ssid=NETGEAR_ORBI_hidden52\nwlg_sta_ssid=NETGEAR_ORBI_hidden52\n```\n\nHere anything starts with `wla` is 5G and `wlg` is 2.4G.\n\nThen we can do\n\n```console\nroot@RBS20:/# config set wlg_sta_ssid=NETGEAR_ORBI_hidden52_disabled\nroot@RBS20:/# config commit\n```\n\nThe `_disabled` suffix is arbitrary with the idea to prevent\nOrbi from connecting to router via 2.4G.\n\n`wlg_ap_bh_ssid` does not need to be set. The `ap` setup will\nautomatically follow the `sta` setup.\n\n`nvram show | grep ssid` should now show the persisted value.\n\nIn RBS20, if you use `reboot` to restart the system,\nit will somehow get stuck with pink led light.\nSo I always just power cycle it, which works just fine.\n\nTo confirm this works, on satellite side, once restarted,\n`iwconfig` should show the new SSID and signal strength should be\n-95dbm indicating it's disconnected.\n\nEnable Telnet in router and log into it. \n```\n# ath01 is 2.4G wifi interface. It should have nothing connected now.\nroot@RBR20:/# wlanconfig ath01 list\n# ath11 is 5G. It should have satellite(s) connected.\nroot@RBR20:/# wlanconfig ath11 list\nADDR               AID CHAN TXRATE RXRATE RSSI MINRSSI MAXRSSI IDLE  TXSEQ  RXSEQ  CAPS        ACAPS\n ERP    STATE MAXRATE(DOT11) HTCAPS ASSOCTIME    IEs   MODE                   PSMODE\n                     1  157 866M    780M   51      44      58    0      0   65535   EPs         0\n   b              0           AWPSM 19:26:27     RSN WME IEEE80211_MODE_11AC_VHT80   0\n```\n","slug":"Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul","published":1,"updated":"2025-09-01T22:26:51.287Z","comments":1,"layout":"post","photos":[],"_id":"cmf1pfvs600078mmg825bdt5d","content":"<h2 id=\"TL-DR\"><a href=\"#TL-DR\" class=\"headerlink\" title=\"TL;DR;\"></a>TL;DR;</h2><p>I was able to “break” Orbi’s 2.4G backhaul fallback<br>and hence force it onto 5G.</p>\n<p>So instead of this</p>\n<img src=\"/2020/04/04/Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul/slowspeed.png\" style=\"width: 400px\" />\n\n<p>I got this:</p>\n<img src=\"/2020/04/04/Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul/speed.png\" style=\"width: 400px\" />\n\n<p>The solution isn’t clean. Basically telnet to the satellite and then</p>\n<figure class=\"highlight console\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@RBS20:/# config set wlg_sta_ssid=&lt;original ssid&gt;_disabled</span><br><span class=\"line\">root@RBS20:/# config commit </span><br></pre></td></tr></table></figure>\n\n<p>Then reboot.</p>\n<p>More details and investigation<br><br>↓</p>\n<span id=\"more\"></span>\n\n<h2 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h2><p>A while back, I got the Netgear Orbi <a href=\"https://www.netgear.com/support/product/RBK20.aspx\">RBK20</a><br>to replace my original not so reliable powerline setup.</p>\n<img src=\"/2020/04/04/Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul/RBK20.png\" class=\"\">\n\n<p>It’s a wifi mesh solution that aims to solve the problem<br>where traditional single pointer router fails -<br>to provide good signal coverage<br>in complex indoor environment with walls in between rooms.<br>This is esp. important in the AC world where<br>the 5G signal falls short as it tends to be more easily blocked/<br>than 2.4G.</p>\n<p>One thing worth mentioning though is that Orbi isn’t<br>a true mesh system, that is, to support 802.11s mesh standard.<br>A true mesh system allows satellites to cooperate together<br>through a routing algorithm to find the optimal data transfer path<br>but Orbi basically supports only star topology or daisy chaining.<br>That said though, in practice, some of the “advanced” true mesh systems<br>on the other hand suffer from lack of dedicated backhaul channel<br>for inter satellite uplink (e.g. Google Wifi) and hence actually<br>perform much worse in reality as they essentially just become<br>more advanced repeaters.</p>\n<h2 id=\"Problem\"><a href=\"#Problem\" class=\"headerlink\" title=\"Problem\"></a>Problem</h2><p>In order for Orbi to have max power, we need to<br>ensure that Orbi has the dedicated backhaul operate in 5G mode,<br>otherwise anything connected to satellites would be bound to<br>2.4G speed which is 192Mbps.</p>\n<p>It’s not unusable technically but it sort of defeats the purpose.<br>What makes things worse is that Netgear decides on a strategy<br>that if 5G backhaul isn’t stable, it would fall back onto 2.4G,<br>which isn’t dedicated but shared with regular 2.4G radio.<br>This isn’t a bad idea per se as home layout may be complex so<br>they sacrifice throughtput in favor of usability in certain<br>scenarios.</p>\n<p>However this setup doesn’t work well in practice.<br>For whatever reason, the system would fall back onto 2.4G<br>randomly even if 5G signal is perfectly fine. And the only<br>way to fix that would be to power cycle the satellite.</p>\n<h2 id=\"Investigation\"><a href=\"#Investigation\" class=\"headerlink\" title=\"Investigation\"></a>Investigation</h2><p>Since Orbi is OpenWRT based, I took a look at the configs<br>that potentially control the behavior and here they are:</p>\n<figure class=\"highlight console\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@RBS20:/# uci show | grep WiFiLink</span><br><span class=\"line\">...</span><br><span class=\"line\">repacd.WiFiLink.RSSIThresholdFar=&#x27;-75&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RSSIThresholdFar5g=&#x27;-82&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RSSIThresholdFar24g=&#x27;-76&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RSSIThresholdNear=&#x27;-60&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RSSIThresholdMin=&#x27;-75&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RSSIThresholdPrefer2GBackhaul=&#x27;-82&#x27;</span><br><span class=\"line\">repacd.WiFiLink.2GBackhaulSwitchDownTime=&#x27;10&#x27;</span><br><span class=\"line\">repacd.WiFiLink.MaxMeasuringStateAttempts=&#x27;30&#x27;</span><br><span class=\"line\">repacd.WiFiLink.DaisyChain=&#x27;1&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RateNumMeasurements=&#x27;5&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RateThresholdMin5GInPercent=&#x27;35&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RateThresholdMax5GInPercent=&#x27;95&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RateThresholdPrefer2GBackhaulInPercent=&#x27;5&#x27;</span><br><span class=\"line\">repacd.WiFiLink.5GBackhaulBadlinkTimeout=&#x27;60&#x27;</span><br><span class=\"line\">repacd.WiFiLink.BSSIDAssociationTimeout=&#x27;170&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RateScalingFactor=&#x27;85&#x27;</span><br><span class=\"line\">repacd.WiFiLink.5GBackhaulEvalTimeShort=&#x27;330&#x27;</span><br><span class=\"line\">repacd.WiFiLink.5GBackhaulEvalTimeLong=&#x27;1800&#x27;</span><br><span class=\"line\">repacd.WiFiLink.2GBackhaulEvalTime=&#x27;1800&#x27;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>So in theory 2G backhaul should only be preferred<br>after signal drops below -82dbm or rate is below 5%/35%.<br>However in practice this almost is never the case.</p>\n<figure class=\"highlight console\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">From router side:</span></span><br><span class=\"line\">root@RBR20:/# wlanconfig ath11 list</span><br><span class=\"line\">ADDR               AID CHAN TXRATE RXRATE RSSI MINRSSI MAXRSSI IDLE  TXSEQ  RXSEQ  CAPS        ACAPS</span><br><span class=\"line\"> ERP    STATE MAXRATE(DOT11) HTCAPS ASSOCTIME    IEs   MODE                   PSMODE</span><br><span class=\"line\">                     1  157 866M    780M   51      44      58    0      0   65535   EPs         0</span><br><span class=\"line\">   b              0           AWPSM 18:46:39     RSN WME IEEE80211_MODE_11AC_VHT80   0</span><br></pre></td></tr></table></figure>\n\n<p>RSSI here is 51 which indicates roughly -44dbm<br>which is perfectly fine. It’s possible that the number would<br>fluctuate during operation but as you see the min value<br>is still way above the threshold. Also in practice,<br>once Orbi picks 2.4G it will stick there which doesn’t make sense<br>but since Orbi’s software isn’t open sourced there’s no way to know<br>exactly how it messes things up (looking at you Netgear developers).</p>\n<h2 id=\"A-Hacky-Solution\"><a href=\"#A-Hacky-Solution\" class=\"headerlink\" title=\"A Hacky Solution\"></a>A Hacky Solution</h2><h3 id=\"Enable-Telnet\"><a href=\"#Enable-Telnet\" class=\"headerlink\" title=\"Enable Telnet\"></a>Enable Telnet</h3><p>Go to http://&lt;satellite ip&gt;/debug.htm, log in with router username &amp; password<br>and check “Enable Telnet”.</p>\n<img src=\"/2020/04/04/Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul/telnet.png\" class=\"\">\n\n<h3 id=\"Initial-Attempt\"><a href=\"#Initial-Attempt\" class=\"headerlink\" title=\"Initial Attempt\"></a>Initial Attempt</h3><p>Since we cannot alter Netgear’s software behavior directly,<br>we have to somehow break the 2.4G connection to force it onto 5G.</p>\n<p>My initial thought was to disable the 2.4G wifi interface.<br>(Spoiler: this does not work somehow).</p>\n<p>This is doable via OpenWRT’s config system.</p>\n<figure class=\"highlight console\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">Show all wifi interfaces. The 2.4G is already disabled here but <span class=\"keyword\">in</span> normal operation it will show something.</span></span><br><span class=\"line\">root@RBS20:/# iwconfig</span><br><span class=\"line\">ath01     IEEE 802.11b  ESSID:&quot;NETGEAR_ORBI_hidden52&quot;</span><br><span class=\"line\">          Mode:Managed  Frequency:2.412 GHz  Access Point: Not-Associated</span><br><span class=\"line\">          Bit Rate:0 kb/s   Tx-Power:25 dBm</span><br><span class=\"line\">          RTS thr:off   Fragment thr:off</span><br><span class=\"line\">          Power Management:off</span><br><span class=\"line\">          Link Quality=0/94  Signal level=-95 dBm  Noise level=-95 dBm</span><br><span class=\"line\">          Rx invalid nwid:0  Rx invalid crypt:0  Rx invalid frag:0</span><br><span class=\"line\">          Tx excessive retries:0  Invalid misc:0   Missed beacon:0</span><br><span class=\"line\"></span><br><span class=\"line\">root@RBS20:/# vi /etc/config/wireless</span><br><span class=\"line\">config wifi-iface</span><br><span class=\"line\">        option device &#x27;wifi0&#x27;</span><br><span class=\"line\">        option network &#x27;0&#x27;</span><br><span class=\"line\">        option bridge &#x27;br0&#x27;</span><br><span class=\"line\">        option mode &#x27;sta&#x27;</span><br><span class=\"line\">        option wds &#x27;1&#x27;</span><br><span class=\"line\">        option athnewind &#x27;1&#x27;</span><br><span class=\"line\">        option vap_ind &#x27;0&#x27;</span><br><span class=\"line\">        option backhaul &#x27;1&#x27;</span><br><span class=\"line\">        option wsplcd_unmanaged &#x27;1&#x27;</span><br><span class=\"line\">        option repacd_security_unmanaged &#x27;1&#x27;</span><br><span class=\"line\">        option dropmdns &#x27;0&#x27;</span><br><span class=\"line\">        option ssid &#x27;NETGEAR_ORBI_hidden52&#x27;</span><br><span class=\"line\">        option encryption &#x27;psk2+ccmp&#x27;</span><br><span class=\"line\">        option ifname &#x27;ath01&#x27;</span><br><span class=\"line\">        option wps_config &#x27;virtual_push_button physical_push_button&#x27;</span><br><span class=\"line\">        option wps_pbc &#x27;1&#x27;</span><br><span class=\"line\">        option dyn_bw_rts &#x27;0&#x27;</span><br><span class=\"line\">        option disabled &#x27;0&#x27;</span><br></pre></td></tr></table></figure>\n\n<p>^ From there we can see <code>ath01</code> is bound to physical interface <code>wifi0</code>.<br>We can just change the disabled value from 0 to 1.<br>Then <code>uci commit</code> will persist the value.</p>\n<p>However for some reason, Orbi will restore values in OpenWRT config,<br>presumably for system integrity reason. YMMV but at least this does<br>not work for RBK20 system.</p>\n<h3 id=\"The-Hacky-Attempt\"><a href=\"#The-Hacky-Attempt\" class=\"headerlink\" title=\"The Hacky Attempt\"></a>The Hacky Attempt</h3><p>While Orbi restores uci configs, it does keep its own configs<br>that are alterable via <code>config</code>.</p>\n<p>Then essentially the solution becomes that we can change the 2.4G<br>backhaul SSID such that it cannot find the right one.</p>\n<p>In my system the original SSID is <code>NETGEAR_ORBI_hidden52</code> from<br><code>iwconfig</code>.<br>We can grab all related configs from</p>\n<figure class=\"highlight console\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@RBS20:/# config show | grep hidden52</span><br><span class=\"line\">wlg_ap_bh_ssid=NETGEAR_ORBI_hidden52</span><br><span class=\"line\">wla_ap_bh_ssid=NETGEAR_ORBI_hidden52</span><br><span class=\"line\">wla_sta_ssid=NETGEAR_ORBI_hidden52</span><br><span class=\"line\">wlg_sta_ssid=NETGEAR_ORBI_hidden52</span><br></pre></td></tr></table></figure>\n\n<p>Here anything starts with <code>wla</code> is 5G and <code>wlg</code> is 2.4G.</p>\n<p>Then we can do</p>\n<figure class=\"highlight console\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@RBS20:/# config set wlg_sta_ssid=NETGEAR_ORBI_hidden52_disabled</span><br><span class=\"line\">root@RBS20:/# config commit</span><br></pre></td></tr></table></figure>\n\n<p>The <code>_disabled</code> suffix is arbitrary with the idea to prevent<br>Orbi from connecting to router via 2.4G.</p>\n<p><code>wlg_ap_bh_ssid</code> does not need to be set. The <code>ap</code> setup will<br>automatically follow the <code>sta</code> setup.</p>\n<p><code>nvram show | grep ssid</code> should now show the persisted value.</p>\n<p>In RBS20, if you use <code>reboot</code> to restart the system,<br>it will somehow get stuck with pink led light.<br>So I always just power cycle it, which works just fine.</p>\n<p>To confirm this works, on satellite side, once restarted,<br><code>iwconfig</code> should show the new SSID and signal strength should be<br>-95dbm indicating it’s disconnected.</p>\n<p>Enable Telnet in router and log into it. </p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># ath01 is 2.4G wifi interface. It should have nothing connected now.</span><br><span class=\"line\">root@RBR20:/# wlanconfig ath01 list</span><br><span class=\"line\"># ath11 is 5G. It should have satellite(s) connected.</span><br><span class=\"line\">root@RBR20:/# wlanconfig ath11 list</span><br><span class=\"line\">ADDR               AID CHAN TXRATE RXRATE RSSI MINRSSI MAXRSSI IDLE  TXSEQ  RXSEQ  CAPS        ACAPS</span><br><span class=\"line\"> ERP    STATE MAXRATE(DOT11) HTCAPS ASSOCTIME    IEs   MODE                   PSMODE</span><br><span class=\"line\">                     1  157 866M    780M   51      44      58    0      0   65535   EPs         0</span><br><span class=\"line\">   b              0           AWPSM 19:26:27     RSN WME IEEE80211_MODE_11AC_VHT80   0</span><br></pre></td></tr></table></figure>\n","thumbnailImageUrl":null,"excerpt":"<h2 id=\"TL-DR\"><a href=\"#TL-DR\" class=\"headerlink\" title=\"TL;DR;\"></a>TL;DR;</h2><p>I was able to “break” Orbi’s 2.4G backhaul fallback<br>and hence force it onto 5G.</p>\n<p>So instead of this</p>\n<img src=\"/2020/04/04/Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul/slowspeed.png\" style=\"width: 400px\" />\n\n<p>I got this:</p>\n<img src=\"/2020/04/04/Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul/speed.png\" style=\"width: 400px\" />\n\n<p>The solution isn’t clean. Basically telnet to the satellite and then</p>\n<figure class=\"highlight console\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@RBS20:/# config set wlg_sta_ssid=&lt;original ssid&gt;_disabled</span><br><span class=\"line\">root@RBS20:/# config commit </span><br></pre></td></tr></table></figure>\n\n<p>Then reboot.</p>\n<p>More details and investigation<br><br>↓</p>","more":"<h2 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h2><p>A while back, I got the Netgear Orbi <a href=\"https://www.netgear.com/support/product/RBK20.aspx\">RBK20</a><br>to replace my original not so reliable powerline setup.</p>\n<img src=\"/2020/04/04/Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul/RBK20.png\" class=\"\">\n\n<p>It’s a wifi mesh solution that aims to solve the problem<br>where traditional single pointer router fails -<br>to provide good signal coverage<br>in complex indoor environment with walls in between rooms.<br>This is esp. important in the AC world where<br>the 5G signal falls short as it tends to be more easily blocked/<br>than 2.4G.</p>\n<p>One thing worth mentioning though is that Orbi isn’t<br>a true mesh system, that is, to support 802.11s mesh standard.<br>A true mesh system allows satellites to cooperate together<br>through a routing algorithm to find the optimal data transfer path<br>but Orbi basically supports only star topology or daisy chaining.<br>That said though, in practice, some of the “advanced” true mesh systems<br>on the other hand suffer from lack of dedicated backhaul channel<br>for inter satellite uplink (e.g. Google Wifi) and hence actually<br>perform much worse in reality as they essentially just become<br>more advanced repeaters.</p>\n<h2 id=\"Problem\"><a href=\"#Problem\" class=\"headerlink\" title=\"Problem\"></a>Problem</h2><p>In order for Orbi to have max power, we need to<br>ensure that Orbi has the dedicated backhaul operate in 5G mode,<br>otherwise anything connected to satellites would be bound to<br>2.4G speed which is 192Mbps.</p>\n<p>It’s not unusable technically but it sort of defeats the purpose.<br>What makes things worse is that Netgear decides on a strategy<br>that if 5G backhaul isn’t stable, it would fall back onto 2.4G,<br>which isn’t dedicated but shared with regular 2.4G radio.<br>This isn’t a bad idea per se as home layout may be complex so<br>they sacrifice throughtput in favor of usability in certain<br>scenarios.</p>\n<p>However this setup doesn’t work well in practice.<br>For whatever reason, the system would fall back onto 2.4G<br>randomly even if 5G signal is perfectly fine. And the only<br>way to fix that would be to power cycle the satellite.</p>\n<h2 id=\"Investigation\"><a href=\"#Investigation\" class=\"headerlink\" title=\"Investigation\"></a>Investigation</h2><p>Since Orbi is OpenWRT based, I took a look at the configs<br>that potentially control the behavior and here they are:</p>\n<figure class=\"highlight console\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@RBS20:/# uci show | grep WiFiLink</span><br><span class=\"line\">...</span><br><span class=\"line\">repacd.WiFiLink.RSSIThresholdFar=&#x27;-75&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RSSIThresholdFar5g=&#x27;-82&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RSSIThresholdFar24g=&#x27;-76&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RSSIThresholdNear=&#x27;-60&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RSSIThresholdMin=&#x27;-75&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RSSIThresholdPrefer2GBackhaul=&#x27;-82&#x27;</span><br><span class=\"line\">repacd.WiFiLink.2GBackhaulSwitchDownTime=&#x27;10&#x27;</span><br><span class=\"line\">repacd.WiFiLink.MaxMeasuringStateAttempts=&#x27;30&#x27;</span><br><span class=\"line\">repacd.WiFiLink.DaisyChain=&#x27;1&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RateNumMeasurements=&#x27;5&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RateThresholdMin5GInPercent=&#x27;35&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RateThresholdMax5GInPercent=&#x27;95&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RateThresholdPrefer2GBackhaulInPercent=&#x27;5&#x27;</span><br><span class=\"line\">repacd.WiFiLink.5GBackhaulBadlinkTimeout=&#x27;60&#x27;</span><br><span class=\"line\">repacd.WiFiLink.BSSIDAssociationTimeout=&#x27;170&#x27;</span><br><span class=\"line\">repacd.WiFiLink.RateScalingFactor=&#x27;85&#x27;</span><br><span class=\"line\">repacd.WiFiLink.5GBackhaulEvalTimeShort=&#x27;330&#x27;</span><br><span class=\"line\">repacd.WiFiLink.5GBackhaulEvalTimeLong=&#x27;1800&#x27;</span><br><span class=\"line\">repacd.WiFiLink.2GBackhaulEvalTime=&#x27;1800&#x27;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n\n<p>So in theory 2G backhaul should only be preferred<br>after signal drops below -82dbm or rate is below 5%/35%.<br>However in practice this almost is never the case.</p>\n<figure class=\"highlight console\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">From router side:</span></span><br><span class=\"line\">root@RBR20:/# wlanconfig ath11 list</span><br><span class=\"line\">ADDR               AID CHAN TXRATE RXRATE RSSI MINRSSI MAXRSSI IDLE  TXSEQ  RXSEQ  CAPS        ACAPS</span><br><span class=\"line\"> ERP    STATE MAXRATE(DOT11) HTCAPS ASSOCTIME    IEs   MODE                   PSMODE</span><br><span class=\"line\">                     1  157 866M    780M   51      44      58    0      0   65535   EPs         0</span><br><span class=\"line\">   b              0           AWPSM 18:46:39     RSN WME IEEE80211_MODE_11AC_VHT80   0</span><br></pre></td></tr></table></figure>\n\n<p>RSSI here is 51 which indicates roughly -44dbm<br>which is perfectly fine. It’s possible that the number would<br>fluctuate during operation but as you see the min value<br>is still way above the threshold. Also in practice,<br>once Orbi picks 2.4G it will stick there which doesn’t make sense<br>but since Orbi’s software isn’t open sourced there’s no way to know<br>exactly how it messes things up (looking at you Netgear developers).</p>\n<h2 id=\"A-Hacky-Solution\"><a href=\"#A-Hacky-Solution\" class=\"headerlink\" title=\"A Hacky Solution\"></a>A Hacky Solution</h2><h3 id=\"Enable-Telnet\"><a href=\"#Enable-Telnet\" class=\"headerlink\" title=\"Enable Telnet\"></a>Enable Telnet</h3><p>Go to http://&lt;satellite ip&gt;/debug.htm, log in with router username &amp; password<br>and check “Enable Telnet”.</p>\n<img src=\"/2020/04/04/Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul/telnet.png\" class=\"\">\n\n<h3 id=\"Initial-Attempt\"><a href=\"#Initial-Attempt\" class=\"headerlink\" title=\"Initial Attempt\"></a>Initial Attempt</h3><p>Since we cannot alter Netgear’s software behavior directly,<br>we have to somehow break the 2.4G connection to force it onto 5G.</p>\n<p>My initial thought was to disable the 2.4G wifi interface.<br>(Spoiler: this does not work somehow).</p>\n<p>This is doable via OpenWRT’s config system.</p>\n<figure class=\"highlight console\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta prompt_\"># </span><span class=\"language-bash\">Show all wifi interfaces. The 2.4G is already disabled here but <span class=\"keyword\">in</span> normal operation it will show something.</span></span><br><span class=\"line\">root@RBS20:/# iwconfig</span><br><span class=\"line\">ath01     IEEE 802.11b  ESSID:&quot;NETGEAR_ORBI_hidden52&quot;</span><br><span class=\"line\">          Mode:Managed  Frequency:2.412 GHz  Access Point: Not-Associated</span><br><span class=\"line\">          Bit Rate:0 kb/s   Tx-Power:25 dBm</span><br><span class=\"line\">          RTS thr:off   Fragment thr:off</span><br><span class=\"line\">          Power Management:off</span><br><span class=\"line\">          Link Quality=0/94  Signal level=-95 dBm  Noise level=-95 dBm</span><br><span class=\"line\">          Rx invalid nwid:0  Rx invalid crypt:0  Rx invalid frag:0</span><br><span class=\"line\">          Tx excessive retries:0  Invalid misc:0   Missed beacon:0</span><br><span class=\"line\"></span><br><span class=\"line\">root@RBS20:/# vi /etc/config/wireless</span><br><span class=\"line\">config wifi-iface</span><br><span class=\"line\">        option device &#x27;wifi0&#x27;</span><br><span class=\"line\">        option network &#x27;0&#x27;</span><br><span class=\"line\">        option bridge &#x27;br0&#x27;</span><br><span class=\"line\">        option mode &#x27;sta&#x27;</span><br><span class=\"line\">        option wds &#x27;1&#x27;</span><br><span class=\"line\">        option athnewind &#x27;1&#x27;</span><br><span class=\"line\">        option vap_ind &#x27;0&#x27;</span><br><span class=\"line\">        option backhaul &#x27;1&#x27;</span><br><span class=\"line\">        option wsplcd_unmanaged &#x27;1&#x27;</span><br><span class=\"line\">        option repacd_security_unmanaged &#x27;1&#x27;</span><br><span class=\"line\">        option dropmdns &#x27;0&#x27;</span><br><span class=\"line\">        option ssid &#x27;NETGEAR_ORBI_hidden52&#x27;</span><br><span class=\"line\">        option encryption &#x27;psk2+ccmp&#x27;</span><br><span class=\"line\">        option ifname &#x27;ath01&#x27;</span><br><span class=\"line\">        option wps_config &#x27;virtual_push_button physical_push_button&#x27;</span><br><span class=\"line\">        option wps_pbc &#x27;1&#x27;</span><br><span class=\"line\">        option dyn_bw_rts &#x27;0&#x27;</span><br><span class=\"line\">        option disabled &#x27;0&#x27;</span><br></pre></td></tr></table></figure>\n\n<p>^ From there we can see <code>ath01</code> is bound to physical interface <code>wifi0</code>.<br>We can just change the disabled value from 0 to 1.<br>Then <code>uci commit</code> will persist the value.</p>\n<p>However for some reason, Orbi will restore values in OpenWRT config,<br>presumably for system integrity reason. YMMV but at least this does<br>not work for RBK20 system.</p>\n<h3 id=\"The-Hacky-Attempt\"><a href=\"#The-Hacky-Attempt\" class=\"headerlink\" title=\"The Hacky Attempt\"></a>The Hacky Attempt</h3><p>While Orbi restores uci configs, it does keep its own configs<br>that are alterable via <code>config</code>.</p>\n<p>Then essentially the solution becomes that we can change the 2.4G<br>backhaul SSID such that it cannot find the right one.</p>\n<p>In my system the original SSID is <code>NETGEAR_ORBI_hidden52</code> from<br><code>iwconfig</code>.<br>We can grab all related configs from</p>\n<figure class=\"highlight console\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@RBS20:/# config show | grep hidden52</span><br><span class=\"line\">wlg_ap_bh_ssid=NETGEAR_ORBI_hidden52</span><br><span class=\"line\">wla_ap_bh_ssid=NETGEAR_ORBI_hidden52</span><br><span class=\"line\">wla_sta_ssid=NETGEAR_ORBI_hidden52</span><br><span class=\"line\">wlg_sta_ssid=NETGEAR_ORBI_hidden52</span><br></pre></td></tr></table></figure>\n\n<p>Here anything starts with <code>wla</code> is 5G and <code>wlg</code> is 2.4G.</p>\n<p>Then we can do</p>\n<figure class=\"highlight console\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@RBS20:/# config set wlg_sta_ssid=NETGEAR_ORBI_hidden52_disabled</span><br><span class=\"line\">root@RBS20:/# config commit</span><br></pre></td></tr></table></figure>\n\n<p>The <code>_disabled</code> suffix is arbitrary with the idea to prevent<br>Orbi from connecting to router via 2.4G.</p>\n<p><code>wlg_ap_bh_ssid</code> does not need to be set. The <code>ap</code> setup will<br>automatically follow the <code>sta</code> setup.</p>\n<p><code>nvram show | grep ssid</code> should now show the persisted value.</p>\n<p>In RBS20, if you use <code>reboot</code> to restart the system,<br>it will somehow get stuck with pink led light.<br>So I always just power cycle it, which works just fine.</p>\n<p>To confirm this works, on satellite side, once restarted,<br><code>iwconfig</code> should show the new SSID and signal strength should be<br>-95dbm indicating it’s disconnected.</p>\n<p>Enable Telnet in router and log into it. </p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># ath01 is 2.4G wifi interface. It should have nothing connected now.</span><br><span class=\"line\">root@RBR20:/# wlanconfig ath01 list</span><br><span class=\"line\"># ath11 is 5G. It should have satellite(s) connected.</span><br><span class=\"line\">root@RBR20:/# wlanconfig ath11 list</span><br><span class=\"line\">ADDR               AID CHAN TXRATE RXRATE RSSI MINRSSI MAXRSSI IDLE  TXSEQ  RXSEQ  CAPS        ACAPS</span><br><span class=\"line\"> ERP    STATE MAXRATE(DOT11) HTCAPS ASSOCTIME    IEs   MODE                   PSMODE</span><br><span class=\"line\">                     1  157 866M    780M   51      44      58    0      0   65535   EPs         0</span><br><span class=\"line\">   b              0           AWPSM 19:26:27     RSN WME IEEE80211_MODE_11AC_VHT80   0</span><br></pre></td></tr></table></figure>"},{"title":"It's all about buffers: zero-copy, mmap and Java NIO","date":"2016-09-10T08:00:27.000Z","_content":"\nThere are use cases where data need to be read from source to a sink without modification. In code this might look quite simple: for example in Java, you may read data from one `InputStream` chunk by chunk into a small buffer (typically 8KB), and feed them into the `OutputStream`, or even better, you could create a `PipedInputStream`, which is basically just a util that maintains that buffer for you. However, if low latency is crucial to your software, this might be quite expensive from the OS perspective and I shall explain.\n\n## What happens under the hood\n\nWell, here's what happens when the above code is used:\n\n{% asset_img non_zero_copy.png %}\n\n1. JVM sends read() syscall. \n2. OS context switches to kernel mode and reads data into the input socket buffer.\n3. OS kernel then copies data into user buffer, and context switches back to user mode. read() returns.\n4. JVM processes code logic and sends write() syscall.\n5. OS context switches to kernel mode and copies data from user buffer to output socket buffer.\n6. OS returns to user mode and logic in JVM continues.\n\n<!-- more -->\n\nThis would be fine if latency and throughput aren't your service's concern or bottleneck, but it would be annoying if you do care, say for a static asset server. There are 4 context switches and 2 unnecessary copies for the above example.\n\n## OS-level zero copy for the rescue\n\nClearly in this use case, the copy from/to user space memory is totally unnecessary because we didn't do anything other than dumping data to a different socket. Zero copy can thus be used here to save the 2 extra copies. The actual implementation doesn't really have a standard and is up to the OS how to achieve that. Typically *nix systems will offer `sendfile()`. Its man page can be found [here](http://man7.org/linux/man-pages/man2/sendfile.2.html). Some say some operating systems have broken versions of that with one of them being OSX [link](https://blog.phusion.nl/2015/06/04/the-brokenness-of-the-sendfile-system-call/). Honestly with such low-level feature, I wouldn't trust Apple's BSD-like system so never tested there.\n\nWith that, the diagram would be like this:\n\n{% asset_img zero_copy.png %}\n\nYou may say OS still has to make a copy of the data in kernel memory space. Yes but from OS's perspective this is already zero-copy because there's no data copied from kernel space to user space. The reason why kernel needs to make a copy is because general hardware DMA access expects consecutive memory space (and hence the buffer). However this is avoidable if the hardware supports scatter-n-gather:\n\n{% asset_img scattergather.png %}\n\nA lot of web servers do support zero-copy such as Tomcat and Apache. For example apache's related doc can be found [here](https://httpd.apache.org/docs/2.4/mod/core.html#enablesendfile) but by default it's off.\n\nNote: Java's NIO offers this through `transferTo` ([doc](https://docs.oracle.com/javase/8/docs/api/java/nio/channels/FileChannel.html#transferTo-long-long-java.nio.channels.WritableByteChannel-)).\n\n## mmap\n\nThe problem with the above zero-copy approach is that because there's no user mode actually involved, code cannot do anything other than piping the stream. However, there's a more expensive yet more useful approach - mmap, short for memory-map.\n\n{% asset_img mmap.png %}\n\nMmap allows code to map file to kernel memory and access that directly as if it were in the application user space, thus avoiding the unnecessary copy. As a tradeoff, that will still involve 4 context switches. But since OS maps certain chunk of file into memory, you get all benefits from OS virtual memory management - hot content can be intelligently cached efficiently, and all data are page-aligned thus no buffer copying is needed to write stuff back.\n\nHowever, nothing comes for free - while mmap does avoid that extra copy, it doesn't guarantee the code will always be faster - depending on the OS implementation, there may be quite a bit of setup and teardown overhead (since it needs to find the space and maintain it in the TLB and make sure to flush it after unmapping) and page fault gets much more expensive since kernel now needs to read from hardware (like disk) to update the memory space and TLB. Hence, if performance is this critical, benchmark is always needed as abusing mmap() may yield worse performance than simply doing the copy.\n\nThe corresponding class in Java is `MappedByteBuffer` from NIO package. It's actually a variation of `DirectByteBuffer` though there's no direct relationship between classes. The actual usage is out of scope of this post.\n\n## NIO DirectByteBuffer\n\nJava NIO introduces `ByteBuffer` which represents the buffer area used for channels. There are 3 main implementations of `ByteBuffer`:\n\n1. `HeapByteBuffer`\n\n    This is used when `ByteBuffer.allocate()` is called. It's called heap because it's maintained in JVM's heap space and hence you get all benefits like GC support and caching optimization. However, it's not page aligned, which means if you need to talk to native code through JNI, JVM would have to make a copy to the aligned buffer space.\n\n2. `DirectByteBuffer`\n\n    Used when `ByteBuffer.allocateDirect()` is called. JVM will allocate memory space outside the heap space using `malloc()`. Because it's not managed by JVM, your memory space is page-aligned and not subject to GC, which makes it perfect candidate for working with native code (e.g. when writing OpenGL stuff). However, you are then \"deteriorated\" to C programmer as you'll have to allocate and deallocate memory yourself to prevent memory leak.\n\n3. `MappedByteBuffer`\n\n    Used when `FileChannel.map()` is called. Similar to `DirectByteBuffer` this is also outside of JVM heap. It essentially functions as a wrapper around OS mmap() system call in order for code to directly manipulate mapped physical memory data.\n\n## Conclusion\n\n`sendfile()` and `mmap()` offer efficient, low-latency low-level solutions to data manipulation across sockets. Again, no code should assume these are silver bullets as real world scenarios may be complex and it might not be worth the effort to switch code to them if this is not the true bottleneck. For software engineering to get the most ROI, in most cases, it's better to \"make it right\" and then \"make it fast\". Without the guardrails offered by JVM, it's easy to make software much more vulnerable to crashing (I literally mean crashing, not exceptions) when it comes to complicated logic.\n\n## Quick Reference\n\n[Efficient data transfer through zero copy](https://www.ibm.com/developerworks/library/j-zerocopy/) - It also covers sendfile() performance comparison.\n\n[Getting started with new I/O (NIO)](http://www.ibm.com/developerworks/java/tutorials/j-nio/j-nio.html)\n","source":"_posts/It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO.md","raw":"---\ntitle: \"It's all about buffers: zero-copy, mmap and Java NIO\"\ndate: 2016-09-10 01:00:27\ncategories:\n- OS\ntags:\n- io\n- os\n- java\n- unix\n---\n\nThere are use cases where data need to be read from source to a sink without modification. In code this might look quite simple: for example in Java, you may read data from one `InputStream` chunk by chunk into a small buffer (typically 8KB), and feed them into the `OutputStream`, or even better, you could create a `PipedInputStream`, which is basically just a util that maintains that buffer for you. However, if low latency is crucial to your software, this might be quite expensive from the OS perspective and I shall explain.\n\n## What happens under the hood\n\nWell, here's what happens when the above code is used:\n\n{% asset_img non_zero_copy.png %}\n\n1. JVM sends read() syscall. \n2. OS context switches to kernel mode and reads data into the input socket buffer.\n3. OS kernel then copies data into user buffer, and context switches back to user mode. read() returns.\n4. JVM processes code logic and sends write() syscall.\n5. OS context switches to kernel mode and copies data from user buffer to output socket buffer.\n6. OS returns to user mode and logic in JVM continues.\n\n<!-- more -->\n\nThis would be fine if latency and throughput aren't your service's concern or bottleneck, but it would be annoying if you do care, say for a static asset server. There are 4 context switches and 2 unnecessary copies for the above example.\n\n## OS-level zero copy for the rescue\n\nClearly in this use case, the copy from/to user space memory is totally unnecessary because we didn't do anything other than dumping data to a different socket. Zero copy can thus be used here to save the 2 extra copies. The actual implementation doesn't really have a standard and is up to the OS how to achieve that. Typically *nix systems will offer `sendfile()`. Its man page can be found [here](http://man7.org/linux/man-pages/man2/sendfile.2.html). Some say some operating systems have broken versions of that with one of them being OSX [link](https://blog.phusion.nl/2015/06/04/the-brokenness-of-the-sendfile-system-call/). Honestly with such low-level feature, I wouldn't trust Apple's BSD-like system so never tested there.\n\nWith that, the diagram would be like this:\n\n{% asset_img zero_copy.png %}\n\nYou may say OS still has to make a copy of the data in kernel memory space. Yes but from OS's perspective this is already zero-copy because there's no data copied from kernel space to user space. The reason why kernel needs to make a copy is because general hardware DMA access expects consecutive memory space (and hence the buffer). However this is avoidable if the hardware supports scatter-n-gather:\n\n{% asset_img scattergather.png %}\n\nA lot of web servers do support zero-copy such as Tomcat and Apache. For example apache's related doc can be found [here](https://httpd.apache.org/docs/2.4/mod/core.html#enablesendfile) but by default it's off.\n\nNote: Java's NIO offers this through `transferTo` ([doc](https://docs.oracle.com/javase/8/docs/api/java/nio/channels/FileChannel.html#transferTo-long-long-java.nio.channels.WritableByteChannel-)).\n\n## mmap\n\nThe problem with the above zero-copy approach is that because there's no user mode actually involved, code cannot do anything other than piping the stream. However, there's a more expensive yet more useful approach - mmap, short for memory-map.\n\n{% asset_img mmap.png %}\n\nMmap allows code to map file to kernel memory and access that directly as if it were in the application user space, thus avoiding the unnecessary copy. As a tradeoff, that will still involve 4 context switches. But since OS maps certain chunk of file into memory, you get all benefits from OS virtual memory management - hot content can be intelligently cached efficiently, and all data are page-aligned thus no buffer copying is needed to write stuff back.\n\nHowever, nothing comes for free - while mmap does avoid that extra copy, it doesn't guarantee the code will always be faster - depending on the OS implementation, there may be quite a bit of setup and teardown overhead (since it needs to find the space and maintain it in the TLB and make sure to flush it after unmapping) and page fault gets much more expensive since kernel now needs to read from hardware (like disk) to update the memory space and TLB. Hence, if performance is this critical, benchmark is always needed as abusing mmap() may yield worse performance than simply doing the copy.\n\nThe corresponding class in Java is `MappedByteBuffer` from NIO package. It's actually a variation of `DirectByteBuffer` though there's no direct relationship between classes. The actual usage is out of scope of this post.\n\n## NIO DirectByteBuffer\n\nJava NIO introduces `ByteBuffer` which represents the buffer area used for channels. There are 3 main implementations of `ByteBuffer`:\n\n1. `HeapByteBuffer`\n\n    This is used when `ByteBuffer.allocate()` is called. It's called heap because it's maintained in JVM's heap space and hence you get all benefits like GC support and caching optimization. However, it's not page aligned, which means if you need to talk to native code through JNI, JVM would have to make a copy to the aligned buffer space.\n\n2. `DirectByteBuffer`\n\n    Used when `ByteBuffer.allocateDirect()` is called. JVM will allocate memory space outside the heap space using `malloc()`. Because it's not managed by JVM, your memory space is page-aligned and not subject to GC, which makes it perfect candidate for working with native code (e.g. when writing OpenGL stuff). However, you are then \"deteriorated\" to C programmer as you'll have to allocate and deallocate memory yourself to prevent memory leak.\n\n3. `MappedByteBuffer`\n\n    Used when `FileChannel.map()` is called. Similar to `DirectByteBuffer` this is also outside of JVM heap. It essentially functions as a wrapper around OS mmap() system call in order for code to directly manipulate mapped physical memory data.\n\n## Conclusion\n\n`sendfile()` and `mmap()` offer efficient, low-latency low-level solutions to data manipulation across sockets. Again, no code should assume these are silver bullets as real world scenarios may be complex and it might not be worth the effort to switch code to them if this is not the true bottleneck. For software engineering to get the most ROI, in most cases, it's better to \"make it right\" and then \"make it fast\". Without the guardrails offered by JVM, it's easy to make software much more vulnerable to crashing (I literally mean crashing, not exceptions) when it comes to complicated logic.\n\n## Quick Reference\n\n[Efficient data transfer through zero copy](https://www.ibm.com/developerworks/library/j-zerocopy/) - It also covers sendfile() performance comparison.\n\n[Getting started with new I/O (NIO)](http://www.ibm.com/developerworks/java/tutorials/j-nio/j-nio.html)\n","slug":"It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO","published":1,"updated":"2025-09-01T22:26:51.286Z","comments":1,"layout":"post","photos":[],"_id":"cmf1pfvs700088mmg6i3n26h6","content":"<p>There are use cases where data need to be read from source to a sink without modification. In code this might look quite simple: for example in Java, you may read data from one <code>InputStream</code> chunk by chunk into a small buffer (typically 8KB), and feed them into the <code>OutputStream</code>, or even better, you could create a <code>PipedInputStream</code>, which is basically just a util that maintains that buffer for you. However, if low latency is crucial to your software, this might be quite expensive from the OS perspective and I shall explain.</p>\n<h2 id=\"What-happens-under-the-hood\"><a href=\"#What-happens-under-the-hood\" class=\"headerlink\" title=\"What happens under the hood\"></a>What happens under the hood</h2><p>Well, here’s what happens when the above code is used:</p>\n<img src=\"/2016/09/10/It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO/non_zero_copy.png\" class=\"\">\n\n<ol>\n<li>JVM sends read() syscall. </li>\n<li>OS context switches to kernel mode and reads data into the input socket buffer.</li>\n<li>OS kernel then copies data into user buffer, and context switches back to user mode. read() returns.</li>\n<li>JVM processes code logic and sends write() syscall.</li>\n<li>OS context switches to kernel mode and copies data from user buffer to output socket buffer.</li>\n<li>OS returns to user mode and logic in JVM continues.</li>\n</ol>\n<span id=\"more\"></span>\n\n<p>This would be fine if latency and throughput aren’t your service’s concern or bottleneck, but it would be annoying if you do care, say for a static asset server. There are 4 context switches and 2 unnecessary copies for the above example.</p>\n<h2 id=\"OS-level-zero-copy-for-the-rescue\"><a href=\"#OS-level-zero-copy-for-the-rescue\" class=\"headerlink\" title=\"OS-level zero copy for the rescue\"></a>OS-level zero copy for the rescue</h2><p>Clearly in this use case, the copy from/to user space memory is totally unnecessary because we didn’t do anything other than dumping data to a different socket. Zero copy can thus be used here to save the 2 extra copies. The actual implementation doesn’t really have a standard and is up to the OS how to achieve that. Typically *nix systems will offer <code>sendfile()</code>. Its man page can be found <a href=\"http://man7.org/linux/man-pages/man2/sendfile.2.html\">here</a>. Some say some operating systems have broken versions of that with one of them being OSX <a href=\"https://blog.phusion.nl/2015/06/04/the-brokenness-of-the-sendfile-system-call/\">link</a>. Honestly with such low-level feature, I wouldn’t trust Apple’s BSD-like system so never tested there.</p>\n<p>With that, the diagram would be like this:</p>\n<img src=\"/2016/09/10/It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO/zero_copy.png\" class=\"\">\n\n<p>You may say OS still has to make a copy of the data in kernel memory space. Yes but from OS’s perspective this is already zero-copy because there’s no data copied from kernel space to user space. The reason why kernel needs to make a copy is because general hardware DMA access expects consecutive memory space (and hence the buffer). However this is avoidable if the hardware supports scatter-n-gather:</p>\n<img src=\"/2016/09/10/It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO/scattergather.png\" class=\"\">\n\n<p>A lot of web servers do support zero-copy such as Tomcat and Apache. For example apache’s related doc can be found <a href=\"https://httpd.apache.org/docs/2.4/mod/core.html#enablesendfile\">here</a> but by default it’s off.</p>\n<p>Note: Java’s NIO offers this through <code>transferTo</code> (<a href=\"https://docs.oracle.com/javase/8/docs/api/java/nio/channels/FileChannel.html#transferTo-long-long-java.nio.channels.WritableByteChannel-\">doc</a>).</p>\n<h2 id=\"mmap\"><a href=\"#mmap\" class=\"headerlink\" title=\"mmap\"></a>mmap</h2><p>The problem with the above zero-copy approach is that because there’s no user mode actually involved, code cannot do anything other than piping the stream. However, there’s a more expensive yet more useful approach - mmap, short for memory-map.</p>\n<img src=\"/2016/09/10/It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO/mmap.png\" class=\"\">\n\n<p>Mmap allows code to map file to kernel memory and access that directly as if it were in the application user space, thus avoiding the unnecessary copy. As a tradeoff, that will still involve 4 context switches. But since OS maps certain chunk of file into memory, you get all benefits from OS virtual memory management - hot content can be intelligently cached efficiently, and all data are page-aligned thus no buffer copying is needed to write stuff back.</p>\n<p>However, nothing comes for free - while mmap does avoid that extra copy, it doesn’t guarantee the code will always be faster - depending on the OS implementation, there may be quite a bit of setup and teardown overhead (since it needs to find the space and maintain it in the TLB and make sure to flush it after unmapping) and page fault gets much more expensive since kernel now needs to read from hardware (like disk) to update the memory space and TLB. Hence, if performance is this critical, benchmark is always needed as abusing mmap() may yield worse performance than simply doing the copy.</p>\n<p>The corresponding class in Java is <code>MappedByteBuffer</code> from NIO package. It’s actually a variation of <code>DirectByteBuffer</code> though there’s no direct relationship between classes. The actual usage is out of scope of this post.</p>\n<h2 id=\"NIO-DirectByteBuffer\"><a href=\"#NIO-DirectByteBuffer\" class=\"headerlink\" title=\"NIO DirectByteBuffer\"></a>NIO DirectByteBuffer</h2><p>Java NIO introduces <code>ByteBuffer</code> which represents the buffer area used for channels. There are 3 main implementations of <code>ByteBuffer</code>:</p>\n<ol>\n<li><p><code>HeapByteBuffer</code></p>\n<p> This is used when <code>ByteBuffer.allocate()</code> is called. It’s called heap because it’s maintained in JVM’s heap space and hence you get all benefits like GC support and caching optimization. However, it’s not page aligned, which means if you need to talk to native code through JNI, JVM would have to make a copy to the aligned buffer space.</p>\n</li>\n<li><p><code>DirectByteBuffer</code></p>\n<p> Used when <code>ByteBuffer.allocateDirect()</code> is called. JVM will allocate memory space outside the heap space using <code>malloc()</code>. Because it’s not managed by JVM, your memory space is page-aligned and not subject to GC, which makes it perfect candidate for working with native code (e.g. when writing OpenGL stuff). However, you are then “deteriorated” to C programmer as you’ll have to allocate and deallocate memory yourself to prevent memory leak.</p>\n</li>\n<li><p><code>MappedByteBuffer</code></p>\n<p> Used when <code>FileChannel.map()</code> is called. Similar to <code>DirectByteBuffer</code> this is also outside of JVM heap. It essentially functions as a wrapper around OS mmap() system call in order for code to directly manipulate mapped physical memory data.</p>\n</li>\n</ol>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p><code>sendfile()</code> and <code>mmap()</code> offer efficient, low-latency low-level solutions to data manipulation across sockets. Again, no code should assume these are silver bullets as real world scenarios may be complex and it might not be worth the effort to switch code to them if this is not the true bottleneck. For software engineering to get the most ROI, in most cases, it’s better to “make it right” and then “make it fast”. Without the guardrails offered by JVM, it’s easy to make software much more vulnerable to crashing (I literally mean crashing, not exceptions) when it comes to complicated logic.</p>\n<h2 id=\"Quick-Reference\"><a href=\"#Quick-Reference\" class=\"headerlink\" title=\"Quick Reference\"></a>Quick Reference</h2><p><a href=\"https://www.ibm.com/developerworks/library/j-zerocopy/\">Efficient data transfer through zero copy</a> - It also covers sendfile() performance comparison.</p>\n<p><a href=\"http://www.ibm.com/developerworks/java/tutorials/j-nio/j-nio.html\">Getting started with new I/O (NIO)</a></p>\n","thumbnailImageUrl":null,"excerpt":"<p>There are use cases where data need to be read from source to a sink without modification. In code this might look quite simple: for example in Java, you may read data from one <code>InputStream</code> chunk by chunk into a small buffer (typically 8KB), and feed them into the <code>OutputStream</code>, or even better, you could create a <code>PipedInputStream</code>, which is basically just a util that maintains that buffer for you. However, if low latency is crucial to your software, this might be quite expensive from the OS perspective and I shall explain.</p>\n<h2 id=\"What-happens-under-the-hood\"><a href=\"#What-happens-under-the-hood\" class=\"headerlink\" title=\"What happens under the hood\"></a>What happens under the hood</h2><p>Well, here’s what happens when the above code is used:</p>\n<img src=\"/2016/09/10/It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO/non_zero_copy.png\" class=\"\">\n\n<ol>\n<li>JVM sends read() syscall. </li>\n<li>OS context switches to kernel mode and reads data into the input socket buffer.</li>\n<li>OS kernel then copies data into user buffer, and context switches back to user mode. read() returns.</li>\n<li>JVM processes code logic and sends write() syscall.</li>\n<li>OS context switches to kernel mode and copies data from user buffer to output socket buffer.</li>\n<li>OS returns to user mode and logic in JVM continues.</li>\n</ol>","more":"<p>This would be fine if latency and throughput aren’t your service’s concern or bottleneck, but it would be annoying if you do care, say for a static asset server. There are 4 context switches and 2 unnecessary copies for the above example.</p>\n<h2 id=\"OS-level-zero-copy-for-the-rescue\"><a href=\"#OS-level-zero-copy-for-the-rescue\" class=\"headerlink\" title=\"OS-level zero copy for the rescue\"></a>OS-level zero copy for the rescue</h2><p>Clearly in this use case, the copy from/to user space memory is totally unnecessary because we didn’t do anything other than dumping data to a different socket. Zero copy can thus be used here to save the 2 extra copies. The actual implementation doesn’t really have a standard and is up to the OS how to achieve that. Typically *nix systems will offer <code>sendfile()</code>. Its man page can be found <a href=\"http://man7.org/linux/man-pages/man2/sendfile.2.html\">here</a>. Some say some operating systems have broken versions of that with one of them being OSX <a href=\"https://blog.phusion.nl/2015/06/04/the-brokenness-of-the-sendfile-system-call/\">link</a>. Honestly with such low-level feature, I wouldn’t trust Apple’s BSD-like system so never tested there.</p>\n<p>With that, the diagram would be like this:</p>\n<img src=\"/2016/09/10/It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO/zero_copy.png\" class=\"\">\n\n<p>You may say OS still has to make a copy of the data in kernel memory space. Yes but from OS’s perspective this is already zero-copy because there’s no data copied from kernel space to user space. The reason why kernel needs to make a copy is because general hardware DMA access expects consecutive memory space (and hence the buffer). However this is avoidable if the hardware supports scatter-n-gather:</p>\n<img src=\"/2016/09/10/It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO/scattergather.png\" class=\"\">\n\n<p>A lot of web servers do support zero-copy such as Tomcat and Apache. For example apache’s related doc can be found <a href=\"https://httpd.apache.org/docs/2.4/mod/core.html#enablesendfile\">here</a> but by default it’s off.</p>\n<p>Note: Java’s NIO offers this through <code>transferTo</code> (<a href=\"https://docs.oracle.com/javase/8/docs/api/java/nio/channels/FileChannel.html#transferTo-long-long-java.nio.channels.WritableByteChannel-\">doc</a>).</p>\n<h2 id=\"mmap\"><a href=\"#mmap\" class=\"headerlink\" title=\"mmap\"></a>mmap</h2><p>The problem with the above zero-copy approach is that because there’s no user mode actually involved, code cannot do anything other than piping the stream. However, there’s a more expensive yet more useful approach - mmap, short for memory-map.</p>\n<img src=\"/2016/09/10/It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO/mmap.png\" class=\"\">\n\n<p>Mmap allows code to map file to kernel memory and access that directly as if it were in the application user space, thus avoiding the unnecessary copy. As a tradeoff, that will still involve 4 context switches. But since OS maps certain chunk of file into memory, you get all benefits from OS virtual memory management - hot content can be intelligently cached efficiently, and all data are page-aligned thus no buffer copying is needed to write stuff back.</p>\n<p>However, nothing comes for free - while mmap does avoid that extra copy, it doesn’t guarantee the code will always be faster - depending on the OS implementation, there may be quite a bit of setup and teardown overhead (since it needs to find the space and maintain it in the TLB and make sure to flush it after unmapping) and page fault gets much more expensive since kernel now needs to read from hardware (like disk) to update the memory space and TLB. Hence, if performance is this critical, benchmark is always needed as abusing mmap() may yield worse performance than simply doing the copy.</p>\n<p>The corresponding class in Java is <code>MappedByteBuffer</code> from NIO package. It’s actually a variation of <code>DirectByteBuffer</code> though there’s no direct relationship between classes. The actual usage is out of scope of this post.</p>\n<h2 id=\"NIO-DirectByteBuffer\"><a href=\"#NIO-DirectByteBuffer\" class=\"headerlink\" title=\"NIO DirectByteBuffer\"></a>NIO DirectByteBuffer</h2><p>Java NIO introduces <code>ByteBuffer</code> which represents the buffer area used for channels. There are 3 main implementations of <code>ByteBuffer</code>:</p>\n<ol>\n<li><p><code>HeapByteBuffer</code></p>\n<p> This is used when <code>ByteBuffer.allocate()</code> is called. It’s called heap because it’s maintained in JVM’s heap space and hence you get all benefits like GC support and caching optimization. However, it’s not page aligned, which means if you need to talk to native code through JNI, JVM would have to make a copy to the aligned buffer space.</p>\n</li>\n<li><p><code>DirectByteBuffer</code></p>\n<p> Used when <code>ByteBuffer.allocateDirect()</code> is called. JVM will allocate memory space outside the heap space using <code>malloc()</code>. Because it’s not managed by JVM, your memory space is page-aligned and not subject to GC, which makes it perfect candidate for working with native code (e.g. when writing OpenGL stuff). However, you are then “deteriorated” to C programmer as you’ll have to allocate and deallocate memory yourself to prevent memory leak.</p>\n</li>\n<li><p><code>MappedByteBuffer</code></p>\n<p> Used when <code>FileChannel.map()</code> is called. Similar to <code>DirectByteBuffer</code> this is also outside of JVM heap. It essentially functions as a wrapper around OS mmap() system call in order for code to directly manipulate mapped physical memory data.</p>\n</li>\n</ol>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p><code>sendfile()</code> and <code>mmap()</code> offer efficient, low-latency low-level solutions to data manipulation across sockets. Again, no code should assume these are silver bullets as real world scenarios may be complex and it might not be worth the effort to switch code to them if this is not the true bottleneck. For software engineering to get the most ROI, in most cases, it’s better to “make it right” and then “make it fast”. Without the guardrails offered by JVM, it’s easy to make software much more vulnerable to crashing (I literally mean crashing, not exceptions) when it comes to complicated logic.</p>\n<h2 id=\"Quick-Reference\"><a href=\"#Quick-Reference\" class=\"headerlink\" title=\"Quick Reference\"></a>Quick Reference</h2><p><a href=\"https://www.ibm.com/developerworks/library/j-zerocopy/\">Efficient data transfer through zero copy</a> - It also covers sendfile() performance comparison.</p>\n<p><a href=\"http://www.ibm.com/developerworks/java/tutorials/j-nio/j-nio.html\">Getting started with new I/O (NIO)</a></p>"},{"title":"Monitor gRPC Microservices in Kubernetes with Amazon X-Ray","date":"2018-11-25T22:11:39.000Z","coverImage":"xray.png","coverMeta":"out","coverSize":"partial","_content":"\nMicroservice architecture is typically useful to solve certain scaling problems where service decoupling/segregation is required to improve development velocity, make service more fault tolerant or handle performance hotspots.\n\nHowever, everything comes with a price and so does microservice. One typical issue is:\n\n<img src=\"{% asset_path ms-failure.png %}\" style=\"width: 600px\" />\n\nWhile this is half joking, monitoring and fault resilency are definitely more challenging in microservice world. While there are frameworks like Hystrix and resilience4j to handle circuit breaking, rate limiting and stuff like that, this post focuses on the first thing: how the heck are my services talking to each other?\n\nAWS X-Ray can fill the gap here by offering service mapping and tracing and thus you can see something like\n\n{% asset_img xray.png %}\n\n{% asset_img tracing.png %}\n\nCompared to generic service monitoring,\nX-Ray has some additional benefits around AWS ecosystem in that\nit will auto expose your AWS resource write\n(yes only write unfortunately) call insights when you use AWS SDK.\nThis applies to SQS, SNS and DynamoDB.\n\n<!-- more -->\n\nBut first of all, you need to understand how X-Ray works:\n\n- X-Ray requires application to forward insights to the daemon.\n  In EC2, this means the daemon process alongside with your application.\n  In Kubenetes, this means you'd need to install it as a daemonset so it would run with your node.\n- When a request enters the first service (typically an API gateway),\n  the service is responsible for creating the first `segment` and generate the `trace ID`\n  (typically created by AWS X-Ray SDK).\n  A `segment` represents the overall lifecycle of a request within **one** application,\n  identified by `segment ID`.\n  A `trace ID` identifies the overall roundtrip of a request across **multiple** applications.\n- A service, when making requests to other services,\n  should generate corresponding `subsegment`s.\n  A `subsegment` is used to identify activities within one application.\n  This is not required for service mapping but nice to have for tracing purposes.\n- A service, when accepting traffic from other services,\n  should relay the trace ID and the previous segment ID (called parent ID in SDK).\n  This is such that the service mapping can be generated.\n\nFor inter-service communication, gRPC is often used. Compared to JSON over REST, gRPC offers more flexibility around query design and better performance thanks to the efficiency of (de)serialization with protobuf and the usage of http2 multiplexing. The extra typing and backward compatibility from protobuf also help documentation and maintenance, improving the overall quality of service quorum.\n\nHowever, while X-Ray SDK offers J2EE servlet filter for general http servers, gRPC does not follow that. The canonical [gRPC Java implementation](https://github.com/grpc/grpc-java) uses netty and has no knowledge around that.\n\nThis means we'd have to write some custom code. Unfortunately the documentation around that is [next to none](https://grpc.io/docs/quickstart/java.html). Luckily, gRPC has implicit support via `io.grpc.ServerInterceptor` and `io.grpc.ClientInterceptor` so it's just a matter of how to wire pieces together.\n\nOverall there are 4 steps:\n\n1. Set up Kubernetes daemonset\n2. Grant permission to Kubernetes nodes so they can write metrics to X-Ray.\n3. Write/use interceptors in code\n4. Route metrics to X-Ray daemon\n\nLet's do this step by step:\n\n#### Set up Kubernetes daemonset\n\nThere's an example offered by Amazon regarding how to install it: [link](https://github.com/aws-samples/aws-xray-kubernetes)\n\n#### Grant permission to Kubernetes nodes\n\nThis is a bit tricky depending how your kube cluster is set up.\n\nIf you use EKS/EC2, you need to grant X-Ray write permission by\nattaching the canned policy to your IAM role for the worker nodes.\n\n<img src=\"{% asset_path iam.png %}\" style=\"width: 400px\" />\n\nIf you host your kubenetes outside AWS ecosystem,\nwell chances are you don't need X-Ray but something generic like Istio's sidecar approach.\nBut if you do need it then you can create IAM users,\nattach the policy and use these users in your code.\n\n#### Write/use interceptors in code\n\nFirst, we need to make sure we use the same language between server and client.\nIn typical HTTP this is the headers.\nIn gRPC this is the metadata, keyed by `Key`.\n\n```java\npublic class Keys {\n\n    public static final Metadata.Key<String> TRACE_ID_HEADER = Metadata.Key.of(\"traceId\", Metadata.ASCII_STRING_MARSHALLER);\n    public static final Metadata.Key<String> PARENT_ID_HEADER = Metadata.Key.of(\"parentId\", Metadata.ASCII_STRING_MARSHALLER);\n\n}\n```\n\nNow, let's implement client interceptor.\n\nFirst you need some X-Ray stuff in classpath\n(assuming Gradle is used for dependence managmement, should be similar for maven/ivy/sbt):\n\n```groovy\ndependencies {\n  compile (\n    \"com.amazonaws:aws-xray-recorder-sdk-core\",\n    \"com.amazonaws:aws-xray-recorder-sdk-aws-sdk\",\n  )\n}\n```\n\nNote for demonstration purpose the verion is omitted here,\nfor actual usage you should peg the latest version at the time.\n\nIf you want X-Ray to instrument your AWS resource calls, you also need:\n\n```groovy\ncompile(\"com.amazonaws:aws-xray-recorder-sdk-aws-sdk-instrumentor\")\n```\n\nNow the code:\n\n```java\npublic class XRayClientInterceptor implements ClientInterceptor {\n\n    private final AWSXRayRecorder recorder = AWSXRayRecorderBuilder.defaultRecorder();\n\n    @Override\n    public <ReqT, RespT> ClientCall<ReqT, RespT> interceptCall(\n            MethodDescriptor<ReqT, RespT> method, CallOptions callOptions, Channel next) {\n        final Segment segment = recorder.getCurrentSegmentOptional().orElseGet(() -> {\n            //noinspection CodeBlock2Expr\n            return recorder.beginSegment(method.getFullMethodName());\n        });\n        final String segmentId = segment.getId();\n        final String traceId = segment.getTraceId().toString();\n        ClientCall<ReqT, RespT> call = next.newCall(method, callOptions);\n        return new ForwardingClientCall.SimpleForwardingClientCall<ReqT, RespT>(call) {\n            @Override\n            public void start(Listener<RespT> responseListener, Metadata headers) {\n                Subsegment callSegment = recorder.beginSubsegment(method.getFullMethodName());\n                final Entity context = recorder.getTraceEntity();\n                headers.discardAll(Keys.PARENT_ID_HEADER);\n                headers.put(Keys.PARENT_ID_HEADER, segmentId);\n                headers.put(Keys.TRACE_ID_HEADER, traceId);\n                delegate().start(\n                        new ForwardingClientCallListener.SimpleForwardingClientCallListener<RespT>(responseListener) {\n                            @Override\n                            public void onClose(io.grpc.Status status, Metadata trailers) {\n                                if (status.getCause() != null) {\n                                    callSegment.addException(status.getCause());\n                                } else if (!status.isOk()) {\n                                    callSegment.setError(true);\n                                }\n                                try {\n                                    super.onClose(status, trailers);\n                                } finally {\n                                    Entity originalContext = recorder.getTraceEntity();\n                                    recorder.setTraceEntity(context);\n                                    try {\n                                        callSegment.close();\n                                    } finally {\n                                        recorder.setTraceEntity(originalContext);\n                                    }\n                                }\n                            }\n                        },\n                        headers);\n            }\n        };\n    }\n}\n```\n\nThere's quite a lot of code here but the key gotchas are:\n\n- You should always use an existing segment if one exists,\n  which is what `getCurrentSegmentOptional()` is for.\n  Fail to do so would result in the loss of previous segment.\n  If in some other code the previous segment is still referenced,\n  you will get missing context exceptions when trying to close it.\n- Always bear in mind that data streaming/async handling is baked in gRPC design.\n  So never close the segment directly after starting forwarding the client call.\n  Instead, implement `ClientCallListener` and let gRPC tell you when\n  it actually starts/finishes it.\n- `AWSXRayRecorder` is thread safe so using one for all calls should be fine.\n  However, all the segments are tracked via `ThreadLocalSegmentContext` by default.\n  That is shared by **all** instances across the entire app by default\n  even if you have multiple `AWSXRayRecorder` instances.\n  What that implies is you should\n  **always remember the corresponding context for that segment/subsegment**,\n  especially when crossing threads. Failure to do so would result in weird errors.\n  This is what `getTraceEntity()` and `setTraceEntity()` are for.\n- The `put()` calls would append if key with the same name already exists.\n  So remember to clean it up first.\n  The trace ID meta doesn't need to be cleared because\n  it's supposed to be the same as mentioned.\n\nAfter that, wire it up when you build the client:\n\n```java\nnewBlockingStub(channel).withInterceptors(new XRayClientInterceptor());\n```\n\nNext, let's build the server side interceptor:\n\nThis has some extra flavors in that it assumes you use a spring based\ngRPC server like the [LogNet Springboot](https://github.com/LogNet/grpc-spring-boot-starter) one.\nThe `GRpcGlobalInterceptor` would tell the runner to inject the interceptor automagically.\nIf that's not the case, that's fine,\njust replace the `appName` with some other logic,\nand wire up the interceptor using `ServerInterceptors.intercept(serviceDefinition, interceptors)`.\n\n```java\n@GRpcGlobalInterceptor\npublic class XRayServerInterceptor implements ServerInterceptor {\n\n    @Value(\"${spring.application.name}\")\n    private String appName;\n\n    @Override\n    public <ReqT, RespT> ServerCall.Listener<ReqT> interceptCall(ServerCall<ReqT, RespT> call, Metadata headers, ServerCallHandler<ReqT, RespT> next) {\n        String traceId = headers.get(Keys.TRACE_ID_HEADER);\n        String parentId = headers.get(Keys.PARENT_ID_HEADER);\n        TraceID tId = new TraceID();\n        if (traceId != null) {\n            tId = TraceID.fromString(traceId);\n        }\n        Segment segment = recorder.beginSegment(appName, tId, parentId);\n        headers.discardAll(Keys.PARENT_ID_HEADER);\n        headers.discardAll(Keys.TRACE_ID_HEADER);\n        headers.put(Keys.PARENT_ID_HEADER, segment.getId());\n        headers.put(Keys.TRACE_ID_HEADER, tId.toString());\n        ServerCall.Listener<ReqT> listener = next.startCall(call, headers);\n\n        return new ForwardingListener<>(listener, call, recorder, recorder.getTraceEntity(), segment);\n    }\n}\n\npublic class ForwardingListener<T, R>\n        extends ForwardingServerCallListener.SimpleForwardingServerCallListener<T> {\n\n    private ServerCall<T, R> call;\n    private AWSXRayRecorder recorder;\n    private Entity entity;\n    private Segment segment;\n\n    public ForwardingListener(ServerCall.Listener<T> delegate,\n            ServerCall<T, R> call,\n            AWSXRayRecorder recorder,\n            Entity entity,\n            Segment segment\n    ) {\n        super(delegate);\n        this.call = call;\n        this.recorder = recorder;\n        this.entity = entity;\n        this.segment = segment;\n    }\n\n    @Override\n    public void onCancel() {\n        recorder.setTraceEntity(entity);\n        if (call.isCancelled()) {\n            return;\n        }\n        segment.setFault(true);\n        try {\n            super.onCancel();\n        }\n        finally {\n            segment.close();\n        }\n    }\n\n    @Override\n    public void onComplete() {\n        recorder.setTraceEntity(entity);\n        try {\n            super.onComplete();\n        }\n        catch (Throwable e) {\n            segment.setError(true);\n        }\n        finally {\n            segment.close();\n        }\n    }\n\n}\n```\n\n#### Route metrics to X-Ray daemon\n\nLast but not least, we need to tell X-Ray SDK to forward them to our daemon:\n\n```yaml\n    spec:\n      containers:\n      ...\n        - name: ...\n          env:\n          - name: AWS_XRAY_DAEMON_ADDRESS \n            value: xray-daemon:2000\n```\n\nThe value corresponds to your daemon name.\n\n`AWS_XRAY_DAEMON_ADDRESS` will be read by AWS SDK at runtime.\n\n#### Done\n\nAnd that's it. Just deploy the apps to kube cluster.\nBear in mind that the service map is bound to time range.\nIt won't show up until you get traffic across your apps.\nAnd if you have traffic split like A/B testing or service\nmigration, you'll see how things evolve over time,\nwhich is pretty cool.\n","source":"_posts/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray.md","raw":"---\ntitle: Monitor gRPC Microservices in Kubernetes with Amazon X-Ray\ndate: 2018-11-25 14:11:39\ncategories:\n- Operation\n- Architecture\ntags:\n- monitoring\n- grpc\n- microservice\n- kubernetes\n- aws\n- xray\ncoverImage: xray.png\ncoverMeta: out\ncoverSize: partial\n---\n\nMicroservice architecture is typically useful to solve certain scaling problems where service decoupling/segregation is required to improve development velocity, make service more fault tolerant or handle performance hotspots.\n\nHowever, everything comes with a price and so does microservice. One typical issue is:\n\n<img src=\"{% asset_path ms-failure.png %}\" style=\"width: 600px\" />\n\nWhile this is half joking, monitoring and fault resilency are definitely more challenging in microservice world. While there are frameworks like Hystrix and resilience4j to handle circuit breaking, rate limiting and stuff like that, this post focuses on the first thing: how the heck are my services talking to each other?\n\nAWS X-Ray can fill the gap here by offering service mapping and tracing and thus you can see something like\n\n{% asset_img xray.png %}\n\n{% asset_img tracing.png %}\n\nCompared to generic service monitoring,\nX-Ray has some additional benefits around AWS ecosystem in that\nit will auto expose your AWS resource write\n(yes only write unfortunately) call insights when you use AWS SDK.\nThis applies to SQS, SNS and DynamoDB.\n\n<!-- more -->\n\nBut first of all, you need to understand how X-Ray works:\n\n- X-Ray requires application to forward insights to the daemon.\n  In EC2, this means the daemon process alongside with your application.\n  In Kubenetes, this means you'd need to install it as a daemonset so it would run with your node.\n- When a request enters the first service (typically an API gateway),\n  the service is responsible for creating the first `segment` and generate the `trace ID`\n  (typically created by AWS X-Ray SDK).\n  A `segment` represents the overall lifecycle of a request within **one** application,\n  identified by `segment ID`.\n  A `trace ID` identifies the overall roundtrip of a request across **multiple** applications.\n- A service, when making requests to other services,\n  should generate corresponding `subsegment`s.\n  A `subsegment` is used to identify activities within one application.\n  This is not required for service mapping but nice to have for tracing purposes.\n- A service, when accepting traffic from other services,\n  should relay the trace ID and the previous segment ID (called parent ID in SDK).\n  This is such that the service mapping can be generated.\n\nFor inter-service communication, gRPC is often used. Compared to JSON over REST, gRPC offers more flexibility around query design and better performance thanks to the efficiency of (de)serialization with protobuf and the usage of http2 multiplexing. The extra typing and backward compatibility from protobuf also help documentation and maintenance, improving the overall quality of service quorum.\n\nHowever, while X-Ray SDK offers J2EE servlet filter for general http servers, gRPC does not follow that. The canonical [gRPC Java implementation](https://github.com/grpc/grpc-java) uses netty and has no knowledge around that.\n\nThis means we'd have to write some custom code. Unfortunately the documentation around that is [next to none](https://grpc.io/docs/quickstart/java.html). Luckily, gRPC has implicit support via `io.grpc.ServerInterceptor` and `io.grpc.ClientInterceptor` so it's just a matter of how to wire pieces together.\n\nOverall there are 4 steps:\n\n1. Set up Kubernetes daemonset\n2. Grant permission to Kubernetes nodes so they can write metrics to X-Ray.\n3. Write/use interceptors in code\n4. Route metrics to X-Ray daemon\n\nLet's do this step by step:\n\n#### Set up Kubernetes daemonset\n\nThere's an example offered by Amazon regarding how to install it: [link](https://github.com/aws-samples/aws-xray-kubernetes)\n\n#### Grant permission to Kubernetes nodes\n\nThis is a bit tricky depending how your kube cluster is set up.\n\nIf you use EKS/EC2, you need to grant X-Ray write permission by\nattaching the canned policy to your IAM role for the worker nodes.\n\n<img src=\"{% asset_path iam.png %}\" style=\"width: 400px\" />\n\nIf you host your kubenetes outside AWS ecosystem,\nwell chances are you don't need X-Ray but something generic like Istio's sidecar approach.\nBut if you do need it then you can create IAM users,\nattach the policy and use these users in your code.\n\n#### Write/use interceptors in code\n\nFirst, we need to make sure we use the same language between server and client.\nIn typical HTTP this is the headers.\nIn gRPC this is the metadata, keyed by `Key`.\n\n```java\npublic class Keys {\n\n    public static final Metadata.Key<String> TRACE_ID_HEADER = Metadata.Key.of(\"traceId\", Metadata.ASCII_STRING_MARSHALLER);\n    public static final Metadata.Key<String> PARENT_ID_HEADER = Metadata.Key.of(\"parentId\", Metadata.ASCII_STRING_MARSHALLER);\n\n}\n```\n\nNow, let's implement client interceptor.\n\nFirst you need some X-Ray stuff in classpath\n(assuming Gradle is used for dependence managmement, should be similar for maven/ivy/sbt):\n\n```groovy\ndependencies {\n  compile (\n    \"com.amazonaws:aws-xray-recorder-sdk-core\",\n    \"com.amazonaws:aws-xray-recorder-sdk-aws-sdk\",\n  )\n}\n```\n\nNote for demonstration purpose the verion is omitted here,\nfor actual usage you should peg the latest version at the time.\n\nIf you want X-Ray to instrument your AWS resource calls, you also need:\n\n```groovy\ncompile(\"com.amazonaws:aws-xray-recorder-sdk-aws-sdk-instrumentor\")\n```\n\nNow the code:\n\n```java\npublic class XRayClientInterceptor implements ClientInterceptor {\n\n    private final AWSXRayRecorder recorder = AWSXRayRecorderBuilder.defaultRecorder();\n\n    @Override\n    public <ReqT, RespT> ClientCall<ReqT, RespT> interceptCall(\n            MethodDescriptor<ReqT, RespT> method, CallOptions callOptions, Channel next) {\n        final Segment segment = recorder.getCurrentSegmentOptional().orElseGet(() -> {\n            //noinspection CodeBlock2Expr\n            return recorder.beginSegment(method.getFullMethodName());\n        });\n        final String segmentId = segment.getId();\n        final String traceId = segment.getTraceId().toString();\n        ClientCall<ReqT, RespT> call = next.newCall(method, callOptions);\n        return new ForwardingClientCall.SimpleForwardingClientCall<ReqT, RespT>(call) {\n            @Override\n            public void start(Listener<RespT> responseListener, Metadata headers) {\n                Subsegment callSegment = recorder.beginSubsegment(method.getFullMethodName());\n                final Entity context = recorder.getTraceEntity();\n                headers.discardAll(Keys.PARENT_ID_HEADER);\n                headers.put(Keys.PARENT_ID_HEADER, segmentId);\n                headers.put(Keys.TRACE_ID_HEADER, traceId);\n                delegate().start(\n                        new ForwardingClientCallListener.SimpleForwardingClientCallListener<RespT>(responseListener) {\n                            @Override\n                            public void onClose(io.grpc.Status status, Metadata trailers) {\n                                if (status.getCause() != null) {\n                                    callSegment.addException(status.getCause());\n                                } else if (!status.isOk()) {\n                                    callSegment.setError(true);\n                                }\n                                try {\n                                    super.onClose(status, trailers);\n                                } finally {\n                                    Entity originalContext = recorder.getTraceEntity();\n                                    recorder.setTraceEntity(context);\n                                    try {\n                                        callSegment.close();\n                                    } finally {\n                                        recorder.setTraceEntity(originalContext);\n                                    }\n                                }\n                            }\n                        },\n                        headers);\n            }\n        };\n    }\n}\n```\n\nThere's quite a lot of code here but the key gotchas are:\n\n- You should always use an existing segment if one exists,\n  which is what `getCurrentSegmentOptional()` is for.\n  Fail to do so would result in the loss of previous segment.\n  If in some other code the previous segment is still referenced,\n  you will get missing context exceptions when trying to close it.\n- Always bear in mind that data streaming/async handling is baked in gRPC design.\n  So never close the segment directly after starting forwarding the client call.\n  Instead, implement `ClientCallListener` and let gRPC tell you when\n  it actually starts/finishes it.\n- `AWSXRayRecorder` is thread safe so using one for all calls should be fine.\n  However, all the segments are tracked via `ThreadLocalSegmentContext` by default.\n  That is shared by **all** instances across the entire app by default\n  even if you have multiple `AWSXRayRecorder` instances.\n  What that implies is you should\n  **always remember the corresponding context for that segment/subsegment**,\n  especially when crossing threads. Failure to do so would result in weird errors.\n  This is what `getTraceEntity()` and `setTraceEntity()` are for.\n- The `put()` calls would append if key with the same name already exists.\n  So remember to clean it up first.\n  The trace ID meta doesn't need to be cleared because\n  it's supposed to be the same as mentioned.\n\nAfter that, wire it up when you build the client:\n\n```java\nnewBlockingStub(channel).withInterceptors(new XRayClientInterceptor());\n```\n\nNext, let's build the server side interceptor:\n\nThis has some extra flavors in that it assumes you use a spring based\ngRPC server like the [LogNet Springboot](https://github.com/LogNet/grpc-spring-boot-starter) one.\nThe `GRpcGlobalInterceptor` would tell the runner to inject the interceptor automagically.\nIf that's not the case, that's fine,\njust replace the `appName` with some other logic,\nand wire up the interceptor using `ServerInterceptors.intercept(serviceDefinition, interceptors)`.\n\n```java\n@GRpcGlobalInterceptor\npublic class XRayServerInterceptor implements ServerInterceptor {\n\n    @Value(\"${spring.application.name}\")\n    private String appName;\n\n    @Override\n    public <ReqT, RespT> ServerCall.Listener<ReqT> interceptCall(ServerCall<ReqT, RespT> call, Metadata headers, ServerCallHandler<ReqT, RespT> next) {\n        String traceId = headers.get(Keys.TRACE_ID_HEADER);\n        String parentId = headers.get(Keys.PARENT_ID_HEADER);\n        TraceID tId = new TraceID();\n        if (traceId != null) {\n            tId = TraceID.fromString(traceId);\n        }\n        Segment segment = recorder.beginSegment(appName, tId, parentId);\n        headers.discardAll(Keys.PARENT_ID_HEADER);\n        headers.discardAll(Keys.TRACE_ID_HEADER);\n        headers.put(Keys.PARENT_ID_HEADER, segment.getId());\n        headers.put(Keys.TRACE_ID_HEADER, tId.toString());\n        ServerCall.Listener<ReqT> listener = next.startCall(call, headers);\n\n        return new ForwardingListener<>(listener, call, recorder, recorder.getTraceEntity(), segment);\n    }\n}\n\npublic class ForwardingListener<T, R>\n        extends ForwardingServerCallListener.SimpleForwardingServerCallListener<T> {\n\n    private ServerCall<T, R> call;\n    private AWSXRayRecorder recorder;\n    private Entity entity;\n    private Segment segment;\n\n    public ForwardingListener(ServerCall.Listener<T> delegate,\n            ServerCall<T, R> call,\n            AWSXRayRecorder recorder,\n            Entity entity,\n            Segment segment\n    ) {\n        super(delegate);\n        this.call = call;\n        this.recorder = recorder;\n        this.entity = entity;\n        this.segment = segment;\n    }\n\n    @Override\n    public void onCancel() {\n        recorder.setTraceEntity(entity);\n        if (call.isCancelled()) {\n            return;\n        }\n        segment.setFault(true);\n        try {\n            super.onCancel();\n        }\n        finally {\n            segment.close();\n        }\n    }\n\n    @Override\n    public void onComplete() {\n        recorder.setTraceEntity(entity);\n        try {\n            super.onComplete();\n        }\n        catch (Throwable e) {\n            segment.setError(true);\n        }\n        finally {\n            segment.close();\n        }\n    }\n\n}\n```\n\n#### Route metrics to X-Ray daemon\n\nLast but not least, we need to tell X-Ray SDK to forward them to our daemon:\n\n```yaml\n    spec:\n      containers:\n      ...\n        - name: ...\n          env:\n          - name: AWS_XRAY_DAEMON_ADDRESS \n            value: xray-daemon:2000\n```\n\nThe value corresponds to your daemon name.\n\n`AWS_XRAY_DAEMON_ADDRESS` will be read by AWS SDK at runtime.\n\n#### Done\n\nAnd that's it. Just deploy the apps to kube cluster.\nBear in mind that the service map is bound to time range.\nIt won't show up until you get traffic across your apps.\nAnd if you have traffic split like A/B testing or service\nmigration, you'll see how things evolve over time,\nwhich is pretty cool.\n","slug":"Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray","published":1,"updated":"2025-09-01T22:26:51.289Z","comments":1,"layout":"post","photos":[],"_id":"cmf1pfvs700098mmg3sccdwdw","content":"<p>Microservice architecture is typically useful to solve certain scaling problems where service decoupling/segregation is required to improve development velocity, make service more fault tolerant or handle performance hotspots.</p>\n<p>However, everything comes with a price and so does microservice. One typical issue is:</p>\n<img src=\"/2018/11/25/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray/ms-failure.png\" style=\"width: 600px\" />\n\n<p>While this is half joking, monitoring and fault resilency are definitely more challenging in microservice world. While there are frameworks like Hystrix and resilience4j to handle circuit breaking, rate limiting and stuff like that, this post focuses on the first thing: how the heck are my services talking to each other?</p>\n<p>AWS X-Ray can fill the gap here by offering service mapping and tracing and thus you can see something like</p>\n<img src=\"/2018/11/25/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray/xray.png\" class=\"\">\n\n<img src=\"/2018/11/25/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray/tracing.png\" class=\"\">\n\n<p>Compared to generic service monitoring,<br>X-Ray has some additional benefits around AWS ecosystem in that<br>it will auto expose your AWS resource write<br>(yes only write unfortunately) call insights when you use AWS SDK.<br>This applies to SQS, SNS and DynamoDB.</p>\n<span id=\"more\"></span>\n\n<p>But first of all, you need to understand how X-Ray works:</p>\n<ul>\n<li>X-Ray requires application to forward insights to the daemon.<br>In EC2, this means the daemon process alongside with your application.<br>In Kubenetes, this means you’d need to install it as a daemonset so it would run with your node.</li>\n<li>When a request enters the first service (typically an API gateway),<br>the service is responsible for creating the first <code>segment</code> and generate the <code>trace ID</code><br>(typically created by AWS X-Ray SDK).<br>A <code>segment</code> represents the overall lifecycle of a request within <strong>one</strong> application,<br>identified by <code>segment ID</code>.<br>A <code>trace ID</code> identifies the overall roundtrip of a request across <strong>multiple</strong> applications.</li>\n<li>A service, when making requests to other services,<br>should generate corresponding <code>subsegment</code>s.<br>A <code>subsegment</code> is used to identify activities within one application.<br>This is not required for service mapping but nice to have for tracing purposes.</li>\n<li>A service, when accepting traffic from other services,<br>should relay the trace ID and the previous segment ID (called parent ID in SDK).<br>This is such that the service mapping can be generated.</li>\n</ul>\n<p>For inter-service communication, gRPC is often used. Compared to JSON over REST, gRPC offers more flexibility around query design and better performance thanks to the efficiency of (de)serialization with protobuf and the usage of http2 multiplexing. The extra typing and backward compatibility from protobuf also help documentation and maintenance, improving the overall quality of service quorum.</p>\n<p>However, while X-Ray SDK offers J2EE servlet filter for general http servers, gRPC does not follow that. The canonical <a href=\"https://github.com/grpc/grpc-java\">gRPC Java implementation</a> uses netty and has no knowledge around that.</p>\n<p>This means we’d have to write some custom code. Unfortunately the documentation around that is <a href=\"https://grpc.io/docs/quickstart/java.html\">next to none</a>. Luckily, gRPC has implicit support via <code>io.grpc.ServerInterceptor</code> and <code>io.grpc.ClientInterceptor</code> so it’s just a matter of how to wire pieces together.</p>\n<p>Overall there are 4 steps:</p>\n<ol>\n<li>Set up Kubernetes daemonset</li>\n<li>Grant permission to Kubernetes nodes so they can write metrics to X-Ray.</li>\n<li>Write/use interceptors in code</li>\n<li>Route metrics to X-Ray daemon</li>\n</ol>\n<p>Let’s do this step by step:</p>\n<h4 id=\"Set-up-Kubernetes-daemonset\"><a href=\"#Set-up-Kubernetes-daemonset\" class=\"headerlink\" title=\"Set up Kubernetes daemonset\"></a>Set up Kubernetes daemonset</h4><p>There’s an example offered by Amazon regarding how to install it: <a href=\"https://github.com/aws-samples/aws-xray-kubernetes\">link</a></p>\n<h4 id=\"Grant-permission-to-Kubernetes-nodes\"><a href=\"#Grant-permission-to-Kubernetes-nodes\" class=\"headerlink\" title=\"Grant permission to Kubernetes nodes\"></a>Grant permission to Kubernetes nodes</h4><p>This is a bit tricky depending how your kube cluster is set up.</p>\n<p>If you use EKS/EC2, you need to grant X-Ray write permission by<br>attaching the canned policy to your IAM role for the worker nodes.</p>\n<img src=\"/2018/11/25/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray/iam.png\" style=\"width: 400px\" />\n\n<p>If you host your kubenetes outside AWS ecosystem,<br>well chances are you don’t need X-Ray but something generic like Istio’s sidecar approach.<br>But if you do need it then you can create IAM users,<br>attach the policy and use these users in your code.</p>\n<h4 id=\"Write-use-interceptors-in-code\"><a href=\"#Write-use-interceptors-in-code\" class=\"headerlink\" title=\"Write/use interceptors in code\"></a>Write/use interceptors in code</h4><p>First, we need to make sure we use the same language between server and client.<br>In typical HTTP this is the headers.<br>In gRPC this is the metadata, keyed by <code>Key</code>.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Keys</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> Metadata.Key&lt;String&gt; TRACE_ID_HEADER = Metadata.Key.of(<span class=\"string\">&quot;traceId&quot;</span>, Metadata.ASCII_STRING_MARSHALLER);</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> Metadata.Key&lt;String&gt; PARENT_ID_HEADER = Metadata.Key.of(<span class=\"string\">&quot;parentId&quot;</span>, Metadata.ASCII_STRING_MARSHALLER);</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Now, let’s implement client interceptor.</p>\n<p>First you need some X-Ray stuff in classpath<br>(assuming Gradle is used for dependence managmement, should be similar for maven/ivy/sbt):</p>\n<figure class=\"highlight groovy\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dependencies &#123;</span><br><span class=\"line\">  compile (</span><br><span class=\"line\">    <span class=\"string\">&quot;com.amazonaws:aws-xray-recorder-sdk-core&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;com.amazonaws:aws-xray-recorder-sdk-aws-sdk&quot;</span>,</span><br><span class=\"line\">  )</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Note for demonstration purpose the verion is omitted here,<br>for actual usage you should peg the latest version at the time.</p>\n<p>If you want X-Ray to instrument your AWS resource calls, you also need:</p>\n<figure class=\"highlight groovy\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">compile(<span class=\"string\">&quot;com.amazonaws:aws-xray-recorder-sdk-aws-sdk-instrumentor&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<p>Now the code:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">XRayClientInterceptor</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">ClientInterceptor</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"type\">AWSXRayRecorder</span> <span class=\"variable\">recorder</span> <span class=\"operator\">=</span> AWSXRayRecorderBuilder.defaultRecorder();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> &lt;ReqT, RespT&gt; ClientCall&lt;ReqT, RespT&gt; <span class=\"title function_\">interceptCall</span><span class=\"params\">(</span></span><br><span class=\"line\"><span class=\"params\">            MethodDescriptor&lt;ReqT, RespT&gt; method, CallOptions callOptions, Channel next)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">Segment</span> <span class=\"variable\">segment</span> <span class=\"operator\">=</span> recorder.getCurrentSegmentOptional().orElseGet(() -&gt; &#123;</span><br><span class=\"line\">            <span class=\"comment\">//noinspection CodeBlock2Expr</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> recorder.beginSegment(method.getFullMethodName());</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">String</span> <span class=\"variable\">segmentId</span> <span class=\"operator\">=</span> segment.getId();</span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">String</span> <span class=\"variable\">traceId</span> <span class=\"operator\">=</span> segment.getTraceId().toString();</span><br><span class=\"line\">        ClientCall&lt;ReqT, RespT&gt; call = next.newCall(method, callOptions);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> <span class=\"title class_\">ForwardingClientCall</span>.SimpleForwardingClientCall&lt;ReqT, RespT&gt;(call) &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">start</span><span class=\"params\">(Listener&lt;RespT&gt; responseListener, Metadata headers)</span> &#123;</span><br><span class=\"line\">                <span class=\"type\">Subsegment</span> <span class=\"variable\">callSegment</span> <span class=\"operator\">=</span> recorder.beginSubsegment(method.getFullMethodName());</span><br><span class=\"line\">                <span class=\"keyword\">final</span> <span class=\"type\">Entity</span> <span class=\"variable\">context</span> <span class=\"operator\">=</span> recorder.getTraceEntity();</span><br><span class=\"line\">                headers.discardAll(Keys.PARENT_ID_HEADER);</span><br><span class=\"line\">                headers.put(Keys.PARENT_ID_HEADER, segmentId);</span><br><span class=\"line\">                headers.put(Keys.TRACE_ID_HEADER, traceId);</span><br><span class=\"line\">                delegate().start(</span><br><span class=\"line\">                        <span class=\"keyword\">new</span> <span class=\"title class_\">ForwardingClientCallListener</span>.SimpleForwardingClientCallListener&lt;RespT&gt;(responseListener) &#123;</span><br><span class=\"line\">                            <span class=\"meta\">@Override</span></span><br><span class=\"line\">                            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onClose</span><span class=\"params\">(io.grpc.Status status, Metadata trailers)</span> &#123;</span><br><span class=\"line\">                                <span class=\"keyword\">if</span> (status.getCause() != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">                                    callSegment.addException(status.getCause());</span><br><span class=\"line\">                                &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (!status.isOk()) &#123;</span><br><span class=\"line\">                                    callSegment.setError(<span class=\"literal\">true</span>);</span><br><span class=\"line\">                                &#125;</span><br><span class=\"line\">                                <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                                    <span class=\"built_in\">super</span>.onClose(status, trailers);</span><br><span class=\"line\">                                &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">                                    <span class=\"type\">Entity</span> <span class=\"variable\">originalContext</span> <span class=\"operator\">=</span> recorder.getTraceEntity();</span><br><span class=\"line\">                                    recorder.setTraceEntity(context);</span><br><span class=\"line\">                                    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                                        callSegment.close();</span><br><span class=\"line\">                                    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">                                        recorder.setTraceEntity(originalContext);</span><br><span class=\"line\">                                    &#125;</span><br><span class=\"line\">                                &#125;</span><br><span class=\"line\">                            &#125;</span><br><span class=\"line\">                        &#125;,</span><br><span class=\"line\">                        headers);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>There’s quite a lot of code here but the key gotchas are:</p>\n<ul>\n<li>You should always use an existing segment if one exists,<br>which is what <code>getCurrentSegmentOptional()</code> is for.<br>Fail to do so would result in the loss of previous segment.<br>If in some other code the previous segment is still referenced,<br>you will get missing context exceptions when trying to close it.</li>\n<li>Always bear in mind that data streaming/async handling is baked in gRPC design.<br>So never close the segment directly after starting forwarding the client call.<br>Instead, implement <code>ClientCallListener</code> and let gRPC tell you when<br>it actually starts/finishes it.</li>\n<li><code>AWSXRayRecorder</code> is thread safe so using one for all calls should be fine.<br>However, all the segments are tracked via <code>ThreadLocalSegmentContext</code> by default.<br>That is shared by <strong>all</strong> instances across the entire app by default<br>even if you have multiple <code>AWSXRayRecorder</code> instances.<br>What that implies is you should<br><strong>always remember the corresponding context for that segment/subsegment</strong>,<br>especially when crossing threads. Failure to do so would result in weird errors.<br>This is what <code>getTraceEntity()</code> and <code>setTraceEntity()</code> are for.</li>\n<li>The <code>put()</code> calls would append if key with the same name already exists.<br>So remember to clean it up first.<br>The trace ID meta doesn’t need to be cleared because<br>it’s supposed to be the same as mentioned.</li>\n</ul>\n<p>After that, wire it up when you build the client:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">newBlockingStub(channel).withInterceptors(<span class=\"keyword\">new</span> <span class=\"title class_\">XRayClientInterceptor</span>());</span><br></pre></td></tr></table></figure>\n\n<p>Next, let’s build the server side interceptor:</p>\n<p>This has some extra flavors in that it assumes you use a spring based<br>gRPC server like the <a href=\"https://github.com/LogNet/grpc-spring-boot-starter\">LogNet Springboot</a> one.<br>The <code>GRpcGlobalInterceptor</code> would tell the runner to inject the interceptor automagically.<br>If that’s not the case, that’s fine,<br>just replace the <code>appName</code> with some other logic,<br>and wire up the interceptor using <code>ServerInterceptors.intercept(serviceDefinition, interceptors)</code>.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@GRpcGlobalInterceptor</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">XRayServerInterceptor</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">ServerInterceptor</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Value(&quot;$&#123;spring.application.name&#125;&quot;)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String appName;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> &lt;ReqT, RespT&gt; ServerCall.Listener&lt;ReqT&gt; <span class=\"title function_\">interceptCall</span><span class=\"params\">(ServerCall&lt;ReqT, RespT&gt; call, Metadata headers, ServerCallHandler&lt;ReqT, RespT&gt; next)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">traceId</span> <span class=\"operator\">=</span> headers.get(Keys.TRACE_ID_HEADER);</span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">parentId</span> <span class=\"operator\">=</span> headers.get(Keys.PARENT_ID_HEADER);</span><br><span class=\"line\">        <span class=\"type\">TraceID</span> <span class=\"variable\">tId</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">TraceID</span>();</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (traceId != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">            tId = TraceID.fromString(traceId);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"type\">Segment</span> <span class=\"variable\">segment</span> <span class=\"operator\">=</span> recorder.beginSegment(appName, tId, parentId);</span><br><span class=\"line\">        headers.discardAll(Keys.PARENT_ID_HEADER);</span><br><span class=\"line\">        headers.discardAll(Keys.TRACE_ID_HEADER);</span><br><span class=\"line\">        headers.put(Keys.PARENT_ID_HEADER, segment.getId());</span><br><span class=\"line\">        headers.put(Keys.TRACE_ID_HEADER, tId.toString());</span><br><span class=\"line\">        ServerCall.Listener&lt;ReqT&gt; listener = next.startCall(call, headers);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> <span class=\"title class_\">ForwardingListener</span>&lt;&gt;(listener, call, recorder, recorder.getTraceEntity(), segment);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">ForwardingListener</span>&lt;T, R&gt;</span><br><span class=\"line\">        <span class=\"keyword\">extends</span> <span class=\"title class_\">ForwardingServerCallListener</span>.SimpleForwardingServerCallListener&lt;T&gt; &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> ServerCall&lt;T, R&gt; call;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> AWSXRayRecorder recorder;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> Entity entity;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> Segment segment;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"title function_\">ForwardingListener</span><span class=\"params\">(ServerCall.Listener&lt;T&gt; delegate,</span></span><br><span class=\"line\"><span class=\"params\">            ServerCall&lt;T, R&gt; call,</span></span><br><span class=\"line\"><span class=\"params\">            AWSXRayRecorder recorder,</span></span><br><span class=\"line\"><span class=\"params\">            Entity entity,</span></span><br><span class=\"line\"><span class=\"params\">            Segment segment</span></span><br><span class=\"line\"><span class=\"params\">    )</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(delegate);</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.call = call;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.recorder = recorder;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.entity = entity;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.segment = segment;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onCancel</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        recorder.setTraceEntity(entity);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (call.isCancelled()) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        segment.setFault(<span class=\"literal\">true</span>);</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"built_in\">super</span>.onCancel();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">            segment.close();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onComplete</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        recorder.setTraceEntity(entity);</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"built_in\">super</span>.onComplete();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">catch</span> (Throwable e) &#123;</span><br><span class=\"line\">            segment.setError(<span class=\"literal\">true</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">            segment.close();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Route-metrics-to-X-Ray-daemon\"><a href=\"#Route-metrics-to-X-Ray-daemon\" class=\"headerlink\" title=\"Route metrics to X-Ray daemon\"></a>Route metrics to X-Ray daemon</h4><p>Last but not least, we need to tell X-Ray SDK to forward them to our daemon:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">  <span class=\"attr\">containers:</span></span><br><span class=\"line\">  <span class=\"string\">...</span></span><br><span class=\"line\">    <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">...</span></span><br><span class=\"line\">      <span class=\"attr\">env:</span></span><br><span class=\"line\">      <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">AWS_XRAY_DAEMON_ADDRESS</span> </span><br><span class=\"line\">        <span class=\"attr\">value:</span> <span class=\"string\">xray-daemon:2000</span></span><br></pre></td></tr></table></figure>\n\n<p>The value corresponds to your daemon name.</p>\n<p><code>AWS_XRAY_DAEMON_ADDRESS</code> will be read by AWS SDK at runtime.</p>\n<h4 id=\"Done\"><a href=\"#Done\" class=\"headerlink\" title=\"Done\"></a>Done</h4><p>And that’s it. Just deploy the apps to kube cluster.<br>Bear in mind that the service map is bound to time range.<br>It won’t show up until you get traffic across your apps.<br>And if you have traffic split like A/B testing or service<br>migration, you’ll see how things evolve over time,<br>which is pretty cool.</p>\n","thumbnailImageUrl":"https://xunnanxu.github.io/2018/11/25/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray/xray.png","excerpt":"<p>Microservice architecture is typically useful to solve certain scaling problems where service decoupling/segregation is required to improve development velocity, make service more fault tolerant or handle performance hotspots.</p>\n<p>However, everything comes with a price and so does microservice. One typical issue is:</p>\n<img src=\"/2018/11/25/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray/ms-failure.png\" style=\"width: 600px\" />\n\n<p>While this is half joking, monitoring and fault resilency are definitely more challenging in microservice world. While there are frameworks like Hystrix and resilience4j to handle circuit breaking, rate limiting and stuff like that, this post focuses on the first thing: how the heck are my services talking to each other?</p>\n<p>AWS X-Ray can fill the gap here by offering service mapping and tracing and thus you can see something like</p>\n<img src=\"/2018/11/25/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray/xray.png\" class=\"\">\n\n<img src=\"/2018/11/25/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray/tracing.png\" class=\"\">\n\n<p>Compared to generic service monitoring,<br>X-Ray has some additional benefits around AWS ecosystem in that<br>it will auto expose your AWS resource write<br>(yes only write unfortunately) call insights when you use AWS SDK.<br>This applies to SQS, SNS and DynamoDB.</p>","more":"<p>But first of all, you need to understand how X-Ray works:</p>\n<ul>\n<li>X-Ray requires application to forward insights to the daemon.<br>In EC2, this means the daemon process alongside with your application.<br>In Kubenetes, this means you’d need to install it as a daemonset so it would run with your node.</li>\n<li>When a request enters the first service (typically an API gateway),<br>the service is responsible for creating the first <code>segment</code> and generate the <code>trace ID</code><br>(typically created by AWS X-Ray SDK).<br>A <code>segment</code> represents the overall lifecycle of a request within <strong>one</strong> application,<br>identified by <code>segment ID</code>.<br>A <code>trace ID</code> identifies the overall roundtrip of a request across <strong>multiple</strong> applications.</li>\n<li>A service, when making requests to other services,<br>should generate corresponding <code>subsegment</code>s.<br>A <code>subsegment</code> is used to identify activities within one application.<br>This is not required for service mapping but nice to have for tracing purposes.</li>\n<li>A service, when accepting traffic from other services,<br>should relay the trace ID and the previous segment ID (called parent ID in SDK).<br>This is such that the service mapping can be generated.</li>\n</ul>\n<p>For inter-service communication, gRPC is often used. Compared to JSON over REST, gRPC offers more flexibility around query design and better performance thanks to the efficiency of (de)serialization with protobuf and the usage of http2 multiplexing. The extra typing and backward compatibility from protobuf also help documentation and maintenance, improving the overall quality of service quorum.</p>\n<p>However, while X-Ray SDK offers J2EE servlet filter for general http servers, gRPC does not follow that. The canonical <a href=\"https://github.com/grpc/grpc-java\">gRPC Java implementation</a> uses netty and has no knowledge around that.</p>\n<p>This means we’d have to write some custom code. Unfortunately the documentation around that is <a href=\"https://grpc.io/docs/quickstart/java.html\">next to none</a>. Luckily, gRPC has implicit support via <code>io.grpc.ServerInterceptor</code> and <code>io.grpc.ClientInterceptor</code> so it’s just a matter of how to wire pieces together.</p>\n<p>Overall there are 4 steps:</p>\n<ol>\n<li>Set up Kubernetes daemonset</li>\n<li>Grant permission to Kubernetes nodes so they can write metrics to X-Ray.</li>\n<li>Write/use interceptors in code</li>\n<li>Route metrics to X-Ray daemon</li>\n</ol>\n<p>Let’s do this step by step:</p>\n<h4 id=\"Set-up-Kubernetes-daemonset\"><a href=\"#Set-up-Kubernetes-daemonset\" class=\"headerlink\" title=\"Set up Kubernetes daemonset\"></a>Set up Kubernetes daemonset</h4><p>There’s an example offered by Amazon regarding how to install it: <a href=\"https://github.com/aws-samples/aws-xray-kubernetes\">link</a></p>\n<h4 id=\"Grant-permission-to-Kubernetes-nodes\"><a href=\"#Grant-permission-to-Kubernetes-nodes\" class=\"headerlink\" title=\"Grant permission to Kubernetes nodes\"></a>Grant permission to Kubernetes nodes</h4><p>This is a bit tricky depending how your kube cluster is set up.</p>\n<p>If you use EKS/EC2, you need to grant X-Ray write permission by<br>attaching the canned policy to your IAM role for the worker nodes.</p>\n<img src=\"/2018/11/25/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray/iam.png\" style=\"width: 400px\" />\n\n<p>If you host your kubenetes outside AWS ecosystem,<br>well chances are you don’t need X-Ray but something generic like Istio’s sidecar approach.<br>But if you do need it then you can create IAM users,<br>attach the policy and use these users in your code.</p>\n<h4 id=\"Write-use-interceptors-in-code\"><a href=\"#Write-use-interceptors-in-code\" class=\"headerlink\" title=\"Write/use interceptors in code\"></a>Write/use interceptors in code</h4><p>First, we need to make sure we use the same language between server and client.<br>In typical HTTP this is the headers.<br>In gRPC this is the metadata, keyed by <code>Key</code>.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Keys</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> Metadata.Key&lt;String&gt; TRACE_ID_HEADER = Metadata.Key.of(<span class=\"string\">&quot;traceId&quot;</span>, Metadata.ASCII_STRING_MARSHALLER);</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> Metadata.Key&lt;String&gt; PARENT_ID_HEADER = Metadata.Key.of(<span class=\"string\">&quot;parentId&quot;</span>, Metadata.ASCII_STRING_MARSHALLER);</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Now, let’s implement client interceptor.</p>\n<p>First you need some X-Ray stuff in classpath<br>(assuming Gradle is used for dependence managmement, should be similar for maven/ivy/sbt):</p>\n<figure class=\"highlight groovy\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dependencies &#123;</span><br><span class=\"line\">  compile (</span><br><span class=\"line\">    <span class=\"string\">&quot;com.amazonaws:aws-xray-recorder-sdk-core&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;com.amazonaws:aws-xray-recorder-sdk-aws-sdk&quot;</span>,</span><br><span class=\"line\">  )</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Note for demonstration purpose the verion is omitted here,<br>for actual usage you should peg the latest version at the time.</p>\n<p>If you want X-Ray to instrument your AWS resource calls, you also need:</p>\n<figure class=\"highlight groovy\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">compile(<span class=\"string\">&quot;com.amazonaws:aws-xray-recorder-sdk-aws-sdk-instrumentor&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<p>Now the code:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">XRayClientInterceptor</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">ClientInterceptor</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"type\">AWSXRayRecorder</span> <span class=\"variable\">recorder</span> <span class=\"operator\">=</span> AWSXRayRecorderBuilder.defaultRecorder();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> &lt;ReqT, RespT&gt; ClientCall&lt;ReqT, RespT&gt; <span class=\"title function_\">interceptCall</span><span class=\"params\">(</span></span><br><span class=\"line\"><span class=\"params\">            MethodDescriptor&lt;ReqT, RespT&gt; method, CallOptions callOptions, Channel next)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">Segment</span> <span class=\"variable\">segment</span> <span class=\"operator\">=</span> recorder.getCurrentSegmentOptional().orElseGet(() -&gt; &#123;</span><br><span class=\"line\">            <span class=\"comment\">//noinspection CodeBlock2Expr</span></span><br><span class=\"line\">            <span class=\"keyword\">return</span> recorder.beginSegment(method.getFullMethodName());</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">String</span> <span class=\"variable\">segmentId</span> <span class=\"operator\">=</span> segment.getId();</span><br><span class=\"line\">        <span class=\"keyword\">final</span> <span class=\"type\">String</span> <span class=\"variable\">traceId</span> <span class=\"operator\">=</span> segment.getTraceId().toString();</span><br><span class=\"line\">        ClientCall&lt;ReqT, RespT&gt; call = next.newCall(method, callOptions);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> <span class=\"title class_\">ForwardingClientCall</span>.SimpleForwardingClientCall&lt;ReqT, RespT&gt;(call) &#123;</span><br><span class=\"line\">            <span class=\"meta\">@Override</span></span><br><span class=\"line\">            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">start</span><span class=\"params\">(Listener&lt;RespT&gt; responseListener, Metadata headers)</span> &#123;</span><br><span class=\"line\">                <span class=\"type\">Subsegment</span> <span class=\"variable\">callSegment</span> <span class=\"operator\">=</span> recorder.beginSubsegment(method.getFullMethodName());</span><br><span class=\"line\">                <span class=\"keyword\">final</span> <span class=\"type\">Entity</span> <span class=\"variable\">context</span> <span class=\"operator\">=</span> recorder.getTraceEntity();</span><br><span class=\"line\">                headers.discardAll(Keys.PARENT_ID_HEADER);</span><br><span class=\"line\">                headers.put(Keys.PARENT_ID_HEADER, segmentId);</span><br><span class=\"line\">                headers.put(Keys.TRACE_ID_HEADER, traceId);</span><br><span class=\"line\">                delegate().start(</span><br><span class=\"line\">                        <span class=\"keyword\">new</span> <span class=\"title class_\">ForwardingClientCallListener</span>.SimpleForwardingClientCallListener&lt;RespT&gt;(responseListener) &#123;</span><br><span class=\"line\">                            <span class=\"meta\">@Override</span></span><br><span class=\"line\">                            <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onClose</span><span class=\"params\">(io.grpc.Status status, Metadata trailers)</span> &#123;</span><br><span class=\"line\">                                <span class=\"keyword\">if</span> (status.getCause() != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">                                    callSegment.addException(status.getCause());</span><br><span class=\"line\">                                &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (!status.isOk()) &#123;</span><br><span class=\"line\">                                    callSegment.setError(<span class=\"literal\">true</span>);</span><br><span class=\"line\">                                &#125;</span><br><span class=\"line\">                                <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                                    <span class=\"built_in\">super</span>.onClose(status, trailers);</span><br><span class=\"line\">                                &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">                                    <span class=\"type\">Entity</span> <span class=\"variable\">originalContext</span> <span class=\"operator\">=</span> recorder.getTraceEntity();</span><br><span class=\"line\">                                    recorder.setTraceEntity(context);</span><br><span class=\"line\">                                    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">                                        callSegment.close();</span><br><span class=\"line\">                                    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">                                        recorder.setTraceEntity(originalContext);</span><br><span class=\"line\">                                    &#125;</span><br><span class=\"line\">                                &#125;</span><br><span class=\"line\">                            &#125;</span><br><span class=\"line\">                        &#125;,</span><br><span class=\"line\">                        headers);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>There’s quite a lot of code here but the key gotchas are:</p>\n<ul>\n<li>You should always use an existing segment if one exists,<br>which is what <code>getCurrentSegmentOptional()</code> is for.<br>Fail to do so would result in the loss of previous segment.<br>If in some other code the previous segment is still referenced,<br>you will get missing context exceptions when trying to close it.</li>\n<li>Always bear in mind that data streaming/async handling is baked in gRPC design.<br>So never close the segment directly after starting forwarding the client call.<br>Instead, implement <code>ClientCallListener</code> and let gRPC tell you when<br>it actually starts/finishes it.</li>\n<li><code>AWSXRayRecorder</code> is thread safe so using one for all calls should be fine.<br>However, all the segments are tracked via <code>ThreadLocalSegmentContext</code> by default.<br>That is shared by <strong>all</strong> instances across the entire app by default<br>even if you have multiple <code>AWSXRayRecorder</code> instances.<br>What that implies is you should<br><strong>always remember the corresponding context for that segment/subsegment</strong>,<br>especially when crossing threads. Failure to do so would result in weird errors.<br>This is what <code>getTraceEntity()</code> and <code>setTraceEntity()</code> are for.</li>\n<li>The <code>put()</code> calls would append if key with the same name already exists.<br>So remember to clean it up first.<br>The trace ID meta doesn’t need to be cleared because<br>it’s supposed to be the same as mentioned.</li>\n</ul>\n<p>After that, wire it up when you build the client:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">newBlockingStub(channel).withInterceptors(<span class=\"keyword\">new</span> <span class=\"title class_\">XRayClientInterceptor</span>());</span><br></pre></td></tr></table></figure>\n\n<p>Next, let’s build the server side interceptor:</p>\n<p>This has some extra flavors in that it assumes you use a spring based<br>gRPC server like the <a href=\"https://github.com/LogNet/grpc-spring-boot-starter\">LogNet Springboot</a> one.<br>The <code>GRpcGlobalInterceptor</code> would tell the runner to inject the interceptor automagically.<br>If that’s not the case, that’s fine,<br>just replace the <code>appName</code> with some other logic,<br>and wire up the interceptor using <code>ServerInterceptors.intercept(serviceDefinition, interceptors)</code>.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">@GRpcGlobalInterceptor</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">XRayServerInterceptor</span> <span class=\"keyword\">implements</span> <span class=\"title class_\">ServerInterceptor</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Value(&quot;$&#123;spring.application.name&#125;&quot;)</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> String appName;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> &lt;ReqT, RespT&gt; ServerCall.Listener&lt;ReqT&gt; <span class=\"title function_\">interceptCall</span><span class=\"params\">(ServerCall&lt;ReqT, RespT&gt; call, Metadata headers, ServerCallHandler&lt;ReqT, RespT&gt; next)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">traceId</span> <span class=\"operator\">=</span> headers.get(Keys.TRACE_ID_HEADER);</span><br><span class=\"line\">        <span class=\"type\">String</span> <span class=\"variable\">parentId</span> <span class=\"operator\">=</span> headers.get(Keys.PARENT_ID_HEADER);</span><br><span class=\"line\">        <span class=\"type\">TraceID</span> <span class=\"variable\">tId</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">TraceID</span>();</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (traceId != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">            tId = TraceID.fromString(traceId);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"type\">Segment</span> <span class=\"variable\">segment</span> <span class=\"operator\">=</span> recorder.beginSegment(appName, tId, parentId);</span><br><span class=\"line\">        headers.discardAll(Keys.PARENT_ID_HEADER);</span><br><span class=\"line\">        headers.discardAll(Keys.TRACE_ID_HEADER);</span><br><span class=\"line\">        headers.put(Keys.PARENT_ID_HEADER, segment.getId());</span><br><span class=\"line\">        headers.put(Keys.TRACE_ID_HEADER, tId.toString());</span><br><span class=\"line\">        ServerCall.Listener&lt;ReqT&gt; listener = next.startCall(call, headers);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> <span class=\"title class_\">ForwardingListener</span>&lt;&gt;(listener, call, recorder, recorder.getTraceEntity(), segment);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">ForwardingListener</span>&lt;T, R&gt;</span><br><span class=\"line\">        <span class=\"keyword\">extends</span> <span class=\"title class_\">ForwardingServerCallListener</span>.SimpleForwardingServerCallListener&lt;T&gt; &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> ServerCall&lt;T, R&gt; call;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> AWSXRayRecorder recorder;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> Entity entity;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> Segment segment;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"title function_\">ForwardingListener</span><span class=\"params\">(ServerCall.Listener&lt;T&gt; delegate,</span></span><br><span class=\"line\"><span class=\"params\">            ServerCall&lt;T, R&gt; call,</span></span><br><span class=\"line\"><span class=\"params\">            AWSXRayRecorder recorder,</span></span><br><span class=\"line\"><span class=\"params\">            Entity entity,</span></span><br><span class=\"line\"><span class=\"params\">            Segment segment</span></span><br><span class=\"line\"><span class=\"params\">    )</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(delegate);</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.call = call;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.recorder = recorder;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.entity = entity;</span><br><span class=\"line\">        <span class=\"built_in\">this</span>.segment = segment;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onCancel</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        recorder.setTraceEntity(entity);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (call.isCancelled()) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">return</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        segment.setFault(<span class=\"literal\">true</span>);</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"built_in\">super</span>.onCancel();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">            segment.close();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"meta\">@Override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onComplete</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        recorder.setTraceEntity(entity);</span><br><span class=\"line\">        <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">            <span class=\"built_in\">super</span>.onComplete();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">catch</span> (Throwable e) &#123;</span><br><span class=\"line\">            segment.setError(<span class=\"literal\">true</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">            segment.close();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Route-metrics-to-X-Ray-daemon\"><a href=\"#Route-metrics-to-X-Ray-daemon\" class=\"headerlink\" title=\"Route metrics to X-Ray daemon\"></a>Route metrics to X-Ray daemon</h4><p>Last but not least, we need to tell X-Ray SDK to forward them to our daemon:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">  <span class=\"attr\">containers:</span></span><br><span class=\"line\">  <span class=\"string\">...</span></span><br><span class=\"line\">    <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">...</span></span><br><span class=\"line\">      <span class=\"attr\">env:</span></span><br><span class=\"line\">      <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">AWS_XRAY_DAEMON_ADDRESS</span> </span><br><span class=\"line\">        <span class=\"attr\">value:</span> <span class=\"string\">xray-daemon:2000</span></span><br></pre></td></tr></table></figure>\n\n<p>The value corresponds to your daemon name.</p>\n<p><code>AWS_XRAY_DAEMON_ADDRESS</code> will be read by AWS SDK at runtime.</p>\n<h4 id=\"Done\"><a href=\"#Done\" class=\"headerlink\" title=\"Done\"></a>Done</h4><p>And that’s it. Just deploy the apps to kube cluster.<br>Bear in mind that the service map is bound to time range.<br>It won’t show up until you get traffic across your apps.<br>And if you have traffic split like A/B testing or service<br>migration, you’ll see how things evolve over time,<br>which is pretty cool.</p>"},{"title":"PXE Boot Diskless Raspberry Pi 4 With Ubuntu, Ubiquiti and Synology (1): DHCP Setup","date":"2020-11-28T20:00:16.000Z","_content":"\nRaspberry Pi 4 is shipped with a flashable EEPROM and supports netbooting. However, the entire setup process is not that straightforward and it's thus worth writing down all the pitfalls through the path especially when it involves a non-\"native\" Linux distribution.\n\nOne thing worth noting is that even though this post mentions \"PXE\" and so does the official document, the boot process isn't entirely PXE compliant and thus regular PXE boot support setup process may not work at all or at least not directly.\n\nHere's a diagram of the network topology I use for the setup:\n\n<img src=\"{% asset_path net-topo.png %}\" />\n\nThe Ubiquiti will also be serving as the DHCP server. The Synology NAS will be serving data via tftp (pxe boot) and nfs (post boot). Due to kernel limitation, my NAS cannot support overlayfs. But for newer version of NAS it might be possible.\n\n## Raspberry Netboot Sequence\n\nThe bootloader in EEPROM by default does not enable netboot so we'd need to enable that first.\n\nOnce enabled, upon powering up, Pi will first send a DHCP request to ask for the TFTP server location and verify if netboot is supported.\n\nAfter that it will fetch the firmware (start4.elf), configs and kernel files (vmlinuz) from that tftp location. The bootloader has a config to specify the exact path for the current device. By default, that's the last 9 chars of the serial number. This is important as certain config (like cmdline.txt) would tell the OS how to mount the root fs and thus needs to be separate.\n\nThe kernel will later be loaded which takes over the boot process and eventually loads the rest of the system distribution.\n\nThe official doc can be found [here](https://www.raspberrypi.org/documentation/hardware/raspberrypi/bootmodes/net.md).\n\n## Flashing the EEPROM to Enable Netboot\n\nWith the basic knowledge equipped, we can continue by first enabling the netboot in bootloader.\n\nTo do that, we need to grab the updated version of bootcode.bin file which supports netboot and update the config.\n\nThis also requires using the raspberry cli binary vcgencmd.\n\n**Pitfall #1: vcgencmd can only work in native Raspberry Pi OS**\n\nYes the vcgencmd binary does not work in Ubuntu or any other \"supported\" Linux distros, even if you compile from source. They would just silently fail. Technically it should be doable if they are shipped with the right \"stuff\" but I decide not to waste more time figuring out what the \"stuff\" is.\n\nHere we need a small microSD card to flash the Raspberry Pi OS onto it and boot the system. For headless setup, don't forget to `touch ssh` to create the file to enable ssh by default.\n\n<!-- more -->\n\nThe default user name and password is `pi` and `raspberry`.\n\nRun\n\n```\nvcgencmd bootloader_config | grep BOOT_ORDER | cut -d '=' -f 2\n```\n\nTo check the current boot order. The boot order is a sequence of digits from right to left. The default is `0x1` (or `0xf41` in later version) which means it's SD card only (or SD -> USB -> restart). We'll change this to `0xf21` (SD -> Network -> restart).\n\nThe full doc about order can be found [here](https://www.raspberrypi.org/documentation/hardware/raspberrypi/bcm2711_bootloader_config.md).\n\nTo do that, first we need to grab a bootloader that supports netboot. Anything late 2020 should work. For example to grab 2020-07-31 version:\n\n```\nwget https://github.com/raspberrypi/rpi-eeprom/raw/master/firmware/stable/pieeprom-2020-07-31.bin\n```\n\nExtract the `bootconf.txt` file and update order:\n\n```\nsudo rpi-eeprom-config pieeprom-2020-07-31.bin > bootconf.txt\nsed -i 's/BOOT_ORDER=.*/BOOT_ORDER=0xf21/g' bootconf.txt\nsudo rpi-eeprom-config --out bootloader.bin --config bootconf.txt pieeprom-2020-07-31.bin\n```\n\nFlash the EEPROM:\n\n```\nsudo rpi-eeprom-update -d -f bootloader.bin\n```\n\nBefore exiting, grab the serial number:\n\n```\ncat /proc/cpuinfo | grep Serial | tail -c 9\n```\n\nAssume this is `123456789`. This will be used in later tftp setup.\n\nUnplug, remove the SD card and replug the Pi. Now it should be stuck on the initial self test screen as it can neither boot from SD or net. This is expected.\n\n## DHCP Setup\n\nNow it comes to the router side as we need to enable the extra options of DHCP such that Pi can find the NAS in the same network.\n\nThis step is vendor specific. I use a Ubiquiti router. The command should be more or less the same for anything using OpenWRT.\n\nTo enable PXE boot, we first need to figure out what options Pi is asking for. In this case we need `tcpdump`.\n\n```\nsudo tcpdump -vnes0 port 67 or port 68\n```\n\n`-v` verbose (to list options)\n`-n` do not try to look up ip address\n`-e` list mac address\n`-s0` do not truncate packet and show full content\n`port 67` is for client -> server packets and `68` is for the other way around\n\nRestart the Pi to capture the packets.\n\nThe DHCP protocol has 4 parts for our interest:\n1. `Discover` - this is the initial DHCP broadcast request and we need to pay attention to the `Parameter-Request` part.\n2. `Offer` - DHCP server responds with allocated IP address and option responses\n3. `Request` - Client echoes the IP back to server to confirm allocation\n4. `ACK` - DHCP server confirms the echo\n\nHere's what Pi sends as the initial request:\n\n```\n19:45:26.963111 <PI MAC ADDR> > ff:ff:ff:ff:ff:ff, ethertype IPv4 (0x0800), length 364: (tos 0x0, ttl 64, id 20586, offset 0, flags [none], proto UDP (17), length 350)\n    0.0.0.0.68 > 255.255.255.255.67: BOOTP/DHCP, Request from <PI MAC ADDR>, length 322, xid 0xd5ec86d6, Flags [none]\n          Client-Ethernet-Address <PI MAC ADDR>\n          Vendor-rfc1048 Extensions\n            Magic Cookie 0x63825363\n            DHCP-Message Option 53, length 1: Discover\n            Parameter-Request Option 55, length 14:\n              Subnet-Mask, Default-Gateway, Vendor-Option, Vendor-Class\n              TFTP, BF, Option 128, Option 129\n              Option 130, Option 131, Option 132, Option 133\n              Option 134, Option 135\n            Vendor-Class Option 60, length 32: \"PXEClient:Arch:00000:UNDI:002001\"\n            ARCH Option 93, length 2: 0\n            NDI Option 94, length 3: 1.2.1\n            GUID Option 97, length 17: 0.82.80.105.52.18.49.192.0.50.167.237.137.157.110.239.229\n```\n\nHere the common options are in plain text like `Subnet-Mask` while some option extensions are just using numbers. The exact reference can be seen in [RFC2132](https://tools.ietf.org/html/rfc2132)\n\nWhat we need are:\n\n`TFTP (66)` - tftp server ip/name -> this should point to the NAS server ip\n`BF (67)` - boot file name -> this technically shouldn't matter for us as the bootloader is directly in Pi's EEPROM but for consistency we will set this to `bootcode.bin`\n`Vendor-Option (43)` - This is important and MUST include `Raspberry Pi Boot`\n`Vendor-Class (60)` - TBH can't remember if this matters for Pi but I set it to `PXEClient` anyways to be PXE compliant in general.\n\nTo set these in Ubiquiti router:\n\nUse `configure` to enter config editing mode.\n\nThe configs are tree-like and we can list the current options using\n\n```\nshow service dhcp-server\n```\n\nHere certain options are known to the router and we can set directly like `tftp-server-name`:\n\nHere the NAS is allocated with a static IP `192.168.1.60` so:\n\n```\nset service dhcp-server shared-network-name LAN subnet 192.168.1.0/24 tftp-server-name 192.168.1.60\nset service dhcp-server shared-network-name LAN subnet 192.168.1.0/24 bootfile-name bootcode.bin\n```\n\nFor other stuff it would be a bit tricky as we need to first tell the router what they are:\n\n```\nset service dhcp-server global-parameters \"option vendor-class-identifier code 60 = string;\"\nset service dhcp-server global-parameters \"option vendor-encapsulated-options code 43 = string;\"\n```\n\nand then set them\n\n```\nset service dhcp-server shared-network-name LAN subnet 192.168.1.0/24 subnet-parameters \"option vendor-class-identifier &quot;PXEClient&quot;;\"\nset service dhcp-server shared-network-name LAN subnet 192.168.1.0/24 subnet-parameters \"option vendor-encapsulated-options &quot;Raspberry Pi Boot   &quot;;\"\n```\n\nThe `&quot;` part is to actually include a double quote there.\n\nDon't forget to comment and save the config:\n```\ncommit; save \n```\n\nThe router should pick up the new config and restart DHCP server as needed. If not reboot it to force the reload.\n\nNow if we tcpdump we'll see the response packet with something like this:\n```\n19:45:27.963654 <ROUTER MAC ADDR> > <PI MAC ADDR>, ethertype IPv4 (0x0800), length 371: (tos 0x10, ttl 128, id 0, offset 0, flags [none], proto UDP (17), length 357)\n    192.168.1.1.67 > 192.168.1.137.68: BOOTP/DHCP, Reply, length 329, xid 0xd5ec86d6, Flags [none]\n          Your-IP 192.168.1.137\n          Server-IP 192.168.1.60\n          Client-Ethernet-Address <PI MAC ADDR>\n          file \"bootcode.bin\"\n          Vendor-rfc1048 Extensions\n            DHCP-Message Option 53, length 1: Offer\n            Server-ID Option 54, length 4: 192.168.1.1\n            Lease-Time Option 51, length 4: 86400\n            Subnet-Mask Option 1, length 4: 255.255.255.0\n            Default-Gateway Option 3, length 4: 192.168.1.1\n            Vendor-Option Option 43, length 20: 82.97.115.112.98.101.114.114.121.32.80.105.32.66.111.111.116.32.32.32\n            Vendor-Class Option 60, length 9: \"PXEClient\"\n            TFTP Option 66, length 12: \"192.168.1.60\"\n            BF Option 67, length 12: \"bootcode.bin\"\n```\n\nOf course in theory this can be further optimized to include the additional info only when vendor-class includes `PXEClient` but I omitted that part.\n\nWith these in place, the Pi should now be stuck accessing the TFTP server. Congrats and we will cover that in the next part.\n","source":"_posts/PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-1-DHCP-Setup.md","raw":"---\ntitle: >-\n  PXE Boot Diskless Raspberry Pi 4 With Ubuntu, Ubiquiti and Synology (1): DHCP\n  Setup\ndate: 2020-11-28 12:00:16\ntags:\n- linux\n- raspberry\n- network\n---\n\nRaspberry Pi 4 is shipped with a flashable EEPROM and supports netbooting. However, the entire setup process is not that straightforward and it's thus worth writing down all the pitfalls through the path especially when it involves a non-\"native\" Linux distribution.\n\nOne thing worth noting is that even though this post mentions \"PXE\" and so does the official document, the boot process isn't entirely PXE compliant and thus regular PXE boot support setup process may not work at all or at least not directly.\n\nHere's a diagram of the network topology I use for the setup:\n\n<img src=\"{% asset_path net-topo.png %}\" />\n\nThe Ubiquiti will also be serving as the DHCP server. The Synology NAS will be serving data via tftp (pxe boot) and nfs (post boot). Due to kernel limitation, my NAS cannot support overlayfs. But for newer version of NAS it might be possible.\n\n## Raspberry Netboot Sequence\n\nThe bootloader in EEPROM by default does not enable netboot so we'd need to enable that first.\n\nOnce enabled, upon powering up, Pi will first send a DHCP request to ask for the TFTP server location and verify if netboot is supported.\n\nAfter that it will fetch the firmware (start4.elf), configs and kernel files (vmlinuz) from that tftp location. The bootloader has a config to specify the exact path for the current device. By default, that's the last 9 chars of the serial number. This is important as certain config (like cmdline.txt) would tell the OS how to mount the root fs and thus needs to be separate.\n\nThe kernel will later be loaded which takes over the boot process and eventually loads the rest of the system distribution.\n\nThe official doc can be found [here](https://www.raspberrypi.org/documentation/hardware/raspberrypi/bootmodes/net.md).\n\n## Flashing the EEPROM to Enable Netboot\n\nWith the basic knowledge equipped, we can continue by first enabling the netboot in bootloader.\n\nTo do that, we need to grab the updated version of bootcode.bin file which supports netboot and update the config.\n\nThis also requires using the raspberry cli binary vcgencmd.\n\n**Pitfall #1: vcgencmd can only work in native Raspberry Pi OS**\n\nYes the vcgencmd binary does not work in Ubuntu or any other \"supported\" Linux distros, even if you compile from source. They would just silently fail. Technically it should be doable if they are shipped with the right \"stuff\" but I decide not to waste more time figuring out what the \"stuff\" is.\n\nHere we need a small microSD card to flash the Raspberry Pi OS onto it and boot the system. For headless setup, don't forget to `touch ssh` to create the file to enable ssh by default.\n\n<!-- more -->\n\nThe default user name and password is `pi` and `raspberry`.\n\nRun\n\n```\nvcgencmd bootloader_config | grep BOOT_ORDER | cut -d '=' -f 2\n```\n\nTo check the current boot order. The boot order is a sequence of digits from right to left. The default is `0x1` (or `0xf41` in later version) which means it's SD card only (or SD -> USB -> restart). We'll change this to `0xf21` (SD -> Network -> restart).\n\nThe full doc about order can be found [here](https://www.raspberrypi.org/documentation/hardware/raspberrypi/bcm2711_bootloader_config.md).\n\nTo do that, first we need to grab a bootloader that supports netboot. Anything late 2020 should work. For example to grab 2020-07-31 version:\n\n```\nwget https://github.com/raspberrypi/rpi-eeprom/raw/master/firmware/stable/pieeprom-2020-07-31.bin\n```\n\nExtract the `bootconf.txt` file and update order:\n\n```\nsudo rpi-eeprom-config pieeprom-2020-07-31.bin > bootconf.txt\nsed -i 's/BOOT_ORDER=.*/BOOT_ORDER=0xf21/g' bootconf.txt\nsudo rpi-eeprom-config --out bootloader.bin --config bootconf.txt pieeprom-2020-07-31.bin\n```\n\nFlash the EEPROM:\n\n```\nsudo rpi-eeprom-update -d -f bootloader.bin\n```\n\nBefore exiting, grab the serial number:\n\n```\ncat /proc/cpuinfo | grep Serial | tail -c 9\n```\n\nAssume this is `123456789`. This will be used in later tftp setup.\n\nUnplug, remove the SD card and replug the Pi. Now it should be stuck on the initial self test screen as it can neither boot from SD or net. This is expected.\n\n## DHCP Setup\n\nNow it comes to the router side as we need to enable the extra options of DHCP such that Pi can find the NAS in the same network.\n\nThis step is vendor specific. I use a Ubiquiti router. The command should be more or less the same for anything using OpenWRT.\n\nTo enable PXE boot, we first need to figure out what options Pi is asking for. In this case we need `tcpdump`.\n\n```\nsudo tcpdump -vnes0 port 67 or port 68\n```\n\n`-v` verbose (to list options)\n`-n` do not try to look up ip address\n`-e` list mac address\n`-s0` do not truncate packet and show full content\n`port 67` is for client -> server packets and `68` is for the other way around\n\nRestart the Pi to capture the packets.\n\nThe DHCP protocol has 4 parts for our interest:\n1. `Discover` - this is the initial DHCP broadcast request and we need to pay attention to the `Parameter-Request` part.\n2. `Offer` - DHCP server responds with allocated IP address and option responses\n3. `Request` - Client echoes the IP back to server to confirm allocation\n4. `ACK` - DHCP server confirms the echo\n\nHere's what Pi sends as the initial request:\n\n```\n19:45:26.963111 <PI MAC ADDR> > ff:ff:ff:ff:ff:ff, ethertype IPv4 (0x0800), length 364: (tos 0x0, ttl 64, id 20586, offset 0, flags [none], proto UDP (17), length 350)\n    0.0.0.0.68 > 255.255.255.255.67: BOOTP/DHCP, Request from <PI MAC ADDR>, length 322, xid 0xd5ec86d6, Flags [none]\n          Client-Ethernet-Address <PI MAC ADDR>\n          Vendor-rfc1048 Extensions\n            Magic Cookie 0x63825363\n            DHCP-Message Option 53, length 1: Discover\n            Parameter-Request Option 55, length 14:\n              Subnet-Mask, Default-Gateway, Vendor-Option, Vendor-Class\n              TFTP, BF, Option 128, Option 129\n              Option 130, Option 131, Option 132, Option 133\n              Option 134, Option 135\n            Vendor-Class Option 60, length 32: \"PXEClient:Arch:00000:UNDI:002001\"\n            ARCH Option 93, length 2: 0\n            NDI Option 94, length 3: 1.2.1\n            GUID Option 97, length 17: 0.82.80.105.52.18.49.192.0.50.167.237.137.157.110.239.229\n```\n\nHere the common options are in plain text like `Subnet-Mask` while some option extensions are just using numbers. The exact reference can be seen in [RFC2132](https://tools.ietf.org/html/rfc2132)\n\nWhat we need are:\n\n`TFTP (66)` - tftp server ip/name -> this should point to the NAS server ip\n`BF (67)` - boot file name -> this technically shouldn't matter for us as the bootloader is directly in Pi's EEPROM but for consistency we will set this to `bootcode.bin`\n`Vendor-Option (43)` - This is important and MUST include `Raspberry Pi Boot`\n`Vendor-Class (60)` - TBH can't remember if this matters for Pi but I set it to `PXEClient` anyways to be PXE compliant in general.\n\nTo set these in Ubiquiti router:\n\nUse `configure` to enter config editing mode.\n\nThe configs are tree-like and we can list the current options using\n\n```\nshow service dhcp-server\n```\n\nHere certain options are known to the router and we can set directly like `tftp-server-name`:\n\nHere the NAS is allocated with a static IP `192.168.1.60` so:\n\n```\nset service dhcp-server shared-network-name LAN subnet 192.168.1.0/24 tftp-server-name 192.168.1.60\nset service dhcp-server shared-network-name LAN subnet 192.168.1.0/24 bootfile-name bootcode.bin\n```\n\nFor other stuff it would be a bit tricky as we need to first tell the router what they are:\n\n```\nset service dhcp-server global-parameters \"option vendor-class-identifier code 60 = string;\"\nset service dhcp-server global-parameters \"option vendor-encapsulated-options code 43 = string;\"\n```\n\nand then set them\n\n```\nset service dhcp-server shared-network-name LAN subnet 192.168.1.0/24 subnet-parameters \"option vendor-class-identifier &quot;PXEClient&quot;;\"\nset service dhcp-server shared-network-name LAN subnet 192.168.1.0/24 subnet-parameters \"option vendor-encapsulated-options &quot;Raspberry Pi Boot   &quot;;\"\n```\n\nThe `&quot;` part is to actually include a double quote there.\n\nDon't forget to comment and save the config:\n```\ncommit; save \n```\n\nThe router should pick up the new config and restart DHCP server as needed. If not reboot it to force the reload.\n\nNow if we tcpdump we'll see the response packet with something like this:\n```\n19:45:27.963654 <ROUTER MAC ADDR> > <PI MAC ADDR>, ethertype IPv4 (0x0800), length 371: (tos 0x10, ttl 128, id 0, offset 0, flags [none], proto UDP (17), length 357)\n    192.168.1.1.67 > 192.168.1.137.68: BOOTP/DHCP, Reply, length 329, xid 0xd5ec86d6, Flags [none]\n          Your-IP 192.168.1.137\n          Server-IP 192.168.1.60\n          Client-Ethernet-Address <PI MAC ADDR>\n          file \"bootcode.bin\"\n          Vendor-rfc1048 Extensions\n            DHCP-Message Option 53, length 1: Offer\n            Server-ID Option 54, length 4: 192.168.1.1\n            Lease-Time Option 51, length 4: 86400\n            Subnet-Mask Option 1, length 4: 255.255.255.0\n            Default-Gateway Option 3, length 4: 192.168.1.1\n            Vendor-Option Option 43, length 20: 82.97.115.112.98.101.114.114.121.32.80.105.32.66.111.111.116.32.32.32\n            Vendor-Class Option 60, length 9: \"PXEClient\"\n            TFTP Option 66, length 12: \"192.168.1.60\"\n            BF Option 67, length 12: \"bootcode.bin\"\n```\n\nOf course in theory this can be further optimized to include the additional info only when vendor-class includes `PXEClient` but I omitted that part.\n\nWith these in place, the Pi should now be stuck accessing the TFTP server. Congrats and we will cover that in the next part.\n","slug":"PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-1-DHCP-Setup","published":1,"updated":"2025-09-01T22:26:51.290Z","comments":1,"layout":"post","photos":[],"_id":"cmf1pfvse001a8mmg6z7y8nyy","content":"<p>Raspberry Pi 4 is shipped with a flashable EEPROM and supports netbooting. However, the entire setup process is not that straightforward and it’s thus worth writing down all the pitfalls through the path especially when it involves a non-“native” Linux distribution.</p>\n<p>One thing worth noting is that even though this post mentions “PXE” and so does the official document, the boot process isn’t entirely PXE compliant and thus regular PXE boot support setup process may not work at all or at least not directly.</p>\n<p>Here’s a diagram of the network topology I use for the setup:</p>\n<img src=\"/2020/11/28/PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-1-DHCP-Setup/net-topo.png\" />\n\n<p>The Ubiquiti will also be serving as the DHCP server. The Synology NAS will be serving data via tftp (pxe boot) and nfs (post boot). Due to kernel limitation, my NAS cannot support overlayfs. But for newer version of NAS it might be possible.</p>\n<h2 id=\"Raspberry-Netboot-Sequence\"><a href=\"#Raspberry-Netboot-Sequence\" class=\"headerlink\" title=\"Raspberry Netboot Sequence\"></a>Raspberry Netboot Sequence</h2><p>The bootloader in EEPROM by default does not enable netboot so we’d need to enable that first.</p>\n<p>Once enabled, upon powering up, Pi will first send a DHCP request to ask for the TFTP server location and verify if netboot is supported.</p>\n<p>After that it will fetch the firmware (start4.elf), configs and kernel files (vmlinuz) from that tftp location. The bootloader has a config to specify the exact path for the current device. By default, that’s the last 9 chars of the serial number. This is important as certain config (like cmdline.txt) would tell the OS how to mount the root fs and thus needs to be separate.</p>\n<p>The kernel will later be loaded which takes over the boot process and eventually loads the rest of the system distribution.</p>\n<p>The official doc can be found <a href=\"https://www.raspberrypi.org/documentation/hardware/raspberrypi/bootmodes/net.md\">here</a>.</p>\n<h2 id=\"Flashing-the-EEPROM-to-Enable-Netboot\"><a href=\"#Flashing-the-EEPROM-to-Enable-Netboot\" class=\"headerlink\" title=\"Flashing the EEPROM to Enable Netboot\"></a>Flashing the EEPROM to Enable Netboot</h2><p>With the basic knowledge equipped, we can continue by first enabling the netboot in bootloader.</p>\n<p>To do that, we need to grab the updated version of bootcode.bin file which supports netboot and update the config.</p>\n<p>This also requires using the raspberry cli binary vcgencmd.</p>\n<p><strong>Pitfall #1: vcgencmd can only work in native Raspberry Pi OS</strong></p>\n<p>Yes the vcgencmd binary does not work in Ubuntu or any other “supported” Linux distros, even if you compile from source. They would just silently fail. Technically it should be doable if they are shipped with the right “stuff” but I decide not to waste more time figuring out what the “stuff” is.</p>\n<p>Here we need a small microSD card to flash the Raspberry Pi OS onto it and boot the system. For headless setup, don’t forget to <code>touch ssh</code> to create the file to enable ssh by default.</p>\n<span id=\"more\"></span>\n\n<p>The default user name and password is <code>pi</code> and <code>raspberry</code>.</p>\n<p>Run</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vcgencmd bootloader_config | grep BOOT_ORDER | cut -d &#x27;=&#x27; -f 2</span><br></pre></td></tr></table></figure>\n\n<p>To check the current boot order. The boot order is a sequence of digits from right to left. The default is <code>0x1</code> (or <code>0xf41</code> in later version) which means it’s SD card only (or SD -&gt; USB -&gt; restart). We’ll change this to <code>0xf21</code> (SD -&gt; Network -&gt; restart).</p>\n<p>The full doc about order can be found <a href=\"https://www.raspberrypi.org/documentation/hardware/raspberrypi/bcm2711_bootloader_config.md\">here</a>.</p>\n<p>To do that, first we need to grab a bootloader that supports netboot. Anything late 2020 should work. For example to grab 2020-07-31 version:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://github.com/raspberrypi/rpi-eeprom/raw/master/firmware/stable/pieeprom-2020-07-31.bin</span><br></pre></td></tr></table></figure>\n\n<p>Extract the <code>bootconf.txt</code> file and update order:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo rpi-eeprom-config pieeprom-2020-07-31.bin &gt; bootconf.txt</span><br><span class=\"line\">sed -i &#x27;s/BOOT_ORDER=.*/BOOT_ORDER=0xf21/g&#x27; bootconf.txt</span><br><span class=\"line\">sudo rpi-eeprom-config --out bootloader.bin --config bootconf.txt pieeprom-2020-07-31.bin</span><br></pre></td></tr></table></figure>\n\n<p>Flash the EEPROM:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo rpi-eeprom-update -d -f bootloader.bin</span><br></pre></td></tr></table></figure>\n\n<p>Before exiting, grab the serial number:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat /proc/cpuinfo | grep Serial | tail -c 9</span><br></pre></td></tr></table></figure>\n\n<p>Assume this is <code>123456789</code>. This will be used in later tftp setup.</p>\n<p>Unplug, remove the SD card and replug the Pi. Now it should be stuck on the initial self test screen as it can neither boot from SD or net. This is expected.</p>\n<h2 id=\"DHCP-Setup\"><a href=\"#DHCP-Setup\" class=\"headerlink\" title=\"DHCP Setup\"></a>DHCP Setup</h2><p>Now it comes to the router side as we need to enable the extra options of DHCP such that Pi can find the NAS in the same network.</p>\n<p>This step is vendor specific. I use a Ubiquiti router. The command should be more or less the same for anything using OpenWRT.</p>\n<p>To enable PXE boot, we first need to figure out what options Pi is asking for. In this case we need <code>tcpdump</code>.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo tcpdump -vnes0 port 67 or port 68</span><br></pre></td></tr></table></figure>\n\n<p><code>-v</code> verbose (to list options)<br><code>-n</code> do not try to look up ip address<br><code>-e</code> list mac address<br><code>-s0</code> do not truncate packet and show full content<br><code>port 67</code> is for client -&gt; server packets and <code>68</code> is for the other way around</p>\n<p>Restart the Pi to capture the packets.</p>\n<p>The DHCP protocol has 4 parts for our interest:</p>\n<ol>\n<li><code>Discover</code> - this is the initial DHCP broadcast request and we need to pay attention to the <code>Parameter-Request</code> part.</li>\n<li><code>Offer</code> - DHCP server responds with allocated IP address and option responses</li>\n<li><code>Request</code> - Client echoes the IP back to server to confirm allocation</li>\n<li><code>ACK</code> - DHCP server confirms the echo</li>\n</ol>\n<p>Here’s what Pi sends as the initial request:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">19:45:26.963111 &lt;PI MAC ADDR&gt; &gt; ff:ff:ff:ff:ff:ff, ethertype IPv4 (0x0800), length 364: (tos 0x0, ttl 64, id 20586, offset 0, flags [none], proto UDP (17), length 350)</span><br><span class=\"line\">    0.0.0.0.68 &gt; 255.255.255.255.67: BOOTP/DHCP, Request from &lt;PI MAC ADDR&gt;, length 322, xid 0xd5ec86d6, Flags [none]</span><br><span class=\"line\">          Client-Ethernet-Address &lt;PI MAC ADDR&gt;</span><br><span class=\"line\">          Vendor-rfc1048 Extensions</span><br><span class=\"line\">            Magic Cookie 0x63825363</span><br><span class=\"line\">            DHCP-Message Option 53, length 1: Discover</span><br><span class=\"line\">            Parameter-Request Option 55, length 14:</span><br><span class=\"line\">              Subnet-Mask, Default-Gateway, Vendor-Option, Vendor-Class</span><br><span class=\"line\">              TFTP, BF, Option 128, Option 129</span><br><span class=\"line\">              Option 130, Option 131, Option 132, Option 133</span><br><span class=\"line\">              Option 134, Option 135</span><br><span class=\"line\">            Vendor-Class Option 60, length 32: &quot;PXEClient:Arch:00000:UNDI:002001&quot;</span><br><span class=\"line\">            ARCH Option 93, length 2: 0</span><br><span class=\"line\">            NDI Option 94, length 3: 1.2.1</span><br><span class=\"line\">            GUID Option 97, length 17: 0.82.80.105.52.18.49.192.0.50.167.237.137.157.110.239.229</span><br></pre></td></tr></table></figure>\n\n<p>Here the common options are in plain text like <code>Subnet-Mask</code> while some option extensions are just using numbers. The exact reference can be seen in <a href=\"https://tools.ietf.org/html/rfc2132\">RFC2132</a></p>\n<p>What we need are:</p>\n<p><code>TFTP (66)</code> - tftp server ip/name -&gt; this should point to the NAS server ip<br><code>BF (67)</code> - boot file name -&gt; this technically shouldn’t matter for us as the bootloader is directly in Pi’s EEPROM but for consistency we will set this to <code>bootcode.bin</code><br><code>Vendor-Option (43)</code> - This is important and MUST include <code>Raspberry Pi Boot</code><br><code>Vendor-Class (60)</code> - TBH can’t remember if this matters for Pi but I set it to <code>PXEClient</code> anyways to be PXE compliant in general.</p>\n<p>To set these in Ubiquiti router:</p>\n<p>Use <code>configure</code> to enter config editing mode.</p>\n<p>The configs are tree-like and we can list the current options using</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show service dhcp-server</span><br></pre></td></tr></table></figure>\n\n<p>Here certain options are known to the router and we can set directly like <code>tftp-server-name</code>:</p>\n<p>Here the NAS is allocated with a static IP <code>192.168.1.60</code> so:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set service dhcp-server shared-network-name LAN subnet 192.168.1.0/24 tftp-server-name 192.168.1.60</span><br><span class=\"line\">set service dhcp-server shared-network-name LAN subnet 192.168.1.0/24 bootfile-name bootcode.bin</span><br></pre></td></tr></table></figure>\n\n<p>For other stuff it would be a bit tricky as we need to first tell the router what they are:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set service dhcp-server global-parameters &quot;option vendor-class-identifier code 60 = string;&quot;</span><br><span class=\"line\">set service dhcp-server global-parameters &quot;option vendor-encapsulated-options code 43 = string;&quot;</span><br></pre></td></tr></table></figure>\n\n<p>and then set them</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set service dhcp-server shared-network-name LAN subnet 192.168.1.0/24 subnet-parameters &quot;option vendor-class-identifier &amp;quot;PXEClient&amp;quot;;&quot;</span><br><span class=\"line\">set service dhcp-server shared-network-name LAN subnet 192.168.1.0/24 subnet-parameters &quot;option vendor-encapsulated-options &amp;quot;Raspberry Pi Boot   &amp;quot;;&quot;</span><br></pre></td></tr></table></figure>\n\n<p>The <code>&amp;quot;</code> part is to actually include a double quote there.</p>\n<p>Don’t forget to comment and save the config:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">commit; save </span><br></pre></td></tr></table></figure>\n\n<p>The router should pick up the new config and restart DHCP server as needed. If not reboot it to force the reload.</p>\n<p>Now if we tcpdump we’ll see the response packet with something like this:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">19:45:27.963654 &lt;ROUTER MAC ADDR&gt; &gt; &lt;PI MAC ADDR&gt;, ethertype IPv4 (0x0800), length 371: (tos 0x10, ttl 128, id 0, offset 0, flags [none], proto UDP (17), length 357)</span><br><span class=\"line\">    192.168.1.1.67 &gt; 192.168.1.137.68: BOOTP/DHCP, Reply, length 329, xid 0xd5ec86d6, Flags [none]</span><br><span class=\"line\">          Your-IP 192.168.1.137</span><br><span class=\"line\">          Server-IP 192.168.1.60</span><br><span class=\"line\">          Client-Ethernet-Address &lt;PI MAC ADDR&gt;</span><br><span class=\"line\">          file &quot;bootcode.bin&quot;</span><br><span class=\"line\">          Vendor-rfc1048 Extensions</span><br><span class=\"line\">            DHCP-Message Option 53, length 1: Offer</span><br><span class=\"line\">            Server-ID Option 54, length 4: 192.168.1.1</span><br><span class=\"line\">            Lease-Time Option 51, length 4: 86400</span><br><span class=\"line\">            Subnet-Mask Option 1, length 4: 255.255.255.0</span><br><span class=\"line\">            Default-Gateway Option 3, length 4: 192.168.1.1</span><br><span class=\"line\">            Vendor-Option Option 43, length 20: 82.97.115.112.98.101.114.114.121.32.80.105.32.66.111.111.116.32.32.32</span><br><span class=\"line\">            Vendor-Class Option 60, length 9: &quot;PXEClient&quot;</span><br><span class=\"line\">            TFTP Option 66, length 12: &quot;192.168.1.60&quot;</span><br><span class=\"line\">            BF Option 67, length 12: &quot;bootcode.bin&quot;</span><br></pre></td></tr></table></figure>\n\n<p>Of course in theory this can be further optimized to include the additional info only when vendor-class includes <code>PXEClient</code> but I omitted that part.</p>\n<p>With these in place, the Pi should now be stuck accessing the TFTP server. Congrats and we will cover that in the next part.</p>\n","thumbnailImageUrl":null,"excerpt":"<p>Raspberry Pi 4 is shipped with a flashable EEPROM and supports netbooting. However, the entire setup process is not that straightforward and it’s thus worth writing down all the pitfalls through the path especially when it involves a non-“native” Linux distribution.</p>\n<p>One thing worth noting is that even though this post mentions “PXE” and so does the official document, the boot process isn’t entirely PXE compliant and thus regular PXE boot support setup process may not work at all or at least not directly.</p>\n<p>Here’s a diagram of the network topology I use for the setup:</p>\n<img src=\"/2020/11/28/PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-1-DHCP-Setup/net-topo.png\" />\n\n<p>The Ubiquiti will also be serving as the DHCP server. The Synology NAS will be serving data via tftp (pxe boot) and nfs (post boot). Due to kernel limitation, my NAS cannot support overlayfs. But for newer version of NAS it might be possible.</p>\n<h2 id=\"Raspberry-Netboot-Sequence\"><a href=\"#Raspberry-Netboot-Sequence\" class=\"headerlink\" title=\"Raspberry Netboot Sequence\"></a>Raspberry Netboot Sequence</h2><p>The bootloader in EEPROM by default does not enable netboot so we’d need to enable that first.</p>\n<p>Once enabled, upon powering up, Pi will first send a DHCP request to ask for the TFTP server location and verify if netboot is supported.</p>\n<p>After that it will fetch the firmware (start4.elf), configs and kernel files (vmlinuz) from that tftp location. The bootloader has a config to specify the exact path for the current device. By default, that’s the last 9 chars of the serial number. This is important as certain config (like cmdline.txt) would tell the OS how to mount the root fs and thus needs to be separate.</p>\n<p>The kernel will later be loaded which takes over the boot process and eventually loads the rest of the system distribution.</p>\n<p>The official doc can be found <a href=\"https://www.raspberrypi.org/documentation/hardware/raspberrypi/bootmodes/net.md\">here</a>.</p>\n<h2 id=\"Flashing-the-EEPROM-to-Enable-Netboot\"><a href=\"#Flashing-the-EEPROM-to-Enable-Netboot\" class=\"headerlink\" title=\"Flashing the EEPROM to Enable Netboot\"></a>Flashing the EEPROM to Enable Netboot</h2><p>With the basic knowledge equipped, we can continue by first enabling the netboot in bootloader.</p>\n<p>To do that, we need to grab the updated version of bootcode.bin file which supports netboot and update the config.</p>\n<p>This also requires using the raspberry cli binary vcgencmd.</p>\n<p><strong>Pitfall #1: vcgencmd can only work in native Raspberry Pi OS</strong></p>\n<p>Yes the vcgencmd binary does not work in Ubuntu or any other “supported” Linux distros, even if you compile from source. They would just silently fail. Technically it should be doable if they are shipped with the right “stuff” but I decide not to waste more time figuring out what the “stuff” is.</p>\n<p>Here we need a small microSD card to flash the Raspberry Pi OS onto it and boot the system. For headless setup, don’t forget to <code>touch ssh</code> to create the file to enable ssh by default.</p>","more":"<p>The default user name and password is <code>pi</code> and <code>raspberry</code>.</p>\n<p>Run</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vcgencmd bootloader_config | grep BOOT_ORDER | cut -d &#x27;=&#x27; -f 2</span><br></pre></td></tr></table></figure>\n\n<p>To check the current boot order. The boot order is a sequence of digits from right to left. The default is <code>0x1</code> (or <code>0xf41</code> in later version) which means it’s SD card only (or SD -&gt; USB -&gt; restart). We’ll change this to <code>0xf21</code> (SD -&gt; Network -&gt; restart).</p>\n<p>The full doc about order can be found <a href=\"https://www.raspberrypi.org/documentation/hardware/raspberrypi/bcm2711_bootloader_config.md\">here</a>.</p>\n<p>To do that, first we need to grab a bootloader that supports netboot. Anything late 2020 should work. For example to grab 2020-07-31 version:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://github.com/raspberrypi/rpi-eeprom/raw/master/firmware/stable/pieeprom-2020-07-31.bin</span><br></pre></td></tr></table></figure>\n\n<p>Extract the <code>bootconf.txt</code> file and update order:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo rpi-eeprom-config pieeprom-2020-07-31.bin &gt; bootconf.txt</span><br><span class=\"line\">sed -i &#x27;s/BOOT_ORDER=.*/BOOT_ORDER=0xf21/g&#x27; bootconf.txt</span><br><span class=\"line\">sudo rpi-eeprom-config --out bootloader.bin --config bootconf.txt pieeprom-2020-07-31.bin</span><br></pre></td></tr></table></figure>\n\n<p>Flash the EEPROM:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo rpi-eeprom-update -d -f bootloader.bin</span><br></pre></td></tr></table></figure>\n\n<p>Before exiting, grab the serial number:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat /proc/cpuinfo | grep Serial | tail -c 9</span><br></pre></td></tr></table></figure>\n\n<p>Assume this is <code>123456789</code>. This will be used in later tftp setup.</p>\n<p>Unplug, remove the SD card and replug the Pi. Now it should be stuck on the initial self test screen as it can neither boot from SD or net. This is expected.</p>\n<h2 id=\"DHCP-Setup\"><a href=\"#DHCP-Setup\" class=\"headerlink\" title=\"DHCP Setup\"></a>DHCP Setup</h2><p>Now it comes to the router side as we need to enable the extra options of DHCP such that Pi can find the NAS in the same network.</p>\n<p>This step is vendor specific. I use a Ubiquiti router. The command should be more or less the same for anything using OpenWRT.</p>\n<p>To enable PXE boot, we first need to figure out what options Pi is asking for. In this case we need <code>tcpdump</code>.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo tcpdump -vnes0 port 67 or port 68</span><br></pre></td></tr></table></figure>\n\n<p><code>-v</code> verbose (to list options)<br><code>-n</code> do not try to look up ip address<br><code>-e</code> list mac address<br><code>-s0</code> do not truncate packet and show full content<br><code>port 67</code> is for client -&gt; server packets and <code>68</code> is for the other way around</p>\n<p>Restart the Pi to capture the packets.</p>\n<p>The DHCP protocol has 4 parts for our interest:</p>\n<ol>\n<li><code>Discover</code> - this is the initial DHCP broadcast request and we need to pay attention to the <code>Parameter-Request</code> part.</li>\n<li><code>Offer</code> - DHCP server responds with allocated IP address and option responses</li>\n<li><code>Request</code> - Client echoes the IP back to server to confirm allocation</li>\n<li><code>ACK</code> - DHCP server confirms the echo</li>\n</ol>\n<p>Here’s what Pi sends as the initial request:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">19:45:26.963111 &lt;PI MAC ADDR&gt; &gt; ff:ff:ff:ff:ff:ff, ethertype IPv4 (0x0800), length 364: (tos 0x0, ttl 64, id 20586, offset 0, flags [none], proto UDP (17), length 350)</span><br><span class=\"line\">    0.0.0.0.68 &gt; 255.255.255.255.67: BOOTP/DHCP, Request from &lt;PI MAC ADDR&gt;, length 322, xid 0xd5ec86d6, Flags [none]</span><br><span class=\"line\">          Client-Ethernet-Address &lt;PI MAC ADDR&gt;</span><br><span class=\"line\">          Vendor-rfc1048 Extensions</span><br><span class=\"line\">            Magic Cookie 0x63825363</span><br><span class=\"line\">            DHCP-Message Option 53, length 1: Discover</span><br><span class=\"line\">            Parameter-Request Option 55, length 14:</span><br><span class=\"line\">              Subnet-Mask, Default-Gateway, Vendor-Option, Vendor-Class</span><br><span class=\"line\">              TFTP, BF, Option 128, Option 129</span><br><span class=\"line\">              Option 130, Option 131, Option 132, Option 133</span><br><span class=\"line\">              Option 134, Option 135</span><br><span class=\"line\">            Vendor-Class Option 60, length 32: &quot;PXEClient:Arch:00000:UNDI:002001&quot;</span><br><span class=\"line\">            ARCH Option 93, length 2: 0</span><br><span class=\"line\">            NDI Option 94, length 3: 1.2.1</span><br><span class=\"line\">            GUID Option 97, length 17: 0.82.80.105.52.18.49.192.0.50.167.237.137.157.110.239.229</span><br></pre></td></tr></table></figure>\n\n<p>Here the common options are in plain text like <code>Subnet-Mask</code> while some option extensions are just using numbers. The exact reference can be seen in <a href=\"https://tools.ietf.org/html/rfc2132\">RFC2132</a></p>\n<p>What we need are:</p>\n<p><code>TFTP (66)</code> - tftp server ip/name -&gt; this should point to the NAS server ip<br><code>BF (67)</code> - boot file name -&gt; this technically shouldn’t matter for us as the bootloader is directly in Pi’s EEPROM but for consistency we will set this to <code>bootcode.bin</code><br><code>Vendor-Option (43)</code> - This is important and MUST include <code>Raspberry Pi Boot</code><br><code>Vendor-Class (60)</code> - TBH can’t remember if this matters for Pi but I set it to <code>PXEClient</code> anyways to be PXE compliant in general.</p>\n<p>To set these in Ubiquiti router:</p>\n<p>Use <code>configure</code> to enter config editing mode.</p>\n<p>The configs are tree-like and we can list the current options using</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show service dhcp-server</span><br></pre></td></tr></table></figure>\n\n<p>Here certain options are known to the router and we can set directly like <code>tftp-server-name</code>:</p>\n<p>Here the NAS is allocated with a static IP <code>192.168.1.60</code> so:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set service dhcp-server shared-network-name LAN subnet 192.168.1.0/24 tftp-server-name 192.168.1.60</span><br><span class=\"line\">set service dhcp-server shared-network-name LAN subnet 192.168.1.0/24 bootfile-name bootcode.bin</span><br></pre></td></tr></table></figure>\n\n<p>For other stuff it would be a bit tricky as we need to first tell the router what they are:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set service dhcp-server global-parameters &quot;option vendor-class-identifier code 60 = string;&quot;</span><br><span class=\"line\">set service dhcp-server global-parameters &quot;option vendor-encapsulated-options code 43 = string;&quot;</span><br></pre></td></tr></table></figure>\n\n<p>and then set them</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set service dhcp-server shared-network-name LAN subnet 192.168.1.0/24 subnet-parameters &quot;option vendor-class-identifier &amp;quot;PXEClient&amp;quot;;&quot;</span><br><span class=\"line\">set service dhcp-server shared-network-name LAN subnet 192.168.1.0/24 subnet-parameters &quot;option vendor-encapsulated-options &amp;quot;Raspberry Pi Boot   &amp;quot;;&quot;</span><br></pre></td></tr></table></figure>\n\n<p>The <code>&amp;quot;</code> part is to actually include a double quote there.</p>\n<p>Don’t forget to comment and save the config:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">commit; save </span><br></pre></td></tr></table></figure>\n\n<p>The router should pick up the new config and restart DHCP server as needed. If not reboot it to force the reload.</p>\n<p>Now if we tcpdump we’ll see the response packet with something like this:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">19:45:27.963654 &lt;ROUTER MAC ADDR&gt; &gt; &lt;PI MAC ADDR&gt;, ethertype IPv4 (0x0800), length 371: (tos 0x10, ttl 128, id 0, offset 0, flags [none], proto UDP (17), length 357)</span><br><span class=\"line\">    192.168.1.1.67 &gt; 192.168.1.137.68: BOOTP/DHCP, Reply, length 329, xid 0xd5ec86d6, Flags [none]</span><br><span class=\"line\">          Your-IP 192.168.1.137</span><br><span class=\"line\">          Server-IP 192.168.1.60</span><br><span class=\"line\">          Client-Ethernet-Address &lt;PI MAC ADDR&gt;</span><br><span class=\"line\">          file &quot;bootcode.bin&quot;</span><br><span class=\"line\">          Vendor-rfc1048 Extensions</span><br><span class=\"line\">            DHCP-Message Option 53, length 1: Offer</span><br><span class=\"line\">            Server-ID Option 54, length 4: 192.168.1.1</span><br><span class=\"line\">            Lease-Time Option 51, length 4: 86400</span><br><span class=\"line\">            Subnet-Mask Option 1, length 4: 255.255.255.0</span><br><span class=\"line\">            Default-Gateway Option 3, length 4: 192.168.1.1</span><br><span class=\"line\">            Vendor-Option Option 43, length 20: 82.97.115.112.98.101.114.114.121.32.80.105.32.66.111.111.116.32.32.32</span><br><span class=\"line\">            Vendor-Class Option 60, length 9: &quot;PXEClient&quot;</span><br><span class=\"line\">            TFTP Option 66, length 12: &quot;192.168.1.60&quot;</span><br><span class=\"line\">            BF Option 67, length 12: &quot;bootcode.bin&quot;</span><br></pre></td></tr></table></figure>\n\n<p>Of course in theory this can be further optimized to include the additional info only when vendor-class includes <code>PXEClient</code> but I omitted that part.</p>\n<p>With these in place, the Pi should now be stuck accessing the TFTP server. Congrats and we will cover that in the next part.</p>"},{"title":"PXE Boot Diskless Raspberry Pi 4 With Ubuntu, Ubiquiti and Synology (2): Config TFTP and NFS mounts","date":"2020-11-29T05:37:17.000Z","_content":"\nOnce we get the DHCP part set up, here comes the exciting part - to actually install the Linux distro and boot the Pi using it instead of the vanilla 32-bit Raspberry Pi OS.\n\nNow our Pi is still stuck at the initial screen, but don't worry, as long as it is trying to read the tftp path we have made progress.\n\n## Enable TFTP on the Server Side\n\nHere I will be using the Synology NAS server as the file system server.\n\nSynology offers tftp support in file sharing directly. On the other hand, it's also fairly straightforward to set up a TFTP server using a Linux server. One thing important is to remember the tftp root directory. That will be used in the next step to copy over the boot files to.\n\nUnder that directory create the Pi corresponding directory like\n\n```\n<tftp root>/123456789\n```\n\n`123456789` refers to last 9 chars of serial no. Pi's bootloader by default will load from there.\n\n\n## Create the TFTP boot directory\n\nWe need to first grab the Linux distro from an image.\n\nHere I picked Ubuntu 20.04 LTS which is \"officially\" supported. Technically this is probably not the best option as the OS itself includes a lot of things that we may not actually need and the size is pretty bulky. On the other end the Alpine one is very skinny and lacks some of the tooling we may need initially. For raspberry, unfortunately each distro may be slightly different (in terms of boot) so while you are free to choose whatever you want, YMMV.\n\nFlash the image into the SD card and we'll get 2 partitions - boot and root.\n\nIn general, it's a good idea to first boot the system using the SD card just in case there's some essential setup we need to complete ahead of time. After that, the remaining Pi boards can replicate the same set of files with minor tweaks.\n\n<!-- more -->\n\nAfter that use `rsync` to copy the boot files:\n\n```\nsudo rsync -xa /boot 192.168.1.60:<tftp root>/123456789/\n```\n\nHere we need to perform some cleanup.\n\nPi will need a few critical files during the boot process.\n\n`start4.elf`, `bcm2711-rpi-4-b.dtb`, `dtb` these are the firmware files\n`config.txt`, `usercfg.txt`, `syscfg.txt` these are the config files to locate the kernel files and related configs\n`vmlinuz`, `System.map`, `initrd.img` Linux kernel files\n`cmdline.txt` contains config params to pass to the kernel\n\nThe overall `boot` directory structure would eventually look roughly like this:\n\n```\n-rwxr--r--   1 root  root     46612 Sep  7 22:20 bcm2711-rpi-4-b.dtb\n-rwxr--r--   1 root  root       184 Sep 13 14:56 cmdline.txt\n-rw-r--r--   1 root  root    218826 Oct 15 06:16 config-5.4.0-1022-raspi\n-rwxr--r--   1 root  root      1189 Nov 27 13:50 config.txt\nlrwxr--r--   1 root  root        43 Oct 21 23:47 dtb -> dtbs/5.4.0-1022-raspi/./bcm2711-rpi-4-b.dtb\nlrwxr--r--   1 root  root        43 Oct 21 23:47 dtb-5.4.0-1022-raspi -> dtbs/5.4.0-1022-raspi/./bcm2711-rpi-4-b.dtb\ndrwxr--r--   8 root  root      4096 Oct 21 23:45 dtbs\ndrwxr--r--   8 root  root      4096 Sep 13 15:13 firmware\n-rwxr--r--   1 root  root      5405 Sep  7 22:20 fixup4.dat\nlrwxr--r--   1 root  root        27 Oct 21 23:46 initrd.img -> initrd.img-5.4.0-1022-raspi\n-rw-r--r--   1 root  root  29325769 Oct 21 23:47 initrd.img-5.4.0-1022-raspi\nlrwxr--r--   1 root  root        27 Oct 21 23:46 initrd.img.old -> initrd.img-5.4.0-1021-raspi\n-rwxr--r--   1 root  root   2272992 Sep  7 22:21 start4.elf\n-rwxr--r--   1 root  root       327 Sep  7 22:21 syscfg.txt\n-rw-------   1 root  root   4164641 Oct 15 06:16 System.map-5.4.0-1022-raspi\n-rwxr--r--   1 root  root       200 Sep  7 22:21 usercfg.txt\nlrwxr--r--   1 root  root        24 Oct 21 23:46 vmlinuz -> vmlinuz-5.4.0-1022-raspi\n-rwx------   1 root  root   8306127 Oct  5 02:16 vmlinuz-5.4.0-1022-raspi\n-rwx------   1 root  root  25907712 Oct 15 10:16 vmlinux-5.4.0-1022-raspi\nlrwxr--r--   1 root  root        24 Oct 21 23:46 vmlinuz.old -> vmlinuz-5.4.0-1021-raspi\n```\n\nWe need to make a few modifications here.\n\n### Uncompress the Linux kernel vmlinuz\n\n**Pitfall #2: Pi does not uncompress the vmlinuz kernel file**\n\nThe typical linux kernel vmlinuz is a gzipped executable that's initially loaded into the memory. However Pi's bootloader somehow does not uncompress it and as a result upon boot it would be stuck in rainbow screen.\n\nBefore uncompressing, be aware that some Linux distro may have a non-0 offset of the actual executable. We can check via `od`. A gzip file has the header `1f 8b 08 00`.\n\n```\nsudo od -A d -t x1 vmlinuz-5.4.0-1022-raspi | grep '1f 8b 08 00'\n\n0000000 1f 8b 08 00 00 00 00 00 02 03 ec 5c 0d 74 14 55\n```\n\nHere the offset is `0000000` and hence we can just do\n\n```\nzcat vmlinuz-5.4.0-1022-raspi > vmlinux-5.4.0-1022-raspi\n```\n\n### Specify the kernel file location in config\n\n**Pitfall #3: The default Uboot config does not work**\n\nI didn't verify other distros but at least with the ubuntu image, the default config is to use uboot. However with netboot config, it would be stuck locating the kernel and other stuff. So here we will tell Pi to boot directly with the vmlinux file we just uncompressed.\n\nSo here we will comment out the original entries and just specify the kernel directly in `all` section:\n```\n[pi4]                            \n# kernel=uboot_rpi_4.bin\nmax_framebuffers=2\n                                 \n[pi2]                            \n# kernel=uboot_rpi_2.bin\n                                 \n[pi3]                            \n# kernel=uboot_rpi_3.bin\n                                 \n[all]                            \narm_64bit=1                      \ndevice_tree_address=0x03000000\nkernel=vmlinux-5.4.0-1022-raspi  \ninitramfs initrd.img-5.4.0-1022-raspi followkernel\n```\n\nHere we are sort of breaking the best practice by specifying the fixed version here so we are not getting updated kernel. We are doing this because the kernel needs to be uncompressed and `initrd` needs to match the version. Technically the kernel update process can include the uncompression as well but that would be a TODO item.\n\n### Update cmdline.txt\n\nHere we need to tell the kernel how to mount the root fs.\n\nSince we are going to set up the diskless boot we will mount via nfs.\n\nThe cmdline.txt would roughly be like:\n```\nnet.ifnames=0 dwc_otg.lpm_enable=0 console=serial0,115200 console=tty1 root=/dev/nfs nfsroot=192.168.1.60:<nfs root>/raspberry/123456789 ip=dhcp elevator=deadline rootwait fixrtc rw\n```\n\nHere the nfs root is the directory in the NAS. `192.168.1.60` is the NAS IP. `123456789` again is the Pi serial but it technically could be anything unique. We are setting it like that for consistency.\n\nSo far we have finished configuring the tftp side.\n\n## Create NFS root fs\n\n### Copy over Files\n\nSimilar to boot directory we will rsync the root fs:\n\n```\nsudo rsync -xa / 192.168.1.60:<nfs root>/raspberry/123456789\n```\n\nHere `-x` is important to ensure we don't copy anything mounted.\nParticularly the `boot` directory would be left out and we can either make a soft link back to the one under tftp, or mount it in `/etc/fstab` explicitly.\n\n\n### Update /etc/fstab\n\nUpdate the one we just copied over to NAS nfs directory as that would be read upon next boot.\n\n```                                         \n192.168.1.60:<nfs root>/raspberry/123456789   /       nfs     defaults,rw,nolock   0  0\ntmpfs   /tmp    tmpfs   defaults    0   0\ntmpfs   /var/tmp    tmpfs   defaults    0   0\ntmpfs   /var/run    tmpfs   defaults    0   0\n```\n\nHere we'll mount some of the tmp dirs to tmpfs (mem fs).\n\nThe root fs is technically not required as in `cmdline.txt` the kernel already mounts it as `rw` but here we are explicitly marking it so and turning off disk check.\n\n### Enable NFS Server\n\nFor Synology this part is easy, in file sharing there's an option to turn on NFS in File Services.\n\n**Remember to turn off NFS squash in shared file**\n\n<img src=\"{% asset_path nfs-permission.png %}\" />\n\n## Testing and Troubleshooting\n\nSo far we have completed the necessary steps. It's time to unplug the Pi, remove the microSD card and reboot.\n\nThe Pi should tell us it fails reading the card and will try PXE boot next.\n\nOn the first screen it should show what files it's trying to read and if it's stuck on some of them check the tftp logs to see if they are placed properly.\n\nNext it will show the rainbow screen like this\n\n<img src=\"{% asset_path rainbow.jpg %}\" />\n\nIf you see this, that means Pi's GPU has been properly initialized and next it would try to load the kernel.\n\nIf the screen is stuck in this step, go back to check if the kernel file is properly uncompressed and referred to in config.txt file.\n\nThis screen should last for a few seconds to tens of seconds depending on network speed. After that the Linux kernel executable should take it over to init other stuff and mount the NFS root.\n\ninitramfs would be loaded at this moment and if the version does not match you might get some random weird errors. If NFS is not mounted correctly, it would tell so.\n\nWith Ubuntu depending on your actual env, there may be a few other extra non critical errors we need to fix like cloud-init but typically they would not block the startup process.\n\nNow we should be able to ssh into the system with the username `ubuntu`.\n\nCongrats we have finished the netboot setup and this can be replicated to other Pis with the same process.","source":"_posts/PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-2-Config-TFTP-and-NFS-mounts.md","raw":"---\ntitle: >-\n  PXE Boot Diskless Raspberry Pi 4 With Ubuntu, Ubiquiti and Synology (2):\n  Config TFTP and NFS mounts\ndate: 2020-11-28 21:37:17\ntags:\n- linux\n- raspberry\n- network\n---\n\nOnce we get the DHCP part set up, here comes the exciting part - to actually install the Linux distro and boot the Pi using it instead of the vanilla 32-bit Raspberry Pi OS.\n\nNow our Pi is still stuck at the initial screen, but don't worry, as long as it is trying to read the tftp path we have made progress.\n\n## Enable TFTP on the Server Side\n\nHere I will be using the Synology NAS server as the file system server.\n\nSynology offers tftp support in file sharing directly. On the other hand, it's also fairly straightforward to set up a TFTP server using a Linux server. One thing important is to remember the tftp root directory. That will be used in the next step to copy over the boot files to.\n\nUnder that directory create the Pi corresponding directory like\n\n```\n<tftp root>/123456789\n```\n\n`123456789` refers to last 9 chars of serial no. Pi's bootloader by default will load from there.\n\n\n## Create the TFTP boot directory\n\nWe need to first grab the Linux distro from an image.\n\nHere I picked Ubuntu 20.04 LTS which is \"officially\" supported. Technically this is probably not the best option as the OS itself includes a lot of things that we may not actually need and the size is pretty bulky. On the other end the Alpine one is very skinny and lacks some of the tooling we may need initially. For raspberry, unfortunately each distro may be slightly different (in terms of boot) so while you are free to choose whatever you want, YMMV.\n\nFlash the image into the SD card and we'll get 2 partitions - boot and root.\n\nIn general, it's a good idea to first boot the system using the SD card just in case there's some essential setup we need to complete ahead of time. After that, the remaining Pi boards can replicate the same set of files with minor tweaks.\n\n<!-- more -->\n\nAfter that use `rsync` to copy the boot files:\n\n```\nsudo rsync -xa /boot 192.168.1.60:<tftp root>/123456789/\n```\n\nHere we need to perform some cleanup.\n\nPi will need a few critical files during the boot process.\n\n`start4.elf`, `bcm2711-rpi-4-b.dtb`, `dtb` these are the firmware files\n`config.txt`, `usercfg.txt`, `syscfg.txt` these are the config files to locate the kernel files and related configs\n`vmlinuz`, `System.map`, `initrd.img` Linux kernel files\n`cmdline.txt` contains config params to pass to the kernel\n\nThe overall `boot` directory structure would eventually look roughly like this:\n\n```\n-rwxr--r--   1 root  root     46612 Sep  7 22:20 bcm2711-rpi-4-b.dtb\n-rwxr--r--   1 root  root       184 Sep 13 14:56 cmdline.txt\n-rw-r--r--   1 root  root    218826 Oct 15 06:16 config-5.4.0-1022-raspi\n-rwxr--r--   1 root  root      1189 Nov 27 13:50 config.txt\nlrwxr--r--   1 root  root        43 Oct 21 23:47 dtb -> dtbs/5.4.0-1022-raspi/./bcm2711-rpi-4-b.dtb\nlrwxr--r--   1 root  root        43 Oct 21 23:47 dtb-5.4.0-1022-raspi -> dtbs/5.4.0-1022-raspi/./bcm2711-rpi-4-b.dtb\ndrwxr--r--   8 root  root      4096 Oct 21 23:45 dtbs\ndrwxr--r--   8 root  root      4096 Sep 13 15:13 firmware\n-rwxr--r--   1 root  root      5405 Sep  7 22:20 fixup4.dat\nlrwxr--r--   1 root  root        27 Oct 21 23:46 initrd.img -> initrd.img-5.4.0-1022-raspi\n-rw-r--r--   1 root  root  29325769 Oct 21 23:47 initrd.img-5.4.0-1022-raspi\nlrwxr--r--   1 root  root        27 Oct 21 23:46 initrd.img.old -> initrd.img-5.4.0-1021-raspi\n-rwxr--r--   1 root  root   2272992 Sep  7 22:21 start4.elf\n-rwxr--r--   1 root  root       327 Sep  7 22:21 syscfg.txt\n-rw-------   1 root  root   4164641 Oct 15 06:16 System.map-5.4.0-1022-raspi\n-rwxr--r--   1 root  root       200 Sep  7 22:21 usercfg.txt\nlrwxr--r--   1 root  root        24 Oct 21 23:46 vmlinuz -> vmlinuz-5.4.0-1022-raspi\n-rwx------   1 root  root   8306127 Oct  5 02:16 vmlinuz-5.4.0-1022-raspi\n-rwx------   1 root  root  25907712 Oct 15 10:16 vmlinux-5.4.0-1022-raspi\nlrwxr--r--   1 root  root        24 Oct 21 23:46 vmlinuz.old -> vmlinuz-5.4.0-1021-raspi\n```\n\nWe need to make a few modifications here.\n\n### Uncompress the Linux kernel vmlinuz\n\n**Pitfall #2: Pi does not uncompress the vmlinuz kernel file**\n\nThe typical linux kernel vmlinuz is a gzipped executable that's initially loaded into the memory. However Pi's bootloader somehow does not uncompress it and as a result upon boot it would be stuck in rainbow screen.\n\nBefore uncompressing, be aware that some Linux distro may have a non-0 offset of the actual executable. We can check via `od`. A gzip file has the header `1f 8b 08 00`.\n\n```\nsudo od -A d -t x1 vmlinuz-5.4.0-1022-raspi | grep '1f 8b 08 00'\n\n0000000 1f 8b 08 00 00 00 00 00 02 03 ec 5c 0d 74 14 55\n```\n\nHere the offset is `0000000` and hence we can just do\n\n```\nzcat vmlinuz-5.4.0-1022-raspi > vmlinux-5.4.0-1022-raspi\n```\n\n### Specify the kernel file location in config\n\n**Pitfall #3: The default Uboot config does not work**\n\nI didn't verify other distros but at least with the ubuntu image, the default config is to use uboot. However with netboot config, it would be stuck locating the kernel and other stuff. So here we will tell Pi to boot directly with the vmlinux file we just uncompressed.\n\nSo here we will comment out the original entries and just specify the kernel directly in `all` section:\n```\n[pi4]                            \n# kernel=uboot_rpi_4.bin\nmax_framebuffers=2\n                                 \n[pi2]                            \n# kernel=uboot_rpi_2.bin\n                                 \n[pi3]                            \n# kernel=uboot_rpi_3.bin\n                                 \n[all]                            \narm_64bit=1                      \ndevice_tree_address=0x03000000\nkernel=vmlinux-5.4.0-1022-raspi  \ninitramfs initrd.img-5.4.0-1022-raspi followkernel\n```\n\nHere we are sort of breaking the best practice by specifying the fixed version here so we are not getting updated kernel. We are doing this because the kernel needs to be uncompressed and `initrd` needs to match the version. Technically the kernel update process can include the uncompression as well but that would be a TODO item.\n\n### Update cmdline.txt\n\nHere we need to tell the kernel how to mount the root fs.\n\nSince we are going to set up the diskless boot we will mount via nfs.\n\nThe cmdline.txt would roughly be like:\n```\nnet.ifnames=0 dwc_otg.lpm_enable=0 console=serial0,115200 console=tty1 root=/dev/nfs nfsroot=192.168.1.60:<nfs root>/raspberry/123456789 ip=dhcp elevator=deadline rootwait fixrtc rw\n```\n\nHere the nfs root is the directory in the NAS. `192.168.1.60` is the NAS IP. `123456789` again is the Pi serial but it technically could be anything unique. We are setting it like that for consistency.\n\nSo far we have finished configuring the tftp side.\n\n## Create NFS root fs\n\n### Copy over Files\n\nSimilar to boot directory we will rsync the root fs:\n\n```\nsudo rsync -xa / 192.168.1.60:<nfs root>/raspberry/123456789\n```\n\nHere `-x` is important to ensure we don't copy anything mounted.\nParticularly the `boot` directory would be left out and we can either make a soft link back to the one under tftp, or mount it in `/etc/fstab` explicitly.\n\n\n### Update /etc/fstab\n\nUpdate the one we just copied over to NAS nfs directory as that would be read upon next boot.\n\n```                                         \n192.168.1.60:<nfs root>/raspberry/123456789   /       nfs     defaults,rw,nolock   0  0\ntmpfs   /tmp    tmpfs   defaults    0   0\ntmpfs   /var/tmp    tmpfs   defaults    0   0\ntmpfs   /var/run    tmpfs   defaults    0   0\n```\n\nHere we'll mount some of the tmp dirs to tmpfs (mem fs).\n\nThe root fs is technically not required as in `cmdline.txt` the kernel already mounts it as `rw` but here we are explicitly marking it so and turning off disk check.\n\n### Enable NFS Server\n\nFor Synology this part is easy, in file sharing there's an option to turn on NFS in File Services.\n\n**Remember to turn off NFS squash in shared file**\n\n<img src=\"{% asset_path nfs-permission.png %}\" />\n\n## Testing and Troubleshooting\n\nSo far we have completed the necessary steps. It's time to unplug the Pi, remove the microSD card and reboot.\n\nThe Pi should tell us it fails reading the card and will try PXE boot next.\n\nOn the first screen it should show what files it's trying to read and if it's stuck on some of them check the tftp logs to see if they are placed properly.\n\nNext it will show the rainbow screen like this\n\n<img src=\"{% asset_path rainbow.jpg %}\" />\n\nIf you see this, that means Pi's GPU has been properly initialized and next it would try to load the kernel.\n\nIf the screen is stuck in this step, go back to check if the kernel file is properly uncompressed and referred to in config.txt file.\n\nThis screen should last for a few seconds to tens of seconds depending on network speed. After that the Linux kernel executable should take it over to init other stuff and mount the NFS root.\n\ninitramfs would be loaded at this moment and if the version does not match you might get some random weird errors. If NFS is not mounted correctly, it would tell so.\n\nWith Ubuntu depending on your actual env, there may be a few other extra non critical errors we need to fix like cloud-init but typically they would not block the startup process.\n\nNow we should be able to ssh into the system with the username `ubuntu`.\n\nCongrats we have finished the netboot setup and this can be replicated to other Pis with the same process.","slug":"PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-2-Config-TFTP-and-NFS-mounts","published":1,"updated":"2025-09-01T22:26:51.291Z","comments":1,"layout":"post","photos":[],"_id":"cmf1pfvse001b8mmg961s6fw4","content":"<p>Once we get the DHCP part set up, here comes the exciting part - to actually install the Linux distro and boot the Pi using it instead of the vanilla 32-bit Raspberry Pi OS.</p>\n<p>Now our Pi is still stuck at the initial screen, but don’t worry, as long as it is trying to read the tftp path we have made progress.</p>\n<h2 id=\"Enable-TFTP-on-the-Server-Side\"><a href=\"#Enable-TFTP-on-the-Server-Side\" class=\"headerlink\" title=\"Enable TFTP on the Server Side\"></a>Enable TFTP on the Server Side</h2><p>Here I will be using the Synology NAS server as the file system server.</p>\n<p>Synology offers tftp support in file sharing directly. On the other hand, it’s also fairly straightforward to set up a TFTP server using a Linux server. One thing important is to remember the tftp root directory. That will be used in the next step to copy over the boot files to.</p>\n<p>Under that directory create the Pi corresponding directory like</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;tftp root&gt;/123456789</span><br></pre></td></tr></table></figure>\n\n<p><code>123456789</code> refers to last 9 chars of serial no. Pi’s bootloader by default will load from there.</p>\n<h2 id=\"Create-the-TFTP-boot-directory\"><a href=\"#Create-the-TFTP-boot-directory\" class=\"headerlink\" title=\"Create the TFTP boot directory\"></a>Create the TFTP boot directory</h2><p>We need to first grab the Linux distro from an image.</p>\n<p>Here I picked Ubuntu 20.04 LTS which is “officially” supported. Technically this is probably not the best option as the OS itself includes a lot of things that we may not actually need and the size is pretty bulky. On the other end the Alpine one is very skinny and lacks some of the tooling we may need initially. For raspberry, unfortunately each distro may be slightly different (in terms of boot) so while you are free to choose whatever you want, YMMV.</p>\n<p>Flash the image into the SD card and we’ll get 2 partitions - boot and root.</p>\n<p>In general, it’s a good idea to first boot the system using the SD card just in case there’s some essential setup we need to complete ahead of time. After that, the remaining Pi boards can replicate the same set of files with minor tweaks.</p>\n<span id=\"more\"></span>\n\n<p>After that use <code>rsync</code> to copy the boot files:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo rsync -xa /boot 192.168.1.60:&lt;tftp root&gt;/123456789/</span><br></pre></td></tr></table></figure>\n\n<p>Here we need to perform some cleanup.</p>\n<p>Pi will need a few critical files during the boot process.</p>\n<p><code>start4.elf</code>, <code>bcm2711-rpi-4-b.dtb</code>, <code>dtb</code> these are the firmware files<br><code>config.txt</code>, <code>usercfg.txt</code>, <code>syscfg.txt</code> these are the config files to locate the kernel files and related configs<br><code>vmlinuz</code>, <code>System.map</code>, <code>initrd.img</code> Linux kernel files<br><code>cmdline.txt</code> contains config params to pass to the kernel</p>\n<p>The overall <code>boot</code> directory structure would eventually look roughly like this:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">-rwxr--r--   1 root  root     46612 Sep  7 22:20 bcm2711-rpi-4-b.dtb</span><br><span class=\"line\">-rwxr--r--   1 root  root       184 Sep 13 14:56 cmdline.txt</span><br><span class=\"line\">-rw-r--r--   1 root  root    218826 Oct 15 06:16 config-5.4.0-1022-raspi</span><br><span class=\"line\">-rwxr--r--   1 root  root      1189 Nov 27 13:50 config.txt</span><br><span class=\"line\">lrwxr--r--   1 root  root        43 Oct 21 23:47 dtb -&gt; dtbs/5.4.0-1022-raspi/./bcm2711-rpi-4-b.dtb</span><br><span class=\"line\">lrwxr--r--   1 root  root        43 Oct 21 23:47 dtb-5.4.0-1022-raspi -&gt; dtbs/5.4.0-1022-raspi/./bcm2711-rpi-4-b.dtb</span><br><span class=\"line\">drwxr--r--   8 root  root      4096 Oct 21 23:45 dtbs</span><br><span class=\"line\">drwxr--r--   8 root  root      4096 Sep 13 15:13 firmware</span><br><span class=\"line\">-rwxr--r--   1 root  root      5405 Sep  7 22:20 fixup4.dat</span><br><span class=\"line\">lrwxr--r--   1 root  root        27 Oct 21 23:46 initrd.img -&gt; initrd.img-5.4.0-1022-raspi</span><br><span class=\"line\">-rw-r--r--   1 root  root  29325769 Oct 21 23:47 initrd.img-5.4.0-1022-raspi</span><br><span class=\"line\">lrwxr--r--   1 root  root        27 Oct 21 23:46 initrd.img.old -&gt; initrd.img-5.4.0-1021-raspi</span><br><span class=\"line\">-rwxr--r--   1 root  root   2272992 Sep  7 22:21 start4.elf</span><br><span class=\"line\">-rwxr--r--   1 root  root       327 Sep  7 22:21 syscfg.txt</span><br><span class=\"line\">-rw-------   1 root  root   4164641 Oct 15 06:16 System.map-5.4.0-1022-raspi</span><br><span class=\"line\">-rwxr--r--   1 root  root       200 Sep  7 22:21 usercfg.txt</span><br><span class=\"line\">lrwxr--r--   1 root  root        24 Oct 21 23:46 vmlinuz -&gt; vmlinuz-5.4.0-1022-raspi</span><br><span class=\"line\">-rwx------   1 root  root   8306127 Oct  5 02:16 vmlinuz-5.4.0-1022-raspi</span><br><span class=\"line\">-rwx------   1 root  root  25907712 Oct 15 10:16 vmlinux-5.4.0-1022-raspi</span><br><span class=\"line\">lrwxr--r--   1 root  root        24 Oct 21 23:46 vmlinuz.old -&gt; vmlinuz-5.4.0-1021-raspi</span><br></pre></td></tr></table></figure>\n\n<p>We need to make a few modifications here.</p>\n<h3 id=\"Uncompress-the-Linux-kernel-vmlinuz\"><a href=\"#Uncompress-the-Linux-kernel-vmlinuz\" class=\"headerlink\" title=\"Uncompress the Linux kernel vmlinuz\"></a>Uncompress the Linux kernel vmlinuz</h3><p><strong>Pitfall #2: Pi does not uncompress the vmlinuz kernel file</strong></p>\n<p>The typical linux kernel vmlinuz is a gzipped executable that’s initially loaded into the memory. However Pi’s bootloader somehow does not uncompress it and as a result upon boot it would be stuck in rainbow screen.</p>\n<p>Before uncompressing, be aware that some Linux distro may have a non-0 offset of the actual executable. We can check via <code>od</code>. A gzip file has the header <code>1f 8b 08 00</code>.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo od -A d -t x1 vmlinuz-5.4.0-1022-raspi | grep &#x27;1f 8b 08 00&#x27;</span><br><span class=\"line\"></span><br><span class=\"line\">0000000 1f 8b 08 00 00 00 00 00 02 03 ec 5c 0d 74 14 55</span><br></pre></td></tr></table></figure>\n\n<p>Here the offset is <code>0000000</code> and hence we can just do</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zcat vmlinuz-5.4.0-1022-raspi &gt; vmlinux-5.4.0-1022-raspi</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Specify-the-kernel-file-location-in-config\"><a href=\"#Specify-the-kernel-file-location-in-config\" class=\"headerlink\" title=\"Specify the kernel file location in config\"></a>Specify the kernel file location in config</h3><p><strong>Pitfall #3: The default Uboot config does not work</strong></p>\n<p>I didn’t verify other distros but at least with the ubuntu image, the default config is to use uboot. However with netboot config, it would be stuck locating the kernel and other stuff. So here we will tell Pi to boot directly with the vmlinux file we just uncompressed.</p>\n<p>So here we will comment out the original entries and just specify the kernel directly in <code>all</code> section:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[pi4]                            </span><br><span class=\"line\"># kernel=uboot_rpi_4.bin</span><br><span class=\"line\">max_framebuffers=2</span><br><span class=\"line\">                                 </span><br><span class=\"line\">[pi2]                            </span><br><span class=\"line\"># kernel=uboot_rpi_2.bin</span><br><span class=\"line\">                                 </span><br><span class=\"line\">[pi3]                            </span><br><span class=\"line\"># kernel=uboot_rpi_3.bin</span><br><span class=\"line\">                                 </span><br><span class=\"line\">[all]                            </span><br><span class=\"line\">arm_64bit=1                      </span><br><span class=\"line\">device_tree_address=0x03000000</span><br><span class=\"line\">kernel=vmlinux-5.4.0-1022-raspi  </span><br><span class=\"line\">initramfs initrd.img-5.4.0-1022-raspi followkernel</span><br></pre></td></tr></table></figure>\n\n<p>Here we are sort of breaking the best practice by specifying the fixed version here so we are not getting updated kernel. We are doing this because the kernel needs to be uncompressed and <code>initrd</code> needs to match the version. Technically the kernel update process can include the uncompression as well but that would be a TODO item.</p>\n<h3 id=\"Update-cmdline-txt\"><a href=\"#Update-cmdline-txt\" class=\"headerlink\" title=\"Update cmdline.txt\"></a>Update cmdline.txt</h3><p>Here we need to tell the kernel how to mount the root fs.</p>\n<p>Since we are going to set up the diskless boot we will mount via nfs.</p>\n<p>The cmdline.txt would roughly be like:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net.ifnames=0 dwc_otg.lpm_enable=0 console=serial0,115200 console=tty1 root=/dev/nfs nfsroot=192.168.1.60:&lt;nfs root&gt;/raspberry/123456789 ip=dhcp elevator=deadline rootwait fixrtc rw</span><br></pre></td></tr></table></figure>\n\n<p>Here the nfs root is the directory in the NAS. <code>192.168.1.60</code> is the NAS IP. <code>123456789</code> again is the Pi serial but it technically could be anything unique. We are setting it like that for consistency.</p>\n<p>So far we have finished configuring the tftp side.</p>\n<h2 id=\"Create-NFS-root-fs\"><a href=\"#Create-NFS-root-fs\" class=\"headerlink\" title=\"Create NFS root fs\"></a>Create NFS root fs</h2><h3 id=\"Copy-over-Files\"><a href=\"#Copy-over-Files\" class=\"headerlink\" title=\"Copy over Files\"></a>Copy over Files</h3><p>Similar to boot directory we will rsync the root fs:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo rsync -xa / 192.168.1.60:&lt;nfs root&gt;/raspberry/123456789</span><br></pre></td></tr></table></figure>\n\n<p>Here <code>-x</code> is important to ensure we don’t copy anything mounted.<br>Particularly the <code>boot</code> directory would be left out and we can either make a soft link back to the one under tftp, or mount it in <code>/etc/fstab</code> explicitly.</p>\n<h3 id=\"Update-etc-fstab\"><a href=\"#Update-etc-fstab\" class=\"headerlink\" title=\"Update /etc/fstab\"></a>Update /etc/fstab</h3><p>Update the one we just copied over to NAS nfs directory as that would be read upon next boot.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">192.168.1.60:&lt;nfs root&gt;/raspberry/123456789   /       nfs     defaults,rw,nolock   0  0</span><br><span class=\"line\">tmpfs   /tmp    tmpfs   defaults    0   0</span><br><span class=\"line\">tmpfs   /var/tmp    tmpfs   defaults    0   0</span><br><span class=\"line\">tmpfs   /var/run    tmpfs   defaults    0   0</span><br></pre></td></tr></table></figure>\n\n<p>Here we’ll mount some of the tmp dirs to tmpfs (mem fs).</p>\n<p>The root fs is technically not required as in <code>cmdline.txt</code> the kernel already mounts it as <code>rw</code> but here we are explicitly marking it so and turning off disk check.</p>\n<h3 id=\"Enable-NFS-Server\"><a href=\"#Enable-NFS-Server\" class=\"headerlink\" title=\"Enable NFS Server\"></a>Enable NFS Server</h3><p>For Synology this part is easy, in file sharing there’s an option to turn on NFS in File Services.</p>\n<p><strong>Remember to turn off NFS squash in shared file</strong></p>\n<img src=\"/2020/11/28/PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-2-Config-TFTP-and-NFS-mounts/nfs-permission.png\" />\n\n<h2 id=\"Testing-and-Troubleshooting\"><a href=\"#Testing-and-Troubleshooting\" class=\"headerlink\" title=\"Testing and Troubleshooting\"></a>Testing and Troubleshooting</h2><p>So far we have completed the necessary steps. It’s time to unplug the Pi, remove the microSD card and reboot.</p>\n<p>The Pi should tell us it fails reading the card and will try PXE boot next.</p>\n<p>On the first screen it should show what files it’s trying to read and if it’s stuck on some of them check the tftp logs to see if they are placed properly.</p>\n<p>Next it will show the rainbow screen like this</p>\n<img src=\"/2020/11/28/PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-2-Config-TFTP-and-NFS-mounts/rainbow.jpg\" />\n\n<p>If you see this, that means Pi’s GPU has been properly initialized and next it would try to load the kernel.</p>\n<p>If the screen is stuck in this step, go back to check if the kernel file is properly uncompressed and referred to in config.txt file.</p>\n<p>This screen should last for a few seconds to tens of seconds depending on network speed. After that the Linux kernel executable should take it over to init other stuff and mount the NFS root.</p>\n<p>initramfs would be loaded at this moment and if the version does not match you might get some random weird errors. If NFS is not mounted correctly, it would tell so.</p>\n<p>With Ubuntu depending on your actual env, there may be a few other extra non critical errors we need to fix like cloud-init but typically they would not block the startup process.</p>\n<p>Now we should be able to ssh into the system with the username <code>ubuntu</code>.</p>\n<p>Congrats we have finished the netboot setup and this can be replicated to other Pis with the same process.</p>\n","thumbnailImageUrl":null,"excerpt":"<p>Once we get the DHCP part set up, here comes the exciting part - to actually install the Linux distro and boot the Pi using it instead of the vanilla 32-bit Raspberry Pi OS.</p>\n<p>Now our Pi is still stuck at the initial screen, but don’t worry, as long as it is trying to read the tftp path we have made progress.</p>\n<h2 id=\"Enable-TFTP-on-the-Server-Side\"><a href=\"#Enable-TFTP-on-the-Server-Side\" class=\"headerlink\" title=\"Enable TFTP on the Server Side\"></a>Enable TFTP on the Server Side</h2><p>Here I will be using the Synology NAS server as the file system server.</p>\n<p>Synology offers tftp support in file sharing directly. On the other hand, it’s also fairly straightforward to set up a TFTP server using a Linux server. One thing important is to remember the tftp root directory. That will be used in the next step to copy over the boot files to.</p>\n<p>Under that directory create the Pi corresponding directory like</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;tftp root&gt;/123456789</span><br></pre></td></tr></table></figure>\n\n<p><code>123456789</code> refers to last 9 chars of serial no. Pi’s bootloader by default will load from there.</p>\n<h2 id=\"Create-the-TFTP-boot-directory\"><a href=\"#Create-the-TFTP-boot-directory\" class=\"headerlink\" title=\"Create the TFTP boot directory\"></a>Create the TFTP boot directory</h2><p>We need to first grab the Linux distro from an image.</p>\n<p>Here I picked Ubuntu 20.04 LTS which is “officially” supported. Technically this is probably not the best option as the OS itself includes a lot of things that we may not actually need and the size is pretty bulky. On the other end the Alpine one is very skinny and lacks some of the tooling we may need initially. For raspberry, unfortunately each distro may be slightly different (in terms of boot) so while you are free to choose whatever you want, YMMV.</p>\n<p>Flash the image into the SD card and we’ll get 2 partitions - boot and root.</p>\n<p>In general, it’s a good idea to first boot the system using the SD card just in case there’s some essential setup we need to complete ahead of time. After that, the remaining Pi boards can replicate the same set of files with minor tweaks.</p>","more":"<p>After that use <code>rsync</code> to copy the boot files:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo rsync -xa /boot 192.168.1.60:&lt;tftp root&gt;/123456789/</span><br></pre></td></tr></table></figure>\n\n<p>Here we need to perform some cleanup.</p>\n<p>Pi will need a few critical files during the boot process.</p>\n<p><code>start4.elf</code>, <code>bcm2711-rpi-4-b.dtb</code>, <code>dtb</code> these are the firmware files<br><code>config.txt</code>, <code>usercfg.txt</code>, <code>syscfg.txt</code> these are the config files to locate the kernel files and related configs<br><code>vmlinuz</code>, <code>System.map</code>, <code>initrd.img</code> Linux kernel files<br><code>cmdline.txt</code> contains config params to pass to the kernel</p>\n<p>The overall <code>boot</code> directory structure would eventually look roughly like this:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">-rwxr--r--   1 root  root     46612 Sep  7 22:20 bcm2711-rpi-4-b.dtb</span><br><span class=\"line\">-rwxr--r--   1 root  root       184 Sep 13 14:56 cmdline.txt</span><br><span class=\"line\">-rw-r--r--   1 root  root    218826 Oct 15 06:16 config-5.4.0-1022-raspi</span><br><span class=\"line\">-rwxr--r--   1 root  root      1189 Nov 27 13:50 config.txt</span><br><span class=\"line\">lrwxr--r--   1 root  root        43 Oct 21 23:47 dtb -&gt; dtbs/5.4.0-1022-raspi/./bcm2711-rpi-4-b.dtb</span><br><span class=\"line\">lrwxr--r--   1 root  root        43 Oct 21 23:47 dtb-5.4.0-1022-raspi -&gt; dtbs/5.4.0-1022-raspi/./bcm2711-rpi-4-b.dtb</span><br><span class=\"line\">drwxr--r--   8 root  root      4096 Oct 21 23:45 dtbs</span><br><span class=\"line\">drwxr--r--   8 root  root      4096 Sep 13 15:13 firmware</span><br><span class=\"line\">-rwxr--r--   1 root  root      5405 Sep  7 22:20 fixup4.dat</span><br><span class=\"line\">lrwxr--r--   1 root  root        27 Oct 21 23:46 initrd.img -&gt; initrd.img-5.4.0-1022-raspi</span><br><span class=\"line\">-rw-r--r--   1 root  root  29325769 Oct 21 23:47 initrd.img-5.4.0-1022-raspi</span><br><span class=\"line\">lrwxr--r--   1 root  root        27 Oct 21 23:46 initrd.img.old -&gt; initrd.img-5.4.0-1021-raspi</span><br><span class=\"line\">-rwxr--r--   1 root  root   2272992 Sep  7 22:21 start4.elf</span><br><span class=\"line\">-rwxr--r--   1 root  root       327 Sep  7 22:21 syscfg.txt</span><br><span class=\"line\">-rw-------   1 root  root   4164641 Oct 15 06:16 System.map-5.4.0-1022-raspi</span><br><span class=\"line\">-rwxr--r--   1 root  root       200 Sep  7 22:21 usercfg.txt</span><br><span class=\"line\">lrwxr--r--   1 root  root        24 Oct 21 23:46 vmlinuz -&gt; vmlinuz-5.4.0-1022-raspi</span><br><span class=\"line\">-rwx------   1 root  root   8306127 Oct  5 02:16 vmlinuz-5.4.0-1022-raspi</span><br><span class=\"line\">-rwx------   1 root  root  25907712 Oct 15 10:16 vmlinux-5.4.0-1022-raspi</span><br><span class=\"line\">lrwxr--r--   1 root  root        24 Oct 21 23:46 vmlinuz.old -&gt; vmlinuz-5.4.0-1021-raspi</span><br></pre></td></tr></table></figure>\n\n<p>We need to make a few modifications here.</p>\n<h3 id=\"Uncompress-the-Linux-kernel-vmlinuz\"><a href=\"#Uncompress-the-Linux-kernel-vmlinuz\" class=\"headerlink\" title=\"Uncompress the Linux kernel vmlinuz\"></a>Uncompress the Linux kernel vmlinuz</h3><p><strong>Pitfall #2: Pi does not uncompress the vmlinuz kernel file</strong></p>\n<p>The typical linux kernel vmlinuz is a gzipped executable that’s initially loaded into the memory. However Pi’s bootloader somehow does not uncompress it and as a result upon boot it would be stuck in rainbow screen.</p>\n<p>Before uncompressing, be aware that some Linux distro may have a non-0 offset of the actual executable. We can check via <code>od</code>. A gzip file has the header <code>1f 8b 08 00</code>.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo od -A d -t x1 vmlinuz-5.4.0-1022-raspi | grep &#x27;1f 8b 08 00&#x27;</span><br><span class=\"line\"></span><br><span class=\"line\">0000000 1f 8b 08 00 00 00 00 00 02 03 ec 5c 0d 74 14 55</span><br></pre></td></tr></table></figure>\n\n<p>Here the offset is <code>0000000</code> and hence we can just do</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zcat vmlinuz-5.4.0-1022-raspi &gt; vmlinux-5.4.0-1022-raspi</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Specify-the-kernel-file-location-in-config\"><a href=\"#Specify-the-kernel-file-location-in-config\" class=\"headerlink\" title=\"Specify the kernel file location in config\"></a>Specify the kernel file location in config</h3><p><strong>Pitfall #3: The default Uboot config does not work</strong></p>\n<p>I didn’t verify other distros but at least with the ubuntu image, the default config is to use uboot. However with netboot config, it would be stuck locating the kernel and other stuff. So here we will tell Pi to boot directly with the vmlinux file we just uncompressed.</p>\n<p>So here we will comment out the original entries and just specify the kernel directly in <code>all</code> section:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[pi4]                            </span><br><span class=\"line\"># kernel=uboot_rpi_4.bin</span><br><span class=\"line\">max_framebuffers=2</span><br><span class=\"line\">                                 </span><br><span class=\"line\">[pi2]                            </span><br><span class=\"line\"># kernel=uboot_rpi_2.bin</span><br><span class=\"line\">                                 </span><br><span class=\"line\">[pi3]                            </span><br><span class=\"line\"># kernel=uboot_rpi_3.bin</span><br><span class=\"line\">                                 </span><br><span class=\"line\">[all]                            </span><br><span class=\"line\">arm_64bit=1                      </span><br><span class=\"line\">device_tree_address=0x03000000</span><br><span class=\"line\">kernel=vmlinux-5.4.0-1022-raspi  </span><br><span class=\"line\">initramfs initrd.img-5.4.0-1022-raspi followkernel</span><br></pre></td></tr></table></figure>\n\n<p>Here we are sort of breaking the best practice by specifying the fixed version here so we are not getting updated kernel. We are doing this because the kernel needs to be uncompressed and <code>initrd</code> needs to match the version. Technically the kernel update process can include the uncompression as well but that would be a TODO item.</p>\n<h3 id=\"Update-cmdline-txt\"><a href=\"#Update-cmdline-txt\" class=\"headerlink\" title=\"Update cmdline.txt\"></a>Update cmdline.txt</h3><p>Here we need to tell the kernel how to mount the root fs.</p>\n<p>Since we are going to set up the diskless boot we will mount via nfs.</p>\n<p>The cmdline.txt would roughly be like:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net.ifnames=0 dwc_otg.lpm_enable=0 console=serial0,115200 console=tty1 root=/dev/nfs nfsroot=192.168.1.60:&lt;nfs root&gt;/raspberry/123456789 ip=dhcp elevator=deadline rootwait fixrtc rw</span><br></pre></td></tr></table></figure>\n\n<p>Here the nfs root is the directory in the NAS. <code>192.168.1.60</code> is the NAS IP. <code>123456789</code> again is the Pi serial but it technically could be anything unique. We are setting it like that for consistency.</p>\n<p>So far we have finished configuring the tftp side.</p>\n<h2 id=\"Create-NFS-root-fs\"><a href=\"#Create-NFS-root-fs\" class=\"headerlink\" title=\"Create NFS root fs\"></a>Create NFS root fs</h2><h3 id=\"Copy-over-Files\"><a href=\"#Copy-over-Files\" class=\"headerlink\" title=\"Copy over Files\"></a>Copy over Files</h3><p>Similar to boot directory we will rsync the root fs:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo rsync -xa / 192.168.1.60:&lt;nfs root&gt;/raspberry/123456789</span><br></pre></td></tr></table></figure>\n\n<p>Here <code>-x</code> is important to ensure we don’t copy anything mounted.<br>Particularly the <code>boot</code> directory would be left out and we can either make a soft link back to the one under tftp, or mount it in <code>/etc/fstab</code> explicitly.</p>\n<h3 id=\"Update-etc-fstab\"><a href=\"#Update-etc-fstab\" class=\"headerlink\" title=\"Update /etc/fstab\"></a>Update /etc/fstab</h3><p>Update the one we just copied over to NAS nfs directory as that would be read upon next boot.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">192.168.1.60:&lt;nfs root&gt;/raspberry/123456789   /       nfs     defaults,rw,nolock   0  0</span><br><span class=\"line\">tmpfs   /tmp    tmpfs   defaults    0   0</span><br><span class=\"line\">tmpfs   /var/tmp    tmpfs   defaults    0   0</span><br><span class=\"line\">tmpfs   /var/run    tmpfs   defaults    0   0</span><br></pre></td></tr></table></figure>\n\n<p>Here we’ll mount some of the tmp dirs to tmpfs (mem fs).</p>\n<p>The root fs is technically not required as in <code>cmdline.txt</code> the kernel already mounts it as <code>rw</code> but here we are explicitly marking it so and turning off disk check.</p>\n<h3 id=\"Enable-NFS-Server\"><a href=\"#Enable-NFS-Server\" class=\"headerlink\" title=\"Enable NFS Server\"></a>Enable NFS Server</h3><p>For Synology this part is easy, in file sharing there’s an option to turn on NFS in File Services.</p>\n<p><strong>Remember to turn off NFS squash in shared file</strong></p>\n<img src=\"/2020/11/28/PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-2-Config-TFTP-and-NFS-mounts/nfs-permission.png\" />\n\n<h2 id=\"Testing-and-Troubleshooting\"><a href=\"#Testing-and-Troubleshooting\" class=\"headerlink\" title=\"Testing and Troubleshooting\"></a>Testing and Troubleshooting</h2><p>So far we have completed the necessary steps. It’s time to unplug the Pi, remove the microSD card and reboot.</p>\n<p>The Pi should tell us it fails reading the card and will try PXE boot next.</p>\n<p>On the first screen it should show what files it’s trying to read and if it’s stuck on some of them check the tftp logs to see if they are placed properly.</p>\n<p>Next it will show the rainbow screen like this</p>\n<img src=\"/2020/11/28/PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-2-Config-TFTP-and-NFS-mounts/rainbow.jpg\" />\n\n<p>If you see this, that means Pi’s GPU has been properly initialized and next it would try to load the kernel.</p>\n<p>If the screen is stuck in this step, go back to check if the kernel file is properly uncompressed and referred to in config.txt file.</p>\n<p>This screen should last for a few seconds to tens of seconds depending on network speed. After that the Linux kernel executable should take it over to init other stuff and mount the NFS root.</p>\n<p>initramfs would be loaded at this moment and if the version does not match you might get some random weird errors. If NFS is not mounted correctly, it would tell so.</p>\n<p>With Ubuntu depending on your actual env, there may be a few other extra non critical errors we need to fix like cloud-init but typically they would not block the startup process.</p>\n<p>Now we should be able to ssh into the system with the username <code>ubuntu</code>.</p>\n<p>Congrats we have finished the netboot setup and this can be replicated to other Pis with the same process.</p>"},{"title":"Why you should ditch Browserify and CommonJS in the http/2 world","date":"2016-09-18T17:15:36.000Z","_content":"\n{% alert info no-icon %}\nStop bundling in the http/2 world since it does it for you.\n{% endalert %}\n\n<!-- toc -->\n\n# Modularization is a great idea\n\nBack in the old days where there were no concept regarding frontend package management, we would lay out all the scripts in order in the html file, and hope for the best that they would somehow work together if order were right. This surely doesn't work well with huge projects, but luckily back then JavaScripts weren't so shiny anyways - UIs weren't so cool and logic was much simpler. However, things do evolve. People soon noticed that this approach wouldn't scale - cooperation across multiple teams becomes super tricky, if not impossible, and it doesn't play well with DRY either.\n\nThen people came up with a great idea of modularizing JS code (probably back in 2003?) the same way you would do for your beloved Java/C++ code libraries. And then there came the CommonJS definition concept by Kevin Dangoor back in 2009. Many people got to know this idea thanks to Node.js, and it works quite well, especially for server side code. Now you can easily use npm and build both the frontend and backend using the same tool very quickly, thanks to the JS community. Since people have the same interface for code modularization, team cooperation becomes much easier and projects gain benefit from much better encapsulation.\n\n<!-- more -->\n\n# And browserify was a great tool\n\nBrowerify is a bundling tool based on CommonJS definition by providing polyfill for `require` and `define` calls in browser. Back in the days when AMD wasn't ready, it gave people an easy way of defining your modules the same way as for other node modules and serving everything together as one giant bundle. The idea is based on the fact that browsers have concurrent http request limit, let alone now full-site https becomes popular and SSL handshaking is quite expensive. So by bundling, you cut the number of required requests and hence you get faster page loading.\n\n# However, there are a few small problems...\n\nUnlike AMD, CommonJS is synchronous and you can tell from their API design:\n\n{% codeblock AMD lang:js %}\ndefine(['foo', 'bar'], function (foo, bar) {\n    // code begins\n});\n{% endcodeblock %}\n\n{% codeblock CommonJS lang:js %}\nvar foo = require('foo');\nvar bar = require('bar');\n\n// code begins\n{% endcodeblock %}\n\nIt's easy to load AMD modules asynchronously because the actual code lives in a callback so your loader can play scatter-n-gather. For CommonJS, however, each require call would have to wait until the previous one comes back, which means the loader cannot utilize Ajax and spread the load to multiple http requests in order to speed up the loading process. The solution for browserify is to bundle everything together and hence it can just grab that reference for you in a map. However, if your code library is huge, then it means your initial page load time and above-the-fold time (time between user hits enter and the content in the current browser window stops changing) will be negatively impacted. Moreover, on demand module loading becomes impossible here while in AMD, it's pretty simple.\n\nAnother problem with browserify, which is the main reason why I don't like it, is that bundling becomes mandatory. This makes local dev environment setup and testing tricky and surprise-prone. Using browserify means if my app requires ABC in general, I still need to bundle everything together while I just want to test C. To me that's just ridiculous.\n\n# And http/2 increases the gap\n\nHttp/2 introduces multiplexing which makes CommonJS approach even more crippled. Multiplexing means now you can use one TCP connection to transfer different content from different sources (URLs) on a single host, which makes asynchronous module loading greater. But if you bundle everything together, sorry you are just ignoring all those benefits.\n\nA lot of CDNs support http/2 now, including Akamai, Cloudflare and Cloudfront, you name it. Here's a demo from cloudflare:\n\n{% asset_img comparison.gif %}\n\n(Tested in Chrome 53.0)\n\n# So what to use instead?\n\nECMA 6 is really the way to go. It consolidates the nice API design from CommonJS and the asynchronous and on-demand module loading feature from AMD. Currently, most browsers don't support it yet (as of Sep 2016). Surprisingly Microsoft leads the way this time by [allowing you to turn on this experimental feature](https://blogs.windows.com/msedgedev/2016/05/17/es6-modules-and-beyond). Regardless of that, Babel can help you turn es6 code to es5, and System.js can fill the gap as the module loader.\n\nJSPM, Babel and System.js is a good combination for the time being. Everything just works seamlessly across different scenarios - you don't need to bundle anything in dev environment while getting all the benefits in production with the same set of configuration. You have the freedom - choose to bundle your scripts for best performance for legacy browser support, or on demand, no bundling for best performance under http/2. The actual workflow configuration is out of the scope of this post. I'll write a post later to discuss this in details, including comparison with the popular bundler - webpack.\n\nHowever, if you can't switch to that for some reason, you can still keep using AMD. The major problems with AMD are:\n\n1. tedious API design (I don't see how this can be changed without a transpiler due to native JS limit)\n2. not friendly to IDE due to separation of module naming and configuration (and webpack suffers from similar issues)\n\nHttp/2 makes a lot of old \"golden rules\" no longer correct. Spriting for images and bundling for scripts used to be so true that some people just blindly follow them. However, the world keeps involving so time to keep our eyes open.","source":"_posts/Why-you-should-ditch-browserify-and-commonjs-in-the-http-2-world.md","raw":"---\ntitle: Why you should ditch Browserify and CommonJS in the http/2 world\ndate: 2016-09-18 10:15:36\ncategories:\n- Frontend\ntags:\n- http2\n- browserify\n- javascript\n- dependency-management\n- modular-design\n- es6\n---\n\n{% alert info no-icon %}\nStop bundling in the http/2 world since it does it for you.\n{% endalert %}\n\n<!-- toc -->\n\n# Modularization is a great idea\n\nBack in the old days where there were no concept regarding frontend package management, we would lay out all the scripts in order in the html file, and hope for the best that they would somehow work together if order were right. This surely doesn't work well with huge projects, but luckily back then JavaScripts weren't so shiny anyways - UIs weren't so cool and logic was much simpler. However, things do evolve. People soon noticed that this approach wouldn't scale - cooperation across multiple teams becomes super tricky, if not impossible, and it doesn't play well with DRY either.\n\nThen people came up with a great idea of modularizing JS code (probably back in 2003?) the same way you would do for your beloved Java/C++ code libraries. And then there came the CommonJS definition concept by Kevin Dangoor back in 2009. Many people got to know this idea thanks to Node.js, and it works quite well, especially for server side code. Now you can easily use npm and build both the frontend and backend using the same tool very quickly, thanks to the JS community. Since people have the same interface for code modularization, team cooperation becomes much easier and projects gain benefit from much better encapsulation.\n\n<!-- more -->\n\n# And browserify was a great tool\n\nBrowerify is a bundling tool based on CommonJS definition by providing polyfill for `require` and `define` calls in browser. Back in the days when AMD wasn't ready, it gave people an easy way of defining your modules the same way as for other node modules and serving everything together as one giant bundle. The idea is based on the fact that browsers have concurrent http request limit, let alone now full-site https becomes popular and SSL handshaking is quite expensive. So by bundling, you cut the number of required requests and hence you get faster page loading.\n\n# However, there are a few small problems...\n\nUnlike AMD, CommonJS is synchronous and you can tell from their API design:\n\n{% codeblock AMD lang:js %}\ndefine(['foo', 'bar'], function (foo, bar) {\n    // code begins\n});\n{% endcodeblock %}\n\n{% codeblock CommonJS lang:js %}\nvar foo = require('foo');\nvar bar = require('bar');\n\n// code begins\n{% endcodeblock %}\n\nIt's easy to load AMD modules asynchronously because the actual code lives in a callback so your loader can play scatter-n-gather. For CommonJS, however, each require call would have to wait until the previous one comes back, which means the loader cannot utilize Ajax and spread the load to multiple http requests in order to speed up the loading process. The solution for browserify is to bundle everything together and hence it can just grab that reference for you in a map. However, if your code library is huge, then it means your initial page load time and above-the-fold time (time between user hits enter and the content in the current browser window stops changing) will be negatively impacted. Moreover, on demand module loading becomes impossible here while in AMD, it's pretty simple.\n\nAnother problem with browserify, which is the main reason why I don't like it, is that bundling becomes mandatory. This makes local dev environment setup and testing tricky and surprise-prone. Using browserify means if my app requires ABC in general, I still need to bundle everything together while I just want to test C. To me that's just ridiculous.\n\n# And http/2 increases the gap\n\nHttp/2 introduces multiplexing which makes CommonJS approach even more crippled. Multiplexing means now you can use one TCP connection to transfer different content from different sources (URLs) on a single host, which makes asynchronous module loading greater. But if you bundle everything together, sorry you are just ignoring all those benefits.\n\nA lot of CDNs support http/2 now, including Akamai, Cloudflare and Cloudfront, you name it. Here's a demo from cloudflare:\n\n{% asset_img comparison.gif %}\n\n(Tested in Chrome 53.0)\n\n# So what to use instead?\n\nECMA 6 is really the way to go. It consolidates the nice API design from CommonJS and the asynchronous and on-demand module loading feature from AMD. Currently, most browsers don't support it yet (as of Sep 2016). Surprisingly Microsoft leads the way this time by [allowing you to turn on this experimental feature](https://blogs.windows.com/msedgedev/2016/05/17/es6-modules-and-beyond). Regardless of that, Babel can help you turn es6 code to es5, and System.js can fill the gap as the module loader.\n\nJSPM, Babel and System.js is a good combination for the time being. Everything just works seamlessly across different scenarios - you don't need to bundle anything in dev environment while getting all the benefits in production with the same set of configuration. You have the freedom - choose to bundle your scripts for best performance for legacy browser support, or on demand, no bundling for best performance under http/2. The actual workflow configuration is out of the scope of this post. I'll write a post later to discuss this in details, including comparison with the popular bundler - webpack.\n\nHowever, if you can't switch to that for some reason, you can still keep using AMD. The major problems with AMD are:\n\n1. tedious API design (I don't see how this can be changed without a transpiler due to native JS limit)\n2. not friendly to IDE due to separation of module naming and configuration (and webpack suffers from similar issues)\n\nHttp/2 makes a lot of old \"golden rules\" no longer correct. Spriting for images and bundling for scripts used to be so true that some people just blindly follow them. However, the world keeps involving so time to keep our eyes open.","slug":"Why-you-should-ditch-browserify-and-commonjs-in-the-http-2-world","published":1,"updated":"2025-09-01T22:26:51.292Z","comments":1,"layout":"post","photos":[],"_id":"cmf1pfvse001d8mmgfwy7006u","content":"<div class=\"alert info no-icon\"><p>Stop bundling in the http/2 world since it does it for you.</p>\n</div>\n\n<!-- toc -->\n\n<h1 id=\"Modularization-is-a-great-idea\"><a href=\"#Modularization-is-a-great-idea\" class=\"headerlink\" title=\"Modularization is a great idea\"></a>Modularization is a great idea</h1><p>Back in the old days where there were no concept regarding frontend package management, we would lay out all the scripts in order in the html file, and hope for the best that they would somehow work together if order were right. This surely doesn’t work well with huge projects, but luckily back then JavaScripts weren’t so shiny anyways - UIs weren’t so cool and logic was much simpler. However, things do evolve. People soon noticed that this approach wouldn’t scale - cooperation across multiple teams becomes super tricky, if not impossible, and it doesn’t play well with DRY either.</p>\n<p>Then people came up with a great idea of modularizing JS code (probably back in 2003?) the same way you would do for your beloved Java/C++ code libraries. And then there came the CommonJS definition concept by Kevin Dangoor back in 2009. Many people got to know this idea thanks to Node.js, and it works quite well, especially for server side code. Now you can easily use npm and build both the frontend and backend using the same tool very quickly, thanks to the JS community. Since people have the same interface for code modularization, team cooperation becomes much easier and projects gain benefit from much better encapsulation.</p>\n<span id=\"more\"></span>\n\n<h1 id=\"And-browserify-was-a-great-tool\"><a href=\"#And-browserify-was-a-great-tool\" class=\"headerlink\" title=\"And browserify was a great tool\"></a>And browserify was a great tool</h1><p>Browerify is a bundling tool based on CommonJS definition by providing polyfill for <code>require</code> and <code>define</code> calls in browser. Back in the days when AMD wasn’t ready, it gave people an easy way of defining your modules the same way as for other node modules and serving everything together as one giant bundle. The idea is based on the fact that browsers have concurrent http request limit, let alone now full-site https becomes popular and SSL handshaking is quite expensive. So by bundling, you cut the number of required requests and hence you get faster page loading.</p>\n<h1 id=\"However-there-are-a-few-small-problems…\"><a href=\"#However-there-are-a-few-small-problems…\" class=\"headerlink\" title=\"However, there are a few small problems…\"></a>However, there are a few small problems…</h1><p>Unlike AMD, CommonJS is synchronous and you can tell from their API design:</p>\n<figure class=\"highlight js\"><figcaption><span>AMD</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"title function_\">define</span>([<span class=\"string\">&#x27;foo&#x27;</span>, <span class=\"string\">&#x27;bar&#x27;</span>], <span class=\"keyword\">function</span> (<span class=\"params\">foo, bar</span>) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// code begins</span></span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight js\"><figcaption><span>CommonJS</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> foo = <span class=\"built_in\">require</span>(<span class=\"string\">&#x27;foo&#x27;</span>);</span><br><span class=\"line\"><span class=\"keyword\">var</span> bar = <span class=\"built_in\">require</span>(<span class=\"string\">&#x27;bar&#x27;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// code begins</span></span><br></pre></td></tr></table></figure>\n\n<p>It’s easy to load AMD modules asynchronously because the actual code lives in a callback so your loader can play scatter-n-gather. For CommonJS, however, each require call would have to wait until the previous one comes back, which means the loader cannot utilize Ajax and spread the load to multiple http requests in order to speed up the loading process. The solution for browserify is to bundle everything together and hence it can just grab that reference for you in a map. However, if your code library is huge, then it means your initial page load time and above-the-fold time (time between user hits enter and the content in the current browser window stops changing) will be negatively impacted. Moreover, on demand module loading becomes impossible here while in AMD, it’s pretty simple.</p>\n<p>Another problem with browserify, which is the main reason why I don’t like it, is that bundling becomes mandatory. This makes local dev environment setup and testing tricky and surprise-prone. Using browserify means if my app requires ABC in general, I still need to bundle everything together while I just want to test C. To me that’s just ridiculous.</p>\n<h1 id=\"And-http-2-increases-the-gap\"><a href=\"#And-http-2-increases-the-gap\" class=\"headerlink\" title=\"And http/2 increases the gap\"></a>And http/2 increases the gap</h1><p>Http/2 introduces multiplexing which makes CommonJS approach even more crippled. Multiplexing means now you can use one TCP connection to transfer different content from different sources (URLs) on a single host, which makes asynchronous module loading greater. But if you bundle everything together, sorry you are just ignoring all those benefits.</p>\n<p>A lot of CDNs support http/2 now, including Akamai, Cloudflare and Cloudfront, you name it. Here’s a demo from cloudflare:</p>\n<img src=\"/2016/09/18/Why-you-should-ditch-browserify-and-commonjs-in-the-http-2-world/comparison.gif\" class=\"\">\n\n<p>(Tested in Chrome 53.0)</p>\n<h1 id=\"So-what-to-use-instead\"><a href=\"#So-what-to-use-instead\" class=\"headerlink\" title=\"So what to use instead?\"></a>So what to use instead?</h1><p>ECMA 6 is really the way to go. It consolidates the nice API design from CommonJS and the asynchronous and on-demand module loading feature from AMD. Currently, most browsers don’t support it yet (as of Sep 2016). Surprisingly Microsoft leads the way this time by <a href=\"https://blogs.windows.com/msedgedev/2016/05/17/es6-modules-and-beyond\">allowing you to turn on this experimental feature</a>. Regardless of that, Babel can help you turn es6 code to es5, and System.js can fill the gap as the module loader.</p>\n<p>JSPM, Babel and System.js is a good combination for the time being. Everything just works seamlessly across different scenarios - you don’t need to bundle anything in dev environment while getting all the benefits in production with the same set of configuration. You have the freedom - choose to bundle your scripts for best performance for legacy browser support, or on demand, no bundling for best performance under http/2. The actual workflow configuration is out of the scope of this post. I’ll write a post later to discuss this in details, including comparison with the popular bundler - webpack.</p>\n<p>However, if you can’t switch to that for some reason, you can still keep using AMD. The major problems with AMD are:</p>\n<ol>\n<li>tedious API design (I don’t see how this can be changed without a transpiler due to native JS limit)</li>\n<li>not friendly to IDE due to separation of module naming and configuration (and webpack suffers from similar issues)</li>\n</ol>\n<p>Http/2 makes a lot of old “golden rules” no longer correct. Spriting for images and bundling for scripts used to be so true that some people just blindly follow them. However, the world keeps involving so time to keep our eyes open.</p>\n","thumbnailImageUrl":null,"excerpt":"<div class=\"alert info no-icon\"><p>Stop bundling in the http/2 world since it does it for you.</p>\n</div>\n\n<!-- toc -->\n\n<h1 id=\"Modularization-is-a-great-idea\"><a href=\"#Modularization-is-a-great-idea\" class=\"headerlink\" title=\"Modularization is a great idea\"></a>Modularization is a great idea</h1><p>Back in the old days where there were no concept regarding frontend package management, we would lay out all the scripts in order in the html file, and hope for the best that they would somehow work together if order were right. This surely doesn’t work well with huge projects, but luckily back then JavaScripts weren’t so shiny anyways - UIs weren’t so cool and logic was much simpler. However, things do evolve. People soon noticed that this approach wouldn’t scale - cooperation across multiple teams becomes super tricky, if not impossible, and it doesn’t play well with DRY either.</p>\n<p>Then people came up with a great idea of modularizing JS code (probably back in 2003?) the same way you would do for your beloved Java/C++ code libraries. And then there came the CommonJS definition concept by Kevin Dangoor back in 2009. Many people got to know this idea thanks to Node.js, and it works quite well, especially for server side code. Now you can easily use npm and build both the frontend and backend using the same tool very quickly, thanks to the JS community. Since people have the same interface for code modularization, team cooperation becomes much easier and projects gain benefit from much better encapsulation.</p>","more":"<h1 id=\"And-browserify-was-a-great-tool\"><a href=\"#And-browserify-was-a-great-tool\" class=\"headerlink\" title=\"And browserify was a great tool\"></a>And browserify was a great tool</h1><p>Browerify is a bundling tool based on CommonJS definition by providing polyfill for <code>require</code> and <code>define</code> calls in browser. Back in the days when AMD wasn’t ready, it gave people an easy way of defining your modules the same way as for other node modules and serving everything together as one giant bundle. The idea is based on the fact that browsers have concurrent http request limit, let alone now full-site https becomes popular and SSL handshaking is quite expensive. So by bundling, you cut the number of required requests and hence you get faster page loading.</p>\n<h1 id=\"However-there-are-a-few-small-problems…\"><a href=\"#However-there-are-a-few-small-problems…\" class=\"headerlink\" title=\"However, there are a few small problems…\"></a>However, there are a few small problems…</h1><p>Unlike AMD, CommonJS is synchronous and you can tell from their API design:</p>\n<figure class=\"highlight js\"><figcaption><span>AMD</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"title function_\">define</span>([<span class=\"string\">&#x27;foo&#x27;</span>, <span class=\"string\">&#x27;bar&#x27;</span>], <span class=\"keyword\">function</span> (<span class=\"params\">foo, bar</span>) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// code begins</span></span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight js\"><figcaption><span>CommonJS</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> foo = <span class=\"built_in\">require</span>(<span class=\"string\">&#x27;foo&#x27;</span>);</span><br><span class=\"line\"><span class=\"keyword\">var</span> bar = <span class=\"built_in\">require</span>(<span class=\"string\">&#x27;bar&#x27;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// code begins</span></span><br></pre></td></tr></table></figure>\n\n<p>It’s easy to load AMD modules asynchronously because the actual code lives in a callback so your loader can play scatter-n-gather. For CommonJS, however, each require call would have to wait until the previous one comes back, which means the loader cannot utilize Ajax and spread the load to multiple http requests in order to speed up the loading process. The solution for browserify is to bundle everything together and hence it can just grab that reference for you in a map. However, if your code library is huge, then it means your initial page load time and above-the-fold time (time between user hits enter and the content in the current browser window stops changing) will be negatively impacted. Moreover, on demand module loading becomes impossible here while in AMD, it’s pretty simple.</p>\n<p>Another problem with browserify, which is the main reason why I don’t like it, is that bundling becomes mandatory. This makes local dev environment setup and testing tricky and surprise-prone. Using browserify means if my app requires ABC in general, I still need to bundle everything together while I just want to test C. To me that’s just ridiculous.</p>\n<h1 id=\"And-http-2-increases-the-gap\"><a href=\"#And-http-2-increases-the-gap\" class=\"headerlink\" title=\"And http/2 increases the gap\"></a>And http/2 increases the gap</h1><p>Http/2 introduces multiplexing which makes CommonJS approach even more crippled. Multiplexing means now you can use one TCP connection to transfer different content from different sources (URLs) on a single host, which makes asynchronous module loading greater. But if you bundle everything together, sorry you are just ignoring all those benefits.</p>\n<p>A lot of CDNs support http/2 now, including Akamai, Cloudflare and Cloudfront, you name it. Here’s a demo from cloudflare:</p>\n<img src=\"/2016/09/18/Why-you-should-ditch-browserify-and-commonjs-in-the-http-2-world/comparison.gif\" class=\"\">\n\n<p>(Tested in Chrome 53.0)</p>\n<h1 id=\"So-what-to-use-instead\"><a href=\"#So-what-to-use-instead\" class=\"headerlink\" title=\"So what to use instead?\"></a>So what to use instead?</h1><p>ECMA 6 is really the way to go. It consolidates the nice API design from CommonJS and the asynchronous and on-demand module loading feature from AMD. Currently, most browsers don’t support it yet (as of Sep 2016). Surprisingly Microsoft leads the way this time by <a href=\"https://blogs.windows.com/msedgedev/2016/05/17/es6-modules-and-beyond\">allowing you to turn on this experimental feature</a>. Regardless of that, Babel can help you turn es6 code to es5, and System.js can fill the gap as the module loader.</p>\n<p>JSPM, Babel and System.js is a good combination for the time being. Everything just works seamlessly across different scenarios - you don’t need to bundle anything in dev environment while getting all the benefits in production with the same set of configuration. You have the freedom - choose to bundle your scripts for best performance for legacy browser support, or on demand, no bundling for best performance under http/2. The actual workflow configuration is out of the scope of this post. I’ll write a post later to discuss this in details, including comparison with the popular bundler - webpack.</p>\n<p>However, if you can’t switch to that for some reason, you can still keep using AMD. The major problems with AMD are:</p>\n<ol>\n<li>tedious API design (I don’t see how this can be changed without a transpiler due to native JS limit)</li>\n<li>not friendly to IDE due to separation of module naming and configuration (and webpack suffers from similar issues)</li>\n</ol>\n<p>Http/2 makes a lot of old “golden rules” no longer correct. Spriting for images and bundling for scripts used to be so true that some people just blindly follow them. However, the world keeps involving so time to keep our eyes open.</p>"},{"title":"Why Keeping High Standards of Shit Is Important","date":"2020-05-23T23:54:54.000Z","_content":"\nWe all write shitty code.\n\nThis seems like a stupidly bold statement from a random person, especially when this person very likely writes shitter stuff than most others. After all, there's a well known joke across the industry - that is, Jeff Dean does not write buggy code, and when he does, it's the problem of the compiler. So at least we have one counter example.\n\nIn common sense, masters don't write shit, and Linus Torvalds very likely nods in the line.\n\nWell, technically speaking, before we criticize this non sense bold statement, we need to define what shit is. And here, I think shitty code is **anything that does not live up to the expectation**. Buggy code is shitty - because it doesn't provide consistently correct results when people expect it to. However, in the real world, we tend to operate at a much higher level - we care more about contracts, functionalities, and behaviors, and less about the exact implementations. For example, when we use an API service, we tend to care about what features it can provide, the QPS it can support, and how easily we can use it - in essence, our expectation. We probably won't care about whether it's run on bare metal or some public cloud, whether the memory is manually managed, by compiler, or by garbage collection.\n\nDoes that then mean all services/software that fail to match expectation start with shitty code? Well not necessarily. Windows 9x was great at the time it was first introduced and enabled a whole industry. Yet as things evolved, a lot of the design issues started to get exacerbated and it eventually got completely replaced by NT kernel. I'm not trying to defend 9x and saying it was very well designed masterpiece by any means - it had some notorious kernel safety issues after all, but just using it as an example to show that even if some software had highlights, over time other factors could expose the shittiness inside - it's just a matter of time. What I believe is that no matter how well the systems were designed and implemented at the beginning, they deteriorate over time. We cannot prevent things from turning into shit - we can only slow down the speed.\n\nWhat makes things more complicated is that, often times, there are way too many factors that have impact on how we value software:\n\n<!-- more -->\n\nAt high level, users' expectations may change, the overall world trend may evolve, or alternative software may leap forward.\n\nUnder microscope, experienced maintainers may leave, tools/development environment may change, or simply resource priority may shift and the project may not be able to be taken care of for years.\n\nFor example, in the cloudify trend, a lot of companies try to ditch Oracle databases in favor of AWS/Azure. E-commers turn to Salesforce rather than costly, heavy weight SAP systems. It's most likely not the problem of the latter systems per se - in fact Oracle is still irreplaceable in certain areas if you are looking for stable, state-of-the-art single-machine db throughput for a traditional RDBMS. It's more about expectation management - if you need to tighten a screw, you need a screwdriver, not a hammer, no matter how good the hammer is, because it won't do the work you expect it to.\n\nWith these said, I'm not trying to say that there's very little engineers have control of. Quite the contrary, the point I'm trying to make here is, despite the fact that there are so many external factors that could accelerate the speed of software deterioration, there's still something engineers have control of to slow that down, with all other things being equal, which is to try to maintain a high standard of shit.\n\nWhile this does seem very meta, the core ideas in practice, in which I believe, are continuous delivery and following best practices.\n\n### Continuous Delivery\n\nContinuous delivery allows for faster code iteration, earlier error detection and recovery, and more agile response to the ever changing requirements. This is a very broad topic in practice and easier said than done, yet it's not a unsolvable problem. At the very basic layer, continuous delivery has 3 fundamental parts - a scalable VCS to allow people to continuously evolve the **entire** code base, a CI/CD platform to allow for reliable shipment and a mechanism to automatically disable dead/unmaintained code.\n\nThere are a lot of good examples that exist in industry: Google and Facebook have put enormous effort into scaling the VCS platform to support their monolithic repos. This is not nonsense - while monolithic repos may not work for everyone, engineering wise, it's the most cost-effective way at scale to save time handling dependency problems and force people to move forward together by atomic large scale code mods.\n\nAmazon, on the other hand, offers Apollo to employees to allow engineers to automate reliable shipment of software to their global fleet. All the automation, at the end of the day, is to eliminate software rot because of human error and laziness.\n\nThe third part - eliminating dead/unmaintained software - is often overlooked and a lot of people/companies have stumped on this. The basic idea is simple - since all software deteriorates over time, the only unchanging thing is to make changes and the only way to avoid forgetting about something is to keep doing it. In practice, at the very minimum, this means setting up some deadline and after which the binary is not allowed to run, in order to force people to upgrade, even if no logic is changed. This is hard indeed in reality because often times when you build infrastructure level software for a global hardware fleet, it's costly to update and update may lead to errors that could be prevented otherwise, let alone for commercial software, where there are business contracts that prevent stuff from being upgraded. So people sit on high horses and say if it ain't break, don't fix it. This is true in most cases. However then those people often are also the ones who fall harder when something crucial, that hasn't been maintained for years, eventually breaks. In practice, this is less of a technical issue - it's more about engineering culture, which essentially is how much time/money the company/team is willing to spend on things that do not have short term tangible value, yet valuable when team needs the strength in unexpected time.\n\n### Best Practices\n\nThe other part - following best practices - is also culture related. Often it's hard to measure, and it's tempting to violate when you are in a rush. A more alarming fact is, it's often hard to keep that as a habit for a team and it takes a long time to build and maintain that culture, yet it's very easy to crush as it's viral - once someone shows the short term benefit of doing so and does not get penalized, others would follow and eventually things get out of control. What's worse, when you ask them why they don't follow, they might bounce back and ask you why their work cannot be valued because they save time by cutting corners yet still get the product features delivered, so they can have more time finding the next feature to develop.\n\nThe thing is, at the very beginning, there's no so-called best practices. However every kind of freedom has a price. People gradually find that certain patterns lead to less bugs and cleaner, easier-to-maintain code. Or in other words, they \"rot\" more slowly than others. Even though all software eventually fails if they don't get maintained, at least they fail in a less hilarious way. Unfortunately a lot of developers nowadays tend to ignore that in favor of pragmatism mainly because unfortunately following best practices tends to not have immediate benefit - unless one has some kind of OCD.\n\nThere's various ways to tackle that in practice. Google, for example, strives to maintain a centralized standard of coding practices. It operates in a model somewhat like GPL license - you need to get the proof that you can write good quality code from people who have the proof. Then in the ideal world, that leads to a situation that people would operate on the same standard using the same language. This also works for small startups through code reviews when you only have that many engineers. This basically operates on a psychological level because when people violate the practices, you feel morally superior to them and you can criticize the violation publicly through code review, which in practice, leads to more people willing to follow the majority.\n\nEverything comes with trade-off though. This, however, in practice leads to slight efficiency penalty, because people may have to fix the \"minutes\" (e.g. switching to multithreading in a non critical code path because in theory you should and other people think so) instead of only focusing more on the higher, directional level (i.e. I can ship my stuff as long as it appears to be working at that moment). So product-oriented companies like Facebook, on the other hand, tend to give trust and some kind of freedom to engineers. As a result, different teams/orgs could have different standards and slightly different preferences. Amazon even pushes that level higher as each team/org owns separate repos. And when things become bad to certain degree, they carry out the big lawn mower ([fastmod](https://github.com/facebookincubator/fastmod) for example) to fix all similar issues in an atomic way (this is why being able to automating the elimination of dead code is important for big companies). Some people don't agree on this approach, but that way does have a valuable point, which is that enforcing individual behavior cannot scale together with the fast changing environment. Practicing large scale automation is more important than to train individuals to be sticking to \"best practices\" and do everything right - simply because nobody can't over time. That way, people spend more time \"solving real world problems\" than \"thinking about whether some piece of code could be refactored to be reused\". After all, what's the value of latter in performance evaluation if it's not measurable?\n\nUnfortunately not all people think that way. A lot of people, especially in open source software community, believe that before you become a pure problem solver, you are an engineer who writes shit. And every piece of shit could fall apart in a different way. The idea of best practice, is to enable other people to follow the logic of your shit more easily, require less context switching, and locate problems faster when things break. This is why people come up with design patterns. To problem solvers' point, they are useless, because they don't address a single real world problem, at least not directly. Some people would argue with them that they allow one to write shit with low coupling and high cohesion. From my perspective, I think the value of following such best practice, is to allow the shit to break, if eventually it will, in a less laughable way.\n\nThere are examples of such failure - not long ago, Facebook updated the server code which led to [massive app launch crash including spotify, tiktok and google](https://www.theverge.com/2020/5/7/21250689/facebook-sdk-bug-ios-app-crash-apple-spotify-venmo-tiktok-tinder) due to a \"simple\" type mismatch error ([this github issue](https://github.com/facebook/facebook-ios-sdk/issues/1373) reveals some technical details). People laughed at that, not because it's yet another critical bug, but it's an example to show the price for ignoring best practices, and hence software breaks in an interesting way. Some people claim that this requires better testing and type checking. IMHO, this is more of a high horse style solution because there's **always** stuff you'd mis-test in large scale software. Over time if there's no issues happening, people get used to and bored so issues would emerge again and similar situations start all over. The general industry best practice would tell us that the only way to fundamentally prevent such issues is to keep the interface in sync. There are various ways to achieve that with one being using an IDL to define such thing (thrift, protobuf, you name it). I'm not a mobile developer so I'm sure there's reasons/trade-offs behind why no such thing is being used in this case, even if some of which might not be purely technical. But regardless of that, the point is these practices are not new. They came from people who had failed in history - coding to interfaces is not a new idea, it's been there for decades in the OOP world. Nothing would fail immediately if one ignores those, but eventually one would join the other group and advocate for such thing.\n\n\nAll software decays. The idea of software engineering, at the end of the day, is not to prevent people from writing shit. However, if we keep trying to maintain a high standard of shit, or trying to get closer to it, then people would be more willing to accept the fact that things would eventually break. And when that happens, there would be more people willing to help it out, and this is how communities are formed, not just by end results, but more by reputation and expectation.\n\nAfter all, we should make each shit do one thing, and do it well.","source":"_posts/Why-Keeping-High-Standard-of-Shit-Is-Important.md","raw":"---\ntitle: Why Keeping High Standards of Shit Is Important\ndate: 2020-05-23 16:54:54\ntags:\n- software engineering\n---\n\nWe all write shitty code.\n\nThis seems like a stupidly bold statement from a random person, especially when this person very likely writes shitter stuff than most others. After all, there's a well known joke across the industry - that is, Jeff Dean does not write buggy code, and when he does, it's the problem of the compiler. So at least we have one counter example.\n\nIn common sense, masters don't write shit, and Linus Torvalds very likely nods in the line.\n\nWell, technically speaking, before we criticize this non sense bold statement, we need to define what shit is. And here, I think shitty code is **anything that does not live up to the expectation**. Buggy code is shitty - because it doesn't provide consistently correct results when people expect it to. However, in the real world, we tend to operate at a much higher level - we care more about contracts, functionalities, and behaviors, and less about the exact implementations. For example, when we use an API service, we tend to care about what features it can provide, the QPS it can support, and how easily we can use it - in essence, our expectation. We probably won't care about whether it's run on bare metal or some public cloud, whether the memory is manually managed, by compiler, or by garbage collection.\n\nDoes that then mean all services/software that fail to match expectation start with shitty code? Well not necessarily. Windows 9x was great at the time it was first introduced and enabled a whole industry. Yet as things evolved, a lot of the design issues started to get exacerbated and it eventually got completely replaced by NT kernel. I'm not trying to defend 9x and saying it was very well designed masterpiece by any means - it had some notorious kernel safety issues after all, but just using it as an example to show that even if some software had highlights, over time other factors could expose the shittiness inside - it's just a matter of time. What I believe is that no matter how well the systems were designed and implemented at the beginning, they deteriorate over time. We cannot prevent things from turning into shit - we can only slow down the speed.\n\nWhat makes things more complicated is that, often times, there are way too many factors that have impact on how we value software:\n\n<!-- more -->\n\nAt high level, users' expectations may change, the overall world trend may evolve, or alternative software may leap forward.\n\nUnder microscope, experienced maintainers may leave, tools/development environment may change, or simply resource priority may shift and the project may not be able to be taken care of for years.\n\nFor example, in the cloudify trend, a lot of companies try to ditch Oracle databases in favor of AWS/Azure. E-commers turn to Salesforce rather than costly, heavy weight SAP systems. It's most likely not the problem of the latter systems per se - in fact Oracle is still irreplaceable in certain areas if you are looking for stable, state-of-the-art single-machine db throughput for a traditional RDBMS. It's more about expectation management - if you need to tighten a screw, you need a screwdriver, not a hammer, no matter how good the hammer is, because it won't do the work you expect it to.\n\nWith these said, I'm not trying to say that there's very little engineers have control of. Quite the contrary, the point I'm trying to make here is, despite the fact that there are so many external factors that could accelerate the speed of software deterioration, there's still something engineers have control of to slow that down, with all other things being equal, which is to try to maintain a high standard of shit.\n\nWhile this does seem very meta, the core ideas in practice, in which I believe, are continuous delivery and following best practices.\n\n### Continuous Delivery\n\nContinuous delivery allows for faster code iteration, earlier error detection and recovery, and more agile response to the ever changing requirements. This is a very broad topic in practice and easier said than done, yet it's not a unsolvable problem. At the very basic layer, continuous delivery has 3 fundamental parts - a scalable VCS to allow people to continuously evolve the **entire** code base, a CI/CD platform to allow for reliable shipment and a mechanism to automatically disable dead/unmaintained code.\n\nThere are a lot of good examples that exist in industry: Google and Facebook have put enormous effort into scaling the VCS platform to support their monolithic repos. This is not nonsense - while monolithic repos may not work for everyone, engineering wise, it's the most cost-effective way at scale to save time handling dependency problems and force people to move forward together by atomic large scale code mods.\n\nAmazon, on the other hand, offers Apollo to employees to allow engineers to automate reliable shipment of software to their global fleet. All the automation, at the end of the day, is to eliminate software rot because of human error and laziness.\n\nThe third part - eliminating dead/unmaintained software - is often overlooked and a lot of people/companies have stumped on this. The basic idea is simple - since all software deteriorates over time, the only unchanging thing is to make changes and the only way to avoid forgetting about something is to keep doing it. In practice, at the very minimum, this means setting up some deadline and after which the binary is not allowed to run, in order to force people to upgrade, even if no logic is changed. This is hard indeed in reality because often times when you build infrastructure level software for a global hardware fleet, it's costly to update and update may lead to errors that could be prevented otherwise, let alone for commercial software, where there are business contracts that prevent stuff from being upgraded. So people sit on high horses and say if it ain't break, don't fix it. This is true in most cases. However then those people often are also the ones who fall harder when something crucial, that hasn't been maintained for years, eventually breaks. In practice, this is less of a technical issue - it's more about engineering culture, which essentially is how much time/money the company/team is willing to spend on things that do not have short term tangible value, yet valuable when team needs the strength in unexpected time.\n\n### Best Practices\n\nThe other part - following best practices - is also culture related. Often it's hard to measure, and it's tempting to violate when you are in a rush. A more alarming fact is, it's often hard to keep that as a habit for a team and it takes a long time to build and maintain that culture, yet it's very easy to crush as it's viral - once someone shows the short term benefit of doing so and does not get penalized, others would follow and eventually things get out of control. What's worse, when you ask them why they don't follow, they might bounce back and ask you why their work cannot be valued because they save time by cutting corners yet still get the product features delivered, so they can have more time finding the next feature to develop.\n\nThe thing is, at the very beginning, there's no so-called best practices. However every kind of freedom has a price. People gradually find that certain patterns lead to less bugs and cleaner, easier-to-maintain code. Or in other words, they \"rot\" more slowly than others. Even though all software eventually fails if they don't get maintained, at least they fail in a less hilarious way. Unfortunately a lot of developers nowadays tend to ignore that in favor of pragmatism mainly because unfortunately following best practices tends to not have immediate benefit - unless one has some kind of OCD.\n\nThere's various ways to tackle that in practice. Google, for example, strives to maintain a centralized standard of coding practices. It operates in a model somewhat like GPL license - you need to get the proof that you can write good quality code from people who have the proof. Then in the ideal world, that leads to a situation that people would operate on the same standard using the same language. This also works for small startups through code reviews when you only have that many engineers. This basically operates on a psychological level because when people violate the practices, you feel morally superior to them and you can criticize the violation publicly through code review, which in practice, leads to more people willing to follow the majority.\n\nEverything comes with trade-off though. This, however, in practice leads to slight efficiency penalty, because people may have to fix the \"minutes\" (e.g. switching to multithreading in a non critical code path because in theory you should and other people think so) instead of only focusing more on the higher, directional level (i.e. I can ship my stuff as long as it appears to be working at that moment). So product-oriented companies like Facebook, on the other hand, tend to give trust and some kind of freedom to engineers. As a result, different teams/orgs could have different standards and slightly different preferences. Amazon even pushes that level higher as each team/org owns separate repos. And when things become bad to certain degree, they carry out the big lawn mower ([fastmod](https://github.com/facebookincubator/fastmod) for example) to fix all similar issues in an atomic way (this is why being able to automating the elimination of dead code is important for big companies). Some people don't agree on this approach, but that way does have a valuable point, which is that enforcing individual behavior cannot scale together with the fast changing environment. Practicing large scale automation is more important than to train individuals to be sticking to \"best practices\" and do everything right - simply because nobody can't over time. That way, people spend more time \"solving real world problems\" than \"thinking about whether some piece of code could be refactored to be reused\". After all, what's the value of latter in performance evaluation if it's not measurable?\n\nUnfortunately not all people think that way. A lot of people, especially in open source software community, believe that before you become a pure problem solver, you are an engineer who writes shit. And every piece of shit could fall apart in a different way. The idea of best practice, is to enable other people to follow the logic of your shit more easily, require less context switching, and locate problems faster when things break. This is why people come up with design patterns. To problem solvers' point, they are useless, because they don't address a single real world problem, at least not directly. Some people would argue with them that they allow one to write shit with low coupling and high cohesion. From my perspective, I think the value of following such best practice, is to allow the shit to break, if eventually it will, in a less laughable way.\n\nThere are examples of such failure - not long ago, Facebook updated the server code which led to [massive app launch crash including spotify, tiktok and google](https://www.theverge.com/2020/5/7/21250689/facebook-sdk-bug-ios-app-crash-apple-spotify-venmo-tiktok-tinder) due to a \"simple\" type mismatch error ([this github issue](https://github.com/facebook/facebook-ios-sdk/issues/1373) reveals some technical details). People laughed at that, not because it's yet another critical bug, but it's an example to show the price for ignoring best practices, and hence software breaks in an interesting way. Some people claim that this requires better testing and type checking. IMHO, this is more of a high horse style solution because there's **always** stuff you'd mis-test in large scale software. Over time if there's no issues happening, people get used to and bored so issues would emerge again and similar situations start all over. The general industry best practice would tell us that the only way to fundamentally prevent such issues is to keep the interface in sync. There are various ways to achieve that with one being using an IDL to define such thing (thrift, protobuf, you name it). I'm not a mobile developer so I'm sure there's reasons/trade-offs behind why no such thing is being used in this case, even if some of which might not be purely technical. But regardless of that, the point is these practices are not new. They came from people who had failed in history - coding to interfaces is not a new idea, it's been there for decades in the OOP world. Nothing would fail immediately if one ignores those, but eventually one would join the other group and advocate for such thing.\n\n\nAll software decays. The idea of software engineering, at the end of the day, is not to prevent people from writing shit. However, if we keep trying to maintain a high standard of shit, or trying to get closer to it, then people would be more willing to accept the fact that things would eventually break. And when that happens, there would be more people willing to help it out, and this is how communities are formed, not just by end results, but more by reputation and expectation.\n\nAfter all, we should make each shit do one thing, and do it well.","slug":"Why-Keeping-High-Standard-of-Shit-Is-Important","published":1,"updated":"2025-09-01T22:26:51.292Z","comments":1,"layout":"post","photos":[],"_id":"cmf1pfvsf001e8mmg0o1a8t9c","content":"<p>We all write shitty code.</p>\n<p>This seems like a stupidly bold statement from a random person, especially when this person very likely writes shitter stuff than most others. After all, there’s a well known joke across the industry - that is, Jeff Dean does not write buggy code, and when he does, it’s the problem of the compiler. So at least we have one counter example.</p>\n<p>In common sense, masters don’t write shit, and Linus Torvalds very likely nods in the line.</p>\n<p>Well, technically speaking, before we criticize this non sense bold statement, we need to define what shit is. And here, I think shitty code is <strong>anything that does not live up to the expectation</strong>. Buggy code is shitty - because it doesn’t provide consistently correct results when people expect it to. However, in the real world, we tend to operate at a much higher level - we care more about contracts, functionalities, and behaviors, and less about the exact implementations. For example, when we use an API service, we tend to care about what features it can provide, the QPS it can support, and how easily we can use it - in essence, our expectation. We probably won’t care about whether it’s run on bare metal or some public cloud, whether the memory is manually managed, by compiler, or by garbage collection.</p>\n<p>Does that then mean all services/software that fail to match expectation start with shitty code? Well not necessarily. Windows 9x was great at the time it was first introduced and enabled a whole industry. Yet as things evolved, a lot of the design issues started to get exacerbated and it eventually got completely replaced by NT kernel. I’m not trying to defend 9x and saying it was very well designed masterpiece by any means - it had some notorious kernel safety issues after all, but just using it as an example to show that even if some software had highlights, over time other factors could expose the shittiness inside - it’s just a matter of time. What I believe is that no matter how well the systems were designed and implemented at the beginning, they deteriorate over time. We cannot prevent things from turning into shit - we can only slow down the speed.</p>\n<p>What makes things more complicated is that, often times, there are way too many factors that have impact on how we value software:</p>\n<span id=\"more\"></span>\n\n<p>At high level, users’ expectations may change, the overall world trend may evolve, or alternative software may leap forward.</p>\n<p>Under microscope, experienced maintainers may leave, tools/development environment may change, or simply resource priority may shift and the project may not be able to be taken care of for years.</p>\n<p>For example, in the cloudify trend, a lot of companies try to ditch Oracle databases in favor of AWS/Azure. E-commers turn to Salesforce rather than costly, heavy weight SAP systems. It’s most likely not the problem of the latter systems per se - in fact Oracle is still irreplaceable in certain areas if you are looking for stable, state-of-the-art single-machine db throughput for a traditional RDBMS. It’s more about expectation management - if you need to tighten a screw, you need a screwdriver, not a hammer, no matter how good the hammer is, because it won’t do the work you expect it to.</p>\n<p>With these said, I’m not trying to say that there’s very little engineers have control of. Quite the contrary, the point I’m trying to make here is, despite the fact that there are so many external factors that could accelerate the speed of software deterioration, there’s still something engineers have control of to slow that down, with all other things being equal, which is to try to maintain a high standard of shit.</p>\n<p>While this does seem very meta, the core ideas in practice, in which I believe, are continuous delivery and following best practices.</p>\n<h3 id=\"Continuous-Delivery\"><a href=\"#Continuous-Delivery\" class=\"headerlink\" title=\"Continuous Delivery\"></a>Continuous Delivery</h3><p>Continuous delivery allows for faster code iteration, earlier error detection and recovery, and more agile response to the ever changing requirements. This is a very broad topic in practice and easier said than done, yet it’s not a unsolvable problem. At the very basic layer, continuous delivery has 3 fundamental parts - a scalable VCS to allow people to continuously evolve the <strong>entire</strong> code base, a CI/CD platform to allow for reliable shipment and a mechanism to automatically disable dead/unmaintained code.</p>\n<p>There are a lot of good examples that exist in industry: Google and Facebook have put enormous effort into scaling the VCS platform to support their monolithic repos. This is not nonsense - while monolithic repos may not work for everyone, engineering wise, it’s the most cost-effective way at scale to save time handling dependency problems and force people to move forward together by atomic large scale code mods.</p>\n<p>Amazon, on the other hand, offers Apollo to employees to allow engineers to automate reliable shipment of software to their global fleet. All the automation, at the end of the day, is to eliminate software rot because of human error and laziness.</p>\n<p>The third part - eliminating dead/unmaintained software - is often overlooked and a lot of people/companies have stumped on this. The basic idea is simple - since all software deteriorates over time, the only unchanging thing is to make changes and the only way to avoid forgetting about something is to keep doing it. In practice, at the very minimum, this means setting up some deadline and after which the binary is not allowed to run, in order to force people to upgrade, even if no logic is changed. This is hard indeed in reality because often times when you build infrastructure level software for a global hardware fleet, it’s costly to update and update may lead to errors that could be prevented otherwise, let alone for commercial software, where there are business contracts that prevent stuff from being upgraded. So people sit on high horses and say if it ain’t break, don’t fix it. This is true in most cases. However then those people often are also the ones who fall harder when something crucial, that hasn’t been maintained for years, eventually breaks. In practice, this is less of a technical issue - it’s more about engineering culture, which essentially is how much time/money the company/team is willing to spend on things that do not have short term tangible value, yet valuable when team needs the strength in unexpected time.</p>\n<h3 id=\"Best-Practices\"><a href=\"#Best-Practices\" class=\"headerlink\" title=\"Best Practices\"></a>Best Practices</h3><p>The other part - following best practices - is also culture related. Often it’s hard to measure, and it’s tempting to violate when you are in a rush. A more alarming fact is, it’s often hard to keep that as a habit for a team and it takes a long time to build and maintain that culture, yet it’s very easy to crush as it’s viral - once someone shows the short term benefit of doing so and does not get penalized, others would follow and eventually things get out of control. What’s worse, when you ask them why they don’t follow, they might bounce back and ask you why their work cannot be valued because they save time by cutting corners yet still get the product features delivered, so they can have more time finding the next feature to develop.</p>\n<p>The thing is, at the very beginning, there’s no so-called best practices. However every kind of freedom has a price. People gradually find that certain patterns lead to less bugs and cleaner, easier-to-maintain code. Or in other words, they “rot” more slowly than others. Even though all software eventually fails if they don’t get maintained, at least they fail in a less hilarious way. Unfortunately a lot of developers nowadays tend to ignore that in favor of pragmatism mainly because unfortunately following best practices tends to not have immediate benefit - unless one has some kind of OCD.</p>\n<p>There’s various ways to tackle that in practice. Google, for example, strives to maintain a centralized standard of coding practices. It operates in a model somewhat like GPL license - you need to get the proof that you can write good quality code from people who have the proof. Then in the ideal world, that leads to a situation that people would operate on the same standard using the same language. This also works for small startups through code reviews when you only have that many engineers. This basically operates on a psychological level because when people violate the practices, you feel morally superior to them and you can criticize the violation publicly through code review, which in practice, leads to more people willing to follow the majority.</p>\n<p>Everything comes with trade-off though. This, however, in practice leads to slight efficiency penalty, because people may have to fix the “minutes” (e.g. switching to multithreading in a non critical code path because in theory you should and other people think so) instead of only focusing more on the higher, directional level (i.e. I can ship my stuff as long as it appears to be working at that moment). So product-oriented companies like Facebook, on the other hand, tend to give trust and some kind of freedom to engineers. As a result, different teams/orgs could have different standards and slightly different preferences. Amazon even pushes that level higher as each team/org owns separate repos. And when things become bad to certain degree, they carry out the big lawn mower (<a href=\"https://github.com/facebookincubator/fastmod\">fastmod</a> for example) to fix all similar issues in an atomic way (this is why being able to automating the elimination of dead code is important for big companies). Some people don’t agree on this approach, but that way does have a valuable point, which is that enforcing individual behavior cannot scale together with the fast changing environment. Practicing large scale automation is more important than to train individuals to be sticking to “best practices” and do everything right - simply because nobody can’t over time. That way, people spend more time “solving real world problems” than “thinking about whether some piece of code could be refactored to be reused”. After all, what’s the value of latter in performance evaluation if it’s not measurable?</p>\n<p>Unfortunately not all people think that way. A lot of people, especially in open source software community, believe that before you become a pure problem solver, you are an engineer who writes shit. And every piece of shit could fall apart in a different way. The idea of best practice, is to enable other people to follow the logic of your shit more easily, require less context switching, and locate problems faster when things break. This is why people come up with design patterns. To problem solvers’ point, they are useless, because they don’t address a single real world problem, at least not directly. Some people would argue with them that they allow one to write shit with low coupling and high cohesion. From my perspective, I think the value of following such best practice, is to allow the shit to break, if eventually it will, in a less laughable way.</p>\n<p>There are examples of such failure - not long ago, Facebook updated the server code which led to <a href=\"https://www.theverge.com/2020/5/7/21250689/facebook-sdk-bug-ios-app-crash-apple-spotify-venmo-tiktok-tinder\">massive app launch crash including spotify, tiktok and google</a> due to a “simple” type mismatch error (<a href=\"https://github.com/facebook/facebook-ios-sdk/issues/1373\">this github issue</a> reveals some technical details). People laughed at that, not because it’s yet another critical bug, but it’s an example to show the price for ignoring best practices, and hence software breaks in an interesting way. Some people claim that this requires better testing and type checking. IMHO, this is more of a high horse style solution because there’s <strong>always</strong> stuff you’d mis-test in large scale software. Over time if there’s no issues happening, people get used to and bored so issues would emerge again and similar situations start all over. The general industry best practice would tell us that the only way to fundamentally prevent such issues is to keep the interface in sync. There are various ways to achieve that with one being using an IDL to define such thing (thrift, protobuf, you name it). I’m not a mobile developer so I’m sure there’s reasons/trade-offs behind why no such thing is being used in this case, even if some of which might not be purely technical. But regardless of that, the point is these practices are not new. They came from people who had failed in history - coding to interfaces is not a new idea, it’s been there for decades in the OOP world. Nothing would fail immediately if one ignores those, but eventually one would join the other group and advocate for such thing.</p>\n<p>All software decays. The idea of software engineering, at the end of the day, is not to prevent people from writing shit. However, if we keep trying to maintain a high standard of shit, or trying to get closer to it, then people would be more willing to accept the fact that things would eventually break. And when that happens, there would be more people willing to help it out, and this is how communities are formed, not just by end results, but more by reputation and expectation.</p>\n<p>After all, we should make each shit do one thing, and do it well.</p>\n","thumbnailImageUrl":null,"excerpt":"<p>We all write shitty code.</p>\n<p>This seems like a stupidly bold statement from a random person, especially when this person very likely writes shitter stuff than most others. After all, there’s a well known joke across the industry - that is, Jeff Dean does not write buggy code, and when he does, it’s the problem of the compiler. So at least we have one counter example.</p>\n<p>In common sense, masters don’t write shit, and Linus Torvalds very likely nods in the line.</p>\n<p>Well, technically speaking, before we criticize this non sense bold statement, we need to define what shit is. And here, I think shitty code is <strong>anything that does not live up to the expectation</strong>. Buggy code is shitty - because it doesn’t provide consistently correct results when people expect it to. However, in the real world, we tend to operate at a much higher level - we care more about contracts, functionalities, and behaviors, and less about the exact implementations. For example, when we use an API service, we tend to care about what features it can provide, the QPS it can support, and how easily we can use it - in essence, our expectation. We probably won’t care about whether it’s run on bare metal or some public cloud, whether the memory is manually managed, by compiler, or by garbage collection.</p>\n<p>Does that then mean all services/software that fail to match expectation start with shitty code? Well not necessarily. Windows 9x was great at the time it was first introduced and enabled a whole industry. Yet as things evolved, a lot of the design issues started to get exacerbated and it eventually got completely replaced by NT kernel. I’m not trying to defend 9x and saying it was very well designed masterpiece by any means - it had some notorious kernel safety issues after all, but just using it as an example to show that even if some software had highlights, over time other factors could expose the shittiness inside - it’s just a matter of time. What I believe is that no matter how well the systems were designed and implemented at the beginning, they deteriorate over time. We cannot prevent things from turning into shit - we can only slow down the speed.</p>\n<p>What makes things more complicated is that, often times, there are way too many factors that have impact on how we value software:</p>","more":"<p>At high level, users’ expectations may change, the overall world trend may evolve, or alternative software may leap forward.</p>\n<p>Under microscope, experienced maintainers may leave, tools/development environment may change, or simply resource priority may shift and the project may not be able to be taken care of for years.</p>\n<p>For example, in the cloudify trend, a lot of companies try to ditch Oracle databases in favor of AWS/Azure. E-commers turn to Salesforce rather than costly, heavy weight SAP systems. It’s most likely not the problem of the latter systems per se - in fact Oracle is still irreplaceable in certain areas if you are looking for stable, state-of-the-art single-machine db throughput for a traditional RDBMS. It’s more about expectation management - if you need to tighten a screw, you need a screwdriver, not a hammer, no matter how good the hammer is, because it won’t do the work you expect it to.</p>\n<p>With these said, I’m not trying to say that there’s very little engineers have control of. Quite the contrary, the point I’m trying to make here is, despite the fact that there are so many external factors that could accelerate the speed of software deterioration, there’s still something engineers have control of to slow that down, with all other things being equal, which is to try to maintain a high standard of shit.</p>\n<p>While this does seem very meta, the core ideas in practice, in which I believe, are continuous delivery and following best practices.</p>\n<h3 id=\"Continuous-Delivery\"><a href=\"#Continuous-Delivery\" class=\"headerlink\" title=\"Continuous Delivery\"></a>Continuous Delivery</h3><p>Continuous delivery allows for faster code iteration, earlier error detection and recovery, and more agile response to the ever changing requirements. This is a very broad topic in practice and easier said than done, yet it’s not a unsolvable problem. At the very basic layer, continuous delivery has 3 fundamental parts - a scalable VCS to allow people to continuously evolve the <strong>entire</strong> code base, a CI/CD platform to allow for reliable shipment and a mechanism to automatically disable dead/unmaintained code.</p>\n<p>There are a lot of good examples that exist in industry: Google and Facebook have put enormous effort into scaling the VCS platform to support their monolithic repos. This is not nonsense - while monolithic repos may not work for everyone, engineering wise, it’s the most cost-effective way at scale to save time handling dependency problems and force people to move forward together by atomic large scale code mods.</p>\n<p>Amazon, on the other hand, offers Apollo to employees to allow engineers to automate reliable shipment of software to their global fleet. All the automation, at the end of the day, is to eliminate software rot because of human error and laziness.</p>\n<p>The third part - eliminating dead/unmaintained software - is often overlooked and a lot of people/companies have stumped on this. The basic idea is simple - since all software deteriorates over time, the only unchanging thing is to make changes and the only way to avoid forgetting about something is to keep doing it. In practice, at the very minimum, this means setting up some deadline and after which the binary is not allowed to run, in order to force people to upgrade, even if no logic is changed. This is hard indeed in reality because often times when you build infrastructure level software for a global hardware fleet, it’s costly to update and update may lead to errors that could be prevented otherwise, let alone for commercial software, where there are business contracts that prevent stuff from being upgraded. So people sit on high horses and say if it ain’t break, don’t fix it. This is true in most cases. However then those people often are also the ones who fall harder when something crucial, that hasn’t been maintained for years, eventually breaks. In practice, this is less of a technical issue - it’s more about engineering culture, which essentially is how much time/money the company/team is willing to spend on things that do not have short term tangible value, yet valuable when team needs the strength in unexpected time.</p>\n<h3 id=\"Best-Practices\"><a href=\"#Best-Practices\" class=\"headerlink\" title=\"Best Practices\"></a>Best Practices</h3><p>The other part - following best practices - is also culture related. Often it’s hard to measure, and it’s tempting to violate when you are in a rush. A more alarming fact is, it’s often hard to keep that as a habit for a team and it takes a long time to build and maintain that culture, yet it’s very easy to crush as it’s viral - once someone shows the short term benefit of doing so and does not get penalized, others would follow and eventually things get out of control. What’s worse, when you ask them why they don’t follow, they might bounce back and ask you why their work cannot be valued because they save time by cutting corners yet still get the product features delivered, so they can have more time finding the next feature to develop.</p>\n<p>The thing is, at the very beginning, there’s no so-called best practices. However every kind of freedom has a price. People gradually find that certain patterns lead to less bugs and cleaner, easier-to-maintain code. Or in other words, they “rot” more slowly than others. Even though all software eventually fails if they don’t get maintained, at least they fail in a less hilarious way. Unfortunately a lot of developers nowadays tend to ignore that in favor of pragmatism mainly because unfortunately following best practices tends to not have immediate benefit - unless one has some kind of OCD.</p>\n<p>There’s various ways to tackle that in practice. Google, for example, strives to maintain a centralized standard of coding practices. It operates in a model somewhat like GPL license - you need to get the proof that you can write good quality code from people who have the proof. Then in the ideal world, that leads to a situation that people would operate on the same standard using the same language. This also works for small startups through code reviews when you only have that many engineers. This basically operates on a psychological level because when people violate the practices, you feel morally superior to them and you can criticize the violation publicly through code review, which in practice, leads to more people willing to follow the majority.</p>\n<p>Everything comes with trade-off though. This, however, in practice leads to slight efficiency penalty, because people may have to fix the “minutes” (e.g. switching to multithreading in a non critical code path because in theory you should and other people think so) instead of only focusing more on the higher, directional level (i.e. I can ship my stuff as long as it appears to be working at that moment). So product-oriented companies like Facebook, on the other hand, tend to give trust and some kind of freedom to engineers. As a result, different teams/orgs could have different standards and slightly different preferences. Amazon even pushes that level higher as each team/org owns separate repos. And when things become bad to certain degree, they carry out the big lawn mower (<a href=\"https://github.com/facebookincubator/fastmod\">fastmod</a> for example) to fix all similar issues in an atomic way (this is why being able to automating the elimination of dead code is important for big companies). Some people don’t agree on this approach, but that way does have a valuable point, which is that enforcing individual behavior cannot scale together with the fast changing environment. Practicing large scale automation is more important than to train individuals to be sticking to “best practices” and do everything right - simply because nobody can’t over time. That way, people spend more time “solving real world problems” than “thinking about whether some piece of code could be refactored to be reused”. After all, what’s the value of latter in performance evaluation if it’s not measurable?</p>\n<p>Unfortunately not all people think that way. A lot of people, especially in open source software community, believe that before you become a pure problem solver, you are an engineer who writes shit. And every piece of shit could fall apart in a different way. The idea of best practice, is to enable other people to follow the logic of your shit more easily, require less context switching, and locate problems faster when things break. This is why people come up with design patterns. To problem solvers’ point, they are useless, because they don’t address a single real world problem, at least not directly. Some people would argue with them that they allow one to write shit with low coupling and high cohesion. From my perspective, I think the value of following such best practice, is to allow the shit to break, if eventually it will, in a less laughable way.</p>\n<p>There are examples of such failure - not long ago, Facebook updated the server code which led to <a href=\"https://www.theverge.com/2020/5/7/21250689/facebook-sdk-bug-ios-app-crash-apple-spotify-venmo-tiktok-tinder\">massive app launch crash including spotify, tiktok and google</a> due to a “simple” type mismatch error (<a href=\"https://github.com/facebook/facebook-ios-sdk/issues/1373\">this github issue</a> reveals some technical details). People laughed at that, not because it’s yet another critical bug, but it’s an example to show the price for ignoring best practices, and hence software breaks in an interesting way. Some people claim that this requires better testing and type checking. IMHO, this is more of a high horse style solution because there’s <strong>always</strong> stuff you’d mis-test in large scale software. Over time if there’s no issues happening, people get used to and bored so issues would emerge again and similar situations start all over. The general industry best practice would tell us that the only way to fundamentally prevent such issues is to keep the interface in sync. There are various ways to achieve that with one being using an IDL to define such thing (thrift, protobuf, you name it). I’m not a mobile developer so I’m sure there’s reasons/trade-offs behind why no such thing is being used in this case, even if some of which might not be purely technical. But regardless of that, the point is these practices are not new. They came from people who had failed in history - coding to interfaces is not a new idea, it’s been there for decades in the OOP world. Nothing would fail immediately if one ignores those, but eventually one would join the other group and advocate for such thing.</p>\n<p>All software decays. The idea of software engineering, at the end of the day, is not to prevent people from writing shit. However, if we keep trying to maintain a high standard of shit, or trying to get closer to it, then people would be more willing to accept the fact that things would eventually break. And when that happens, there would be more people willing to help it out, and this is how communities are formed, not just by end results, but more by reputation and expectation.</p>\n<p>After all, we should make each shit do one thing, and do it well.</p>"},{"title":"Workflow Processing Engine Overview 2018: Airflow vs Azkaban vs Conductor vs Oozie vs Amazon Step Functions","date":"2018-04-14T04:33:33.000Z","_content":"|                                 | Airflow                                | Azkaban                                         | Conductor                      | Oozie                                      | Step Functions                      |\n|:-------------------------------:|----------------------------------------|-------------------------------------------------|--------------------------------|--------------------------------------------|-------------------------------------|\n|            **Owner**            | Apache<br>(previously Airbnb)          | LinkedIn                                        | Netflix                        | Apache                                     | Amazon                              |\n|          **Community**          | Very Active                            | Somewhat active                                 | Active                         | Active                                     | N/A                                 |\n|           **History**           | 4 years                                | 7 years                                         | 1.5 years                      | 8 years                                    | 1.5 years                           |\n|         **Main Purpose**        | General Purpose Batch Processing       | Hadoop Job Scheduling                           | Microservice orchestration     | Hadoop Job Scheduling                      | General Purpose Workflow Processing |\n|       **Flow Definition**       | Python                                 | Custom DSL                                      | JSON                           | XML                                        | JSON                                |\n|   **Support for single node**   | Yes                                    | Yes                                             | Yes                            | Yes                                        | N/A                                 |\n|       **Quick demo setup**      | Yes                                    | Yes                                             | Yes                            | No                                         | N/A                                 |\n|        **Support for HA**       | Yes                                    | Yes                                             | Yes                            | Yes                                        | Yes                                 |\n|   **Single Point of Failure**   | Yes<br>(Single scheduler)              | Yes<br>(Single web and scheduler combined node) | No                             | No                                         | No                                  |\n|     **HA Extra Requirement**    | Celery/Dask/Mesos + Load Balancer + DB | DB                                              | Load Balancer (web nodes) + DB | Load Balancer (web nodes) + DB + Zookeeper | Native                              |\n|           **Cron Job**          | Yes                                    | Yes                                             | No                             | Yes                                        | Yes                                 |\n|       **Execution Model**       | Push                                   | Push                                            | Poll                           | Poll                                       | Unknown                             |\n|       **Rest API Trigger**      | Yes                                    | Yes                                             | Yes                            | Yes                                        | Yes                                 |\n|   **Parameterized Execution**   | Yes                                    | Yes                                             | Yes                            | Yes                                        | Yes                                 |\n|  **Trigger by External Event**  | Yes                                    | No                                              | No                             | Yes                                        | Yes                                 |\n| **Native Waiting Task Support** | Yes                                    | No                                              | Yes (external signal required) | No                                         | Yes                                 |\n|     **Backfilling support**     | Yes                                    | No                                              | No                             | Yes                                        | No                                  |\n|  **Native Web Authentication**  | LDAP/Password                          | XML Password                                    | No                             | Kerberos                                   | N/A (AWS login)                 |\n|          **Monitoring**         | Yes                                    | Limited                                         | Limited                        | Yes                                        | Limited                             |\n|         **Scalability**         | Depending on executor setup            | Good                                            | Very Good                      | Very Good                                  | Very Good                           |\n\n## Update\n\n- *(2018.11) Oozie has Kerberos auth over SPNEGO for web (thanks to Justin Miller for pointing it out)*\n\n## Disclaimer\nI'm not an expert in any of those engines.\nI've used some of those (Airflow & Azkaban) and checked the code.\nFor some others I either only read the code (Conductor) or the docs (Oozie/AWS Step Functions).\nAs most of them are OSS projects, it's certainly possible that I might have missed certain undocumented features,\nor community-contributed plugins. I'm happy to update this if you see anything wrong.\n\nBottom line: Use your own judgement when reading this post.\n\n## Airflow\n\n### The Good\nAirflow is a super feature rich engine compared to all other solutions.\nNot only you can use plugins to support all kinds of jobs,\nranging from data processing jobs: Hive, Pig (though you can also submit them via shell command),\nto general flow management like triggering by existence of file/db entry/s3 content,\nor waiting for expected output from a web endpoint,\nbut also it provides a nice UI that allows you to check your DAGs (workflow dependencies) through code/graph,\nand monitors the real time execution of jobs.\n\nAirflow is also highly customizable with a currently vigorous community.\nYou can run all your jobs through a single node using local executor,\nor distribute them onto a group of worker nodes through Celery/Dask/Mesos orchestration.\n\n### The Bad\nAirflow by itself is still not very mature (in fact maybe Oozie is the only \"mature\" engine here).\nThe scheduler would need to periodically poll the scheduling plan and send jobs to executors.\nThis means it along would continuously dump enormous amount of logs out of the box.\nAs it works by \"ticking\", your jobs are not guaranteed to get scheduled in \"real-time\" if that makes sense\nand this would get worse as the number of concurrent jobs increases.\nMeanwhile as you have one centralized scheduler, if it goes down or gets stuck, your running jobs won't be\naffected as that the job of executors, but no new jobs will get scheduled. This is especially confusing when\nyou run this with a HA setup where you have multiple web nodes, a scheduler, a broker\n(typically a message queue in Celery case), multiple executors. When scheduler is stuck for whatever reason,\nall you see in web UI is all tasks are running, but in fact they are not actually moving forward while executors\nare happily reporting they are fine. In other words, the default monitoring is still far from bullet proof.\n\nThe web UI is very nice from the first look. However it sometimes is confusing to new users.\nWhat does it mean my DAG runs are \"running\" but my tasks have no state? The charts are not search friendly either,\nlet alone some of the features are still far from well documented\n(though the document does look nice, I mean, compared to Oozie, which does seem out-dated).\n\nThe backfilling design is good in certain cases but very error prone in others.\nIf you have a flow with cron schedules disabled and re-enabled later, it would try to play catch up,\nand if your jobs is not designed to be idempotent, shit would happen for real.\n\n## Azkaban\n\n### The Good\nOf all the engines, Azkaban is probably the easiest to get going out of the box.\nUI is very intuitive and easy to use. Scheduling and REST APIs works just fine.\n\nLimited HA setup works out of the box.\nThere's no need for load balancer because you can only have one web node.\nYou can configure how it selects executor nodes to push jobs to and it generally seems to scale pretty nicely.\nYou can easily run tens of thousands of jobs as long as you have enough capacity for the executor nodes.\n\n### The Bad\nIt is not very feature rich out of the box as a general purpose orchestration engine,\nbut likely that's not what's originally designed for. It's strength lies in native support for Hadoop/Pig/Hive,\nthough you can also achieve those using command line. But itself cannot trigger jobs through external resources like\nAirflow, nor does it support job waiting pattern. Although you can do busy waiting through java code/scripts, that\nleads to bad resource utilization.\n\nThe documentation and configuration are generally a bit confusing compared to others. It's likely that it wasn't supposed\nto be OSed at the beginning. The design is okish but you better have a big data center to run the executors as scheduling \nwould get stalled when executors run out of resources without extra monitoring stuff. The code quality overall is a bit towards\nthe lower end compared to others so it generally only scales well when resource is not a problem.\n\nThe setup/design is not cloud friendly. You are pretty much supposed to have stable bare metal rather than dynamically\nallocated virtual instances with dynamic IPs. Scheduling would go south if machines vanish.\n\nThe monitoring part is sort of acceptable through JMX (does not seem documented). But it generally doesn't work well if your\nmachines are heavily loaded, unfortunately, as the endpoints may get stuck.\n\n## Conductor\n\n### The Good\nIt's a bit unfair to put Conductor into this competition as it's real purpose is for microservice orchestration, whatever that means.\nIt's HA model involves a quorum of servers sitting behind load balancer putting tasks onto a message queue which the worker nodes would\npoll from, which means it's less likely you'll run into stalled scheduling.\nWith the help of parameterized execution through API, it's actually quite good at scheduling and scaling provided\nthat you set up your load balancer/service discovery layer properly.\n\n### The Bad\nThe UI needs a bit more love. There's currently very limited monitoring there. Although for general purpose scheduling that's probably\ngood enough.\n\nIt's pretty bare-bone out of the box. There's not even native support for running shell scripts, though it's pretty easy to implement\na task worker through python to do the job with the examples provided.\n\n## Oozie\n\n### The Good\nOozie provides a seemingly reliable HA model through the db setup (seemingly b/c I've not dug into it).\nIt provides native support for Hadoop related jobs as it was sort of built for that eco system.\n\n### The Bad\nNot a very good candidate for general purpose flow scheduling as the XML definition is quite verbose\nand cumbersome for defining light weight jobs.\n\nIt also requires quite a bit of peripheral setup. You need a zookeeper cluster, a db, a load balancer\nand each node needs to run a web app container like Tomcat. The initial setup also takes some time which is\nnot friendly to first time users to pilot stuff.\n\n## Step Functions\n\n### The Good\nStep Functions is fairly new (launch in Dec 2016). However the future seems promising. With the HA nature of cloud\nplatform and lambda functions, it almost feels like it can easily scale infinitely (compared to others).\n\nIt also offers some useful features for general purpose workflow handling like waiting support and dynamic branching\nbased on output.\n\nIt's also fairly cheap:\n\n- 4,000 state transitions are free each month\n- $0.025 per 1,000 state transitions thereafter ($0.000025 per state transition)\n\nIf you don't run tens of thousands of jobs, this might be even better than running your own cluster of things.\n\n### The Bad\nCan only be used by AWS users. Deal breaker if you are not one of them yet.\n\nLambda requires extra work for production level iteration/deployment.\n\nThere's no UI (well there is but it's really just a console).\nSo if you need any level of monitoring beyond that you need to build it using cloudwatch by yourself.\n","source":"_posts/Workflow-Processing-Engine-Overview-2018-Airflow-vs-Azkaban-vs-Conductor-vs-Oozie-vs-Amazon-Step-Functions.md","raw":"---\ntitle: \"Workflow Processing Engine Overview 2018: Airflow vs Azkaban vs Conductor vs Oozie vs Amazon Step Functions\"\ndate: 2018-04-13 21:33:33\ntags:\n    - opensource\n    - workflow\n    - aws\n    - airflow\n    - azkaban\n    - review\n---\n|                                 | Airflow                                | Azkaban                                         | Conductor                      | Oozie                                      | Step Functions                      |\n|:-------------------------------:|----------------------------------------|-------------------------------------------------|--------------------------------|--------------------------------------------|-------------------------------------|\n|            **Owner**            | Apache<br>(previously Airbnb)          | LinkedIn                                        | Netflix                        | Apache                                     | Amazon                              |\n|          **Community**          | Very Active                            | Somewhat active                                 | Active                         | Active                                     | N/A                                 |\n|           **History**           | 4 years                                | 7 years                                         | 1.5 years                      | 8 years                                    | 1.5 years                           |\n|         **Main Purpose**        | General Purpose Batch Processing       | Hadoop Job Scheduling                           | Microservice orchestration     | Hadoop Job Scheduling                      | General Purpose Workflow Processing |\n|       **Flow Definition**       | Python                                 | Custom DSL                                      | JSON                           | XML                                        | JSON                                |\n|   **Support for single node**   | Yes                                    | Yes                                             | Yes                            | Yes                                        | N/A                                 |\n|       **Quick demo setup**      | Yes                                    | Yes                                             | Yes                            | No                                         | N/A                                 |\n|        **Support for HA**       | Yes                                    | Yes                                             | Yes                            | Yes                                        | Yes                                 |\n|   **Single Point of Failure**   | Yes<br>(Single scheduler)              | Yes<br>(Single web and scheduler combined node) | No                             | No                                         | No                                  |\n|     **HA Extra Requirement**    | Celery/Dask/Mesos + Load Balancer + DB | DB                                              | Load Balancer (web nodes) + DB | Load Balancer (web nodes) + DB + Zookeeper | Native                              |\n|           **Cron Job**          | Yes                                    | Yes                                             | No                             | Yes                                        | Yes                                 |\n|       **Execution Model**       | Push                                   | Push                                            | Poll                           | Poll                                       | Unknown                             |\n|       **Rest API Trigger**      | Yes                                    | Yes                                             | Yes                            | Yes                                        | Yes                                 |\n|   **Parameterized Execution**   | Yes                                    | Yes                                             | Yes                            | Yes                                        | Yes                                 |\n|  **Trigger by External Event**  | Yes                                    | No                                              | No                             | Yes                                        | Yes                                 |\n| **Native Waiting Task Support** | Yes                                    | No                                              | Yes (external signal required) | No                                         | Yes                                 |\n|     **Backfilling support**     | Yes                                    | No                                              | No                             | Yes                                        | No                                  |\n|  **Native Web Authentication**  | LDAP/Password                          | XML Password                                    | No                             | Kerberos                                   | N/A (AWS login)                 |\n|          **Monitoring**         | Yes                                    | Limited                                         | Limited                        | Yes                                        | Limited                             |\n|         **Scalability**         | Depending on executor setup            | Good                                            | Very Good                      | Very Good                                  | Very Good                           |\n\n## Update\n\n- *(2018.11) Oozie has Kerberos auth over SPNEGO for web (thanks to Justin Miller for pointing it out)*\n\n## Disclaimer\nI'm not an expert in any of those engines.\nI've used some of those (Airflow & Azkaban) and checked the code.\nFor some others I either only read the code (Conductor) or the docs (Oozie/AWS Step Functions).\nAs most of them are OSS projects, it's certainly possible that I might have missed certain undocumented features,\nor community-contributed plugins. I'm happy to update this if you see anything wrong.\n\nBottom line: Use your own judgement when reading this post.\n\n## Airflow\n\n### The Good\nAirflow is a super feature rich engine compared to all other solutions.\nNot only you can use plugins to support all kinds of jobs,\nranging from data processing jobs: Hive, Pig (though you can also submit them via shell command),\nto general flow management like triggering by existence of file/db entry/s3 content,\nor waiting for expected output from a web endpoint,\nbut also it provides a nice UI that allows you to check your DAGs (workflow dependencies) through code/graph,\nand monitors the real time execution of jobs.\n\nAirflow is also highly customizable with a currently vigorous community.\nYou can run all your jobs through a single node using local executor,\nor distribute them onto a group of worker nodes through Celery/Dask/Mesos orchestration.\n\n### The Bad\nAirflow by itself is still not very mature (in fact maybe Oozie is the only \"mature\" engine here).\nThe scheduler would need to periodically poll the scheduling plan and send jobs to executors.\nThis means it along would continuously dump enormous amount of logs out of the box.\nAs it works by \"ticking\", your jobs are not guaranteed to get scheduled in \"real-time\" if that makes sense\nand this would get worse as the number of concurrent jobs increases.\nMeanwhile as you have one centralized scheduler, if it goes down or gets stuck, your running jobs won't be\naffected as that the job of executors, but no new jobs will get scheduled. This is especially confusing when\nyou run this with a HA setup where you have multiple web nodes, a scheduler, a broker\n(typically a message queue in Celery case), multiple executors. When scheduler is stuck for whatever reason,\nall you see in web UI is all tasks are running, but in fact they are not actually moving forward while executors\nare happily reporting they are fine. In other words, the default monitoring is still far from bullet proof.\n\nThe web UI is very nice from the first look. However it sometimes is confusing to new users.\nWhat does it mean my DAG runs are \"running\" but my tasks have no state? The charts are not search friendly either,\nlet alone some of the features are still far from well documented\n(though the document does look nice, I mean, compared to Oozie, which does seem out-dated).\n\nThe backfilling design is good in certain cases but very error prone in others.\nIf you have a flow with cron schedules disabled and re-enabled later, it would try to play catch up,\nand if your jobs is not designed to be idempotent, shit would happen for real.\n\n## Azkaban\n\n### The Good\nOf all the engines, Azkaban is probably the easiest to get going out of the box.\nUI is very intuitive and easy to use. Scheduling and REST APIs works just fine.\n\nLimited HA setup works out of the box.\nThere's no need for load balancer because you can only have one web node.\nYou can configure how it selects executor nodes to push jobs to and it generally seems to scale pretty nicely.\nYou can easily run tens of thousands of jobs as long as you have enough capacity for the executor nodes.\n\n### The Bad\nIt is not very feature rich out of the box as a general purpose orchestration engine,\nbut likely that's not what's originally designed for. It's strength lies in native support for Hadoop/Pig/Hive,\nthough you can also achieve those using command line. But itself cannot trigger jobs through external resources like\nAirflow, nor does it support job waiting pattern. Although you can do busy waiting through java code/scripts, that\nleads to bad resource utilization.\n\nThe documentation and configuration are generally a bit confusing compared to others. It's likely that it wasn't supposed\nto be OSed at the beginning. The design is okish but you better have a big data center to run the executors as scheduling \nwould get stalled when executors run out of resources without extra monitoring stuff. The code quality overall is a bit towards\nthe lower end compared to others so it generally only scales well when resource is not a problem.\n\nThe setup/design is not cloud friendly. You are pretty much supposed to have stable bare metal rather than dynamically\nallocated virtual instances with dynamic IPs. Scheduling would go south if machines vanish.\n\nThe monitoring part is sort of acceptable through JMX (does not seem documented). But it generally doesn't work well if your\nmachines are heavily loaded, unfortunately, as the endpoints may get stuck.\n\n## Conductor\n\n### The Good\nIt's a bit unfair to put Conductor into this competition as it's real purpose is for microservice orchestration, whatever that means.\nIt's HA model involves a quorum of servers sitting behind load balancer putting tasks onto a message queue which the worker nodes would\npoll from, which means it's less likely you'll run into stalled scheduling.\nWith the help of parameterized execution through API, it's actually quite good at scheduling and scaling provided\nthat you set up your load balancer/service discovery layer properly.\n\n### The Bad\nThe UI needs a bit more love. There's currently very limited monitoring there. Although for general purpose scheduling that's probably\ngood enough.\n\nIt's pretty bare-bone out of the box. There's not even native support for running shell scripts, though it's pretty easy to implement\na task worker through python to do the job with the examples provided.\n\n## Oozie\n\n### The Good\nOozie provides a seemingly reliable HA model through the db setup (seemingly b/c I've not dug into it).\nIt provides native support for Hadoop related jobs as it was sort of built for that eco system.\n\n### The Bad\nNot a very good candidate for general purpose flow scheduling as the XML definition is quite verbose\nand cumbersome for defining light weight jobs.\n\nIt also requires quite a bit of peripheral setup. You need a zookeeper cluster, a db, a load balancer\nand each node needs to run a web app container like Tomcat. The initial setup also takes some time which is\nnot friendly to first time users to pilot stuff.\n\n## Step Functions\n\n### The Good\nStep Functions is fairly new (launch in Dec 2016). However the future seems promising. With the HA nature of cloud\nplatform and lambda functions, it almost feels like it can easily scale infinitely (compared to others).\n\nIt also offers some useful features for general purpose workflow handling like waiting support and dynamic branching\nbased on output.\n\nIt's also fairly cheap:\n\n- 4,000 state transitions are free each month\n- $0.025 per 1,000 state transitions thereafter ($0.000025 per state transition)\n\nIf you don't run tens of thousands of jobs, this might be even better than running your own cluster of things.\n\n### The Bad\nCan only be used by AWS users. Deal breaker if you are not one of them yet.\n\nLambda requires extra work for production level iteration/deployment.\n\nThere's no UI (well there is but it's really just a console).\nSo if you need any level of monitoring beyond that you need to build it using cloudwatch by yourself.\n","slug":"Workflow-Processing-Engine-Overview-2018-Airflow-vs-Azkaban-vs-Conductor-vs-Oozie-vs-Amazon-Step-Functions","published":1,"updated":"2025-09-01T22:26:51.293Z","comments":1,"layout":"post","photos":[],"_id":"cmf1pfvsf001f8mmgeb9ad2mk","content":"<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th>Airflow</th>\n<th>Azkaban</th>\n<th>Conductor</th>\n<th>Oozie</th>\n<th>Step Functions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"><strong>Owner</strong></td>\n<td>Apache<br>(previously Airbnb)</td>\n<td>LinkedIn</td>\n<td>Netflix</td>\n<td>Apache</td>\n<td>Amazon</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Community</strong></td>\n<td>Very Active</td>\n<td>Somewhat active</td>\n<td>Active</td>\n<td>Active</td>\n<td>N/A</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>History</strong></td>\n<td>4 years</td>\n<td>7 years</td>\n<td>1.5 years</td>\n<td>8 years</td>\n<td>1.5 years</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Main Purpose</strong></td>\n<td>General Purpose Batch Processing</td>\n<td>Hadoop Job Scheduling</td>\n<td>Microservice orchestration</td>\n<td>Hadoop Job Scheduling</td>\n<td>General Purpose Workflow Processing</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Flow Definition</strong></td>\n<td>Python</td>\n<td>Custom DSL</td>\n<td>JSON</td>\n<td>XML</td>\n<td>JSON</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Support for single node</strong></td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>N/A</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Quick demo setup</strong></td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>No</td>\n<td>N/A</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Support for HA</strong></td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Single Point of Failure</strong></td>\n<td>Yes<br>(Single scheduler)</td>\n<td>Yes<br>(Single web and scheduler combined node)</td>\n<td>No</td>\n<td>No</td>\n<td>No</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>HA Extra Requirement</strong></td>\n<td>Celery/Dask/Mesos + Load Balancer + DB</td>\n<td>DB</td>\n<td>Load Balancer (web nodes) + DB</td>\n<td>Load Balancer (web nodes) + DB + Zookeeper</td>\n<td>Native</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Cron Job</strong></td>\n<td>Yes</td>\n<td>Yes</td>\n<td>No</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Execution Model</strong></td>\n<td>Push</td>\n<td>Push</td>\n<td>Poll</td>\n<td>Poll</td>\n<td>Unknown</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Rest API Trigger</strong></td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Parameterized Execution</strong></td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Trigger by External Event</strong></td>\n<td>Yes</td>\n<td>No</td>\n<td>No</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Native Waiting Task Support</strong></td>\n<td>Yes</td>\n<td>No</td>\n<td>Yes (external signal required)</td>\n<td>No</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Backfilling support</strong></td>\n<td>Yes</td>\n<td>No</td>\n<td>No</td>\n<td>Yes</td>\n<td>No</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Native Web Authentication</strong></td>\n<td>LDAP/Password</td>\n<td>XML Password</td>\n<td>No</td>\n<td>Kerberos</td>\n<td>N/A (AWS login)</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Monitoring</strong></td>\n<td>Yes</td>\n<td>Limited</td>\n<td>Limited</td>\n<td>Yes</td>\n<td>Limited</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Scalability</strong></td>\n<td>Depending on executor setup</td>\n<td>Good</td>\n<td>Very Good</td>\n<td>Very Good</td>\n<td>Very Good</td>\n</tr>\n</tbody></table>\n<h2 id=\"Update\"><a href=\"#Update\" class=\"headerlink\" title=\"Update\"></a>Update</h2><ul>\n<li><em>(2018.11) Oozie has Kerberos auth over SPNEGO for web (thanks to Justin Miller for pointing it out)</em></li>\n</ul>\n<h2 id=\"Disclaimer\"><a href=\"#Disclaimer\" class=\"headerlink\" title=\"Disclaimer\"></a>Disclaimer</h2><p>I’m not an expert in any of those engines.<br>I’ve used some of those (Airflow &amp; Azkaban) and checked the code.<br>For some others I either only read the code (Conductor) or the docs (Oozie/AWS Step Functions).<br>As most of them are OSS projects, it’s certainly possible that I might have missed certain undocumented features,<br>or community-contributed plugins. I’m happy to update this if you see anything wrong.</p>\n<p>Bottom line: Use your own judgement when reading this post.</p>\n<h2 id=\"Airflow\"><a href=\"#Airflow\" class=\"headerlink\" title=\"Airflow\"></a>Airflow</h2><h3 id=\"The-Good\"><a href=\"#The-Good\" class=\"headerlink\" title=\"The Good\"></a>The Good</h3><p>Airflow is a super feature rich engine compared to all other solutions.<br>Not only you can use plugins to support all kinds of jobs,<br>ranging from data processing jobs: Hive, Pig (though you can also submit them via shell command),<br>to general flow management like triggering by existence of file/db entry/s3 content,<br>or waiting for expected output from a web endpoint,<br>but also it provides a nice UI that allows you to check your DAGs (workflow dependencies) through code/graph,<br>and monitors the real time execution of jobs.</p>\n<p>Airflow is also highly customizable with a currently vigorous community.<br>You can run all your jobs through a single node using local executor,<br>or distribute them onto a group of worker nodes through Celery/Dask/Mesos orchestration.</p>\n<h3 id=\"The-Bad\"><a href=\"#The-Bad\" class=\"headerlink\" title=\"The Bad\"></a>The Bad</h3><p>Airflow by itself is still not very mature (in fact maybe Oozie is the only “mature” engine here).<br>The scheduler would need to periodically poll the scheduling plan and send jobs to executors.<br>This means it along would continuously dump enormous amount of logs out of the box.<br>As it works by “ticking”, your jobs are not guaranteed to get scheduled in “real-time” if that makes sense<br>and this would get worse as the number of concurrent jobs increases.<br>Meanwhile as you have one centralized scheduler, if it goes down or gets stuck, your running jobs won’t be<br>affected as that the job of executors, but no new jobs will get scheduled. This is especially confusing when<br>you run this with a HA setup where you have multiple web nodes, a scheduler, a broker<br>(typically a message queue in Celery case), multiple executors. When scheduler is stuck for whatever reason,<br>all you see in web UI is all tasks are running, but in fact they are not actually moving forward while executors<br>are happily reporting they are fine. In other words, the default monitoring is still far from bullet proof.</p>\n<p>The web UI is very nice from the first look. However it sometimes is confusing to new users.<br>What does it mean my DAG runs are “running” but my tasks have no state? The charts are not search friendly either,<br>let alone some of the features are still far from well documented<br>(though the document does look nice, I mean, compared to Oozie, which does seem out-dated).</p>\n<p>The backfilling design is good in certain cases but very error prone in others.<br>If you have a flow with cron schedules disabled and re-enabled later, it would try to play catch up,<br>and if your jobs is not designed to be idempotent, shit would happen for real.</p>\n<h2 id=\"Azkaban\"><a href=\"#Azkaban\" class=\"headerlink\" title=\"Azkaban\"></a>Azkaban</h2><h3 id=\"The-Good-1\"><a href=\"#The-Good-1\" class=\"headerlink\" title=\"The Good\"></a>The Good</h3><p>Of all the engines, Azkaban is probably the easiest to get going out of the box.<br>UI is very intuitive and easy to use. Scheduling and REST APIs works just fine.</p>\n<p>Limited HA setup works out of the box.<br>There’s no need for load balancer because you can only have one web node.<br>You can configure how it selects executor nodes to push jobs to and it generally seems to scale pretty nicely.<br>You can easily run tens of thousands of jobs as long as you have enough capacity for the executor nodes.</p>\n<h3 id=\"The-Bad-1\"><a href=\"#The-Bad-1\" class=\"headerlink\" title=\"The Bad\"></a>The Bad</h3><p>It is not very feature rich out of the box as a general purpose orchestration engine,<br>but likely that’s not what’s originally designed for. It’s strength lies in native support for Hadoop/Pig/Hive,<br>though you can also achieve those using command line. But itself cannot trigger jobs through external resources like<br>Airflow, nor does it support job waiting pattern. Although you can do busy waiting through java code/scripts, that<br>leads to bad resource utilization.</p>\n<p>The documentation and configuration are generally a bit confusing compared to others. It’s likely that it wasn’t supposed<br>to be OSed at the beginning. The design is okish but you better have a big data center to run the executors as scheduling<br>would get stalled when executors run out of resources without extra monitoring stuff. The code quality overall is a bit towards<br>the lower end compared to others so it generally only scales well when resource is not a problem.</p>\n<p>The setup/design is not cloud friendly. You are pretty much supposed to have stable bare metal rather than dynamically<br>allocated virtual instances with dynamic IPs. Scheduling would go south if machines vanish.</p>\n<p>The monitoring part is sort of acceptable through JMX (does not seem documented). But it generally doesn’t work well if your<br>machines are heavily loaded, unfortunately, as the endpoints may get stuck.</p>\n<h2 id=\"Conductor\"><a href=\"#Conductor\" class=\"headerlink\" title=\"Conductor\"></a>Conductor</h2><h3 id=\"The-Good-2\"><a href=\"#The-Good-2\" class=\"headerlink\" title=\"The Good\"></a>The Good</h3><p>It’s a bit unfair to put Conductor into this competition as it’s real purpose is for microservice orchestration, whatever that means.<br>It’s HA model involves a quorum of servers sitting behind load balancer putting tasks onto a message queue which the worker nodes would<br>poll from, which means it’s less likely you’ll run into stalled scheduling.<br>With the help of parameterized execution through API, it’s actually quite good at scheduling and scaling provided<br>that you set up your load balancer/service discovery layer properly.</p>\n<h3 id=\"The-Bad-2\"><a href=\"#The-Bad-2\" class=\"headerlink\" title=\"The Bad\"></a>The Bad</h3><p>The UI needs a bit more love. There’s currently very limited monitoring there. Although for general purpose scheduling that’s probably<br>good enough.</p>\n<p>It’s pretty bare-bone out of the box. There’s not even native support for running shell scripts, though it’s pretty easy to implement<br>a task worker through python to do the job with the examples provided.</p>\n<h2 id=\"Oozie\"><a href=\"#Oozie\" class=\"headerlink\" title=\"Oozie\"></a>Oozie</h2><h3 id=\"The-Good-3\"><a href=\"#The-Good-3\" class=\"headerlink\" title=\"The Good\"></a>The Good</h3><p>Oozie provides a seemingly reliable HA model through the db setup (seemingly b/c I’ve not dug into it).<br>It provides native support for Hadoop related jobs as it was sort of built for that eco system.</p>\n<h3 id=\"The-Bad-3\"><a href=\"#The-Bad-3\" class=\"headerlink\" title=\"The Bad\"></a>The Bad</h3><p>Not a very good candidate for general purpose flow scheduling as the XML definition is quite verbose<br>and cumbersome for defining light weight jobs.</p>\n<p>It also requires quite a bit of peripheral setup. You need a zookeeper cluster, a db, a load balancer<br>and each node needs to run a web app container like Tomcat. The initial setup also takes some time which is<br>not friendly to first time users to pilot stuff.</p>\n<h2 id=\"Step-Functions\"><a href=\"#Step-Functions\" class=\"headerlink\" title=\"Step Functions\"></a>Step Functions</h2><h3 id=\"The-Good-4\"><a href=\"#The-Good-4\" class=\"headerlink\" title=\"The Good\"></a>The Good</h3><p>Step Functions is fairly new (launch in Dec 2016). However the future seems promising. With the HA nature of cloud<br>platform and lambda functions, it almost feels like it can easily scale infinitely (compared to others).</p>\n<p>It also offers some useful features for general purpose workflow handling like waiting support and dynamic branching<br>based on output.</p>\n<p>It’s also fairly cheap:</p>\n<ul>\n<li>4,000 state transitions are free each month</li>\n<li>$0.025 per 1,000 state transitions thereafter ($0.000025 per state transition)</li>\n</ul>\n<p>If you don’t run tens of thousands of jobs, this might be even better than running your own cluster of things.</p>\n<h3 id=\"The-Bad-4\"><a href=\"#The-Bad-4\" class=\"headerlink\" title=\"The Bad\"></a>The Bad</h3><p>Can only be used by AWS users. Deal breaker if you are not one of them yet.</p>\n<p>Lambda requires extra work for production level iteration/deployment.</p>\n<p>There’s no UI (well there is but it’s really just a console).<br>So if you need any level of monitoring beyond that you need to build it using cloudwatch by yourself.</p>\n","thumbnailImageUrl":null,"excerpt":"","more":"<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th>Airflow</th>\n<th>Azkaban</th>\n<th>Conductor</th>\n<th>Oozie</th>\n<th>Step Functions</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"><strong>Owner</strong></td>\n<td>Apache<br>(previously Airbnb)</td>\n<td>LinkedIn</td>\n<td>Netflix</td>\n<td>Apache</td>\n<td>Amazon</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Community</strong></td>\n<td>Very Active</td>\n<td>Somewhat active</td>\n<td>Active</td>\n<td>Active</td>\n<td>N/A</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>History</strong></td>\n<td>4 years</td>\n<td>7 years</td>\n<td>1.5 years</td>\n<td>8 years</td>\n<td>1.5 years</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Main Purpose</strong></td>\n<td>General Purpose Batch Processing</td>\n<td>Hadoop Job Scheduling</td>\n<td>Microservice orchestration</td>\n<td>Hadoop Job Scheduling</td>\n<td>General Purpose Workflow Processing</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Flow Definition</strong></td>\n<td>Python</td>\n<td>Custom DSL</td>\n<td>JSON</td>\n<td>XML</td>\n<td>JSON</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Support for single node</strong></td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>N/A</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Quick demo setup</strong></td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>No</td>\n<td>N/A</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Support for HA</strong></td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Single Point of Failure</strong></td>\n<td>Yes<br>(Single scheduler)</td>\n<td>Yes<br>(Single web and scheduler combined node)</td>\n<td>No</td>\n<td>No</td>\n<td>No</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>HA Extra Requirement</strong></td>\n<td>Celery/Dask/Mesos + Load Balancer + DB</td>\n<td>DB</td>\n<td>Load Balancer (web nodes) + DB</td>\n<td>Load Balancer (web nodes) + DB + Zookeeper</td>\n<td>Native</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Cron Job</strong></td>\n<td>Yes</td>\n<td>Yes</td>\n<td>No</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Execution Model</strong></td>\n<td>Push</td>\n<td>Push</td>\n<td>Poll</td>\n<td>Poll</td>\n<td>Unknown</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Rest API Trigger</strong></td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Parameterized Execution</strong></td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Trigger by External Event</strong></td>\n<td>Yes</td>\n<td>No</td>\n<td>No</td>\n<td>Yes</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Native Waiting Task Support</strong></td>\n<td>Yes</td>\n<td>No</td>\n<td>Yes (external signal required)</td>\n<td>No</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Backfilling support</strong></td>\n<td>Yes</td>\n<td>No</td>\n<td>No</td>\n<td>Yes</td>\n<td>No</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Native Web Authentication</strong></td>\n<td>LDAP/Password</td>\n<td>XML Password</td>\n<td>No</td>\n<td>Kerberos</td>\n<td>N/A (AWS login)</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Monitoring</strong></td>\n<td>Yes</td>\n<td>Limited</td>\n<td>Limited</td>\n<td>Yes</td>\n<td>Limited</td>\n</tr>\n<tr>\n<td align=\"center\"><strong>Scalability</strong></td>\n<td>Depending on executor setup</td>\n<td>Good</td>\n<td>Very Good</td>\n<td>Very Good</td>\n<td>Very Good</td>\n</tr>\n</tbody></table>\n<h2 id=\"Update\"><a href=\"#Update\" class=\"headerlink\" title=\"Update\"></a>Update</h2><ul>\n<li><em>(2018.11) Oozie has Kerberos auth over SPNEGO for web (thanks to Justin Miller for pointing it out)</em></li>\n</ul>\n<h2 id=\"Disclaimer\"><a href=\"#Disclaimer\" class=\"headerlink\" title=\"Disclaimer\"></a>Disclaimer</h2><p>I’m not an expert in any of those engines.<br>I’ve used some of those (Airflow &amp; Azkaban) and checked the code.<br>For some others I either only read the code (Conductor) or the docs (Oozie/AWS Step Functions).<br>As most of them are OSS projects, it’s certainly possible that I might have missed certain undocumented features,<br>or community-contributed plugins. I’m happy to update this if you see anything wrong.</p>\n<p>Bottom line: Use your own judgement when reading this post.</p>\n<h2 id=\"Airflow\"><a href=\"#Airflow\" class=\"headerlink\" title=\"Airflow\"></a>Airflow</h2><h3 id=\"The-Good\"><a href=\"#The-Good\" class=\"headerlink\" title=\"The Good\"></a>The Good</h3><p>Airflow is a super feature rich engine compared to all other solutions.<br>Not only you can use plugins to support all kinds of jobs,<br>ranging from data processing jobs: Hive, Pig (though you can also submit them via shell command),<br>to general flow management like triggering by existence of file/db entry/s3 content,<br>or waiting for expected output from a web endpoint,<br>but also it provides a nice UI that allows you to check your DAGs (workflow dependencies) through code/graph,<br>and monitors the real time execution of jobs.</p>\n<p>Airflow is also highly customizable with a currently vigorous community.<br>You can run all your jobs through a single node using local executor,<br>or distribute them onto a group of worker nodes through Celery/Dask/Mesos orchestration.</p>\n<h3 id=\"The-Bad\"><a href=\"#The-Bad\" class=\"headerlink\" title=\"The Bad\"></a>The Bad</h3><p>Airflow by itself is still not very mature (in fact maybe Oozie is the only “mature” engine here).<br>The scheduler would need to periodically poll the scheduling plan and send jobs to executors.<br>This means it along would continuously dump enormous amount of logs out of the box.<br>As it works by “ticking”, your jobs are not guaranteed to get scheduled in “real-time” if that makes sense<br>and this would get worse as the number of concurrent jobs increases.<br>Meanwhile as you have one centralized scheduler, if it goes down or gets stuck, your running jobs won’t be<br>affected as that the job of executors, but no new jobs will get scheduled. This is especially confusing when<br>you run this with a HA setup where you have multiple web nodes, a scheduler, a broker<br>(typically a message queue in Celery case), multiple executors. When scheduler is stuck for whatever reason,<br>all you see in web UI is all tasks are running, but in fact they are not actually moving forward while executors<br>are happily reporting they are fine. In other words, the default monitoring is still far from bullet proof.</p>\n<p>The web UI is very nice from the first look. However it sometimes is confusing to new users.<br>What does it mean my DAG runs are “running” but my tasks have no state? The charts are not search friendly either,<br>let alone some of the features are still far from well documented<br>(though the document does look nice, I mean, compared to Oozie, which does seem out-dated).</p>\n<p>The backfilling design is good in certain cases but very error prone in others.<br>If you have a flow with cron schedules disabled and re-enabled later, it would try to play catch up,<br>and if your jobs is not designed to be idempotent, shit would happen for real.</p>\n<h2 id=\"Azkaban\"><a href=\"#Azkaban\" class=\"headerlink\" title=\"Azkaban\"></a>Azkaban</h2><h3 id=\"The-Good-1\"><a href=\"#The-Good-1\" class=\"headerlink\" title=\"The Good\"></a>The Good</h3><p>Of all the engines, Azkaban is probably the easiest to get going out of the box.<br>UI is very intuitive and easy to use. Scheduling and REST APIs works just fine.</p>\n<p>Limited HA setup works out of the box.<br>There’s no need for load balancer because you can only have one web node.<br>You can configure how it selects executor nodes to push jobs to and it generally seems to scale pretty nicely.<br>You can easily run tens of thousands of jobs as long as you have enough capacity for the executor nodes.</p>\n<h3 id=\"The-Bad-1\"><a href=\"#The-Bad-1\" class=\"headerlink\" title=\"The Bad\"></a>The Bad</h3><p>It is not very feature rich out of the box as a general purpose orchestration engine,<br>but likely that’s not what’s originally designed for. It’s strength lies in native support for Hadoop/Pig/Hive,<br>though you can also achieve those using command line. But itself cannot trigger jobs through external resources like<br>Airflow, nor does it support job waiting pattern. Although you can do busy waiting through java code/scripts, that<br>leads to bad resource utilization.</p>\n<p>The documentation and configuration are generally a bit confusing compared to others. It’s likely that it wasn’t supposed<br>to be OSed at the beginning. The design is okish but you better have a big data center to run the executors as scheduling<br>would get stalled when executors run out of resources without extra monitoring stuff. The code quality overall is a bit towards<br>the lower end compared to others so it generally only scales well when resource is not a problem.</p>\n<p>The setup/design is not cloud friendly. You are pretty much supposed to have stable bare metal rather than dynamically<br>allocated virtual instances with dynamic IPs. Scheduling would go south if machines vanish.</p>\n<p>The monitoring part is sort of acceptable through JMX (does not seem documented). But it generally doesn’t work well if your<br>machines are heavily loaded, unfortunately, as the endpoints may get stuck.</p>\n<h2 id=\"Conductor\"><a href=\"#Conductor\" class=\"headerlink\" title=\"Conductor\"></a>Conductor</h2><h3 id=\"The-Good-2\"><a href=\"#The-Good-2\" class=\"headerlink\" title=\"The Good\"></a>The Good</h3><p>It’s a bit unfair to put Conductor into this competition as it’s real purpose is for microservice orchestration, whatever that means.<br>It’s HA model involves a quorum of servers sitting behind load balancer putting tasks onto a message queue which the worker nodes would<br>poll from, which means it’s less likely you’ll run into stalled scheduling.<br>With the help of parameterized execution through API, it’s actually quite good at scheduling and scaling provided<br>that you set up your load balancer/service discovery layer properly.</p>\n<h3 id=\"The-Bad-2\"><a href=\"#The-Bad-2\" class=\"headerlink\" title=\"The Bad\"></a>The Bad</h3><p>The UI needs a bit more love. There’s currently very limited monitoring there. Although for general purpose scheduling that’s probably<br>good enough.</p>\n<p>It’s pretty bare-bone out of the box. There’s not even native support for running shell scripts, though it’s pretty easy to implement<br>a task worker through python to do the job with the examples provided.</p>\n<h2 id=\"Oozie\"><a href=\"#Oozie\" class=\"headerlink\" title=\"Oozie\"></a>Oozie</h2><h3 id=\"The-Good-3\"><a href=\"#The-Good-3\" class=\"headerlink\" title=\"The Good\"></a>The Good</h3><p>Oozie provides a seemingly reliable HA model through the db setup (seemingly b/c I’ve not dug into it).<br>It provides native support for Hadoop related jobs as it was sort of built for that eco system.</p>\n<h3 id=\"The-Bad-3\"><a href=\"#The-Bad-3\" class=\"headerlink\" title=\"The Bad\"></a>The Bad</h3><p>Not a very good candidate for general purpose flow scheduling as the XML definition is quite verbose<br>and cumbersome for defining light weight jobs.</p>\n<p>It also requires quite a bit of peripheral setup. You need a zookeeper cluster, a db, a load balancer<br>and each node needs to run a web app container like Tomcat. The initial setup also takes some time which is<br>not friendly to first time users to pilot stuff.</p>\n<h2 id=\"Step-Functions\"><a href=\"#Step-Functions\" class=\"headerlink\" title=\"Step Functions\"></a>Step Functions</h2><h3 id=\"The-Good-4\"><a href=\"#The-Good-4\" class=\"headerlink\" title=\"The Good\"></a>The Good</h3><p>Step Functions is fairly new (launch in Dec 2016). However the future seems promising. With the HA nature of cloud<br>platform and lambda functions, it almost feels like it can easily scale infinitely (compared to others).</p>\n<p>It also offers some useful features for general purpose workflow handling like waiting support and dynamic branching<br>based on output.</p>\n<p>It’s also fairly cheap:</p>\n<ul>\n<li>4,000 state transitions are free each month</li>\n<li>$0.025 per 1,000 state transitions thereafter ($0.000025 per state transition)</li>\n</ul>\n<p>If you don’t run tens of thousands of jobs, this might be even better than running your own cluster of things.</p>\n<h3 id=\"The-Bad-4\"><a href=\"#The-Bad-4\" class=\"headerlink\" title=\"The Bad\"></a>The Bad</h3><p>Can only be used by AWS users. Deal breaker if you are not one of them yet.</p>\n<p>Lambda requires extra work for production level iteration/deployment.</p>\n<p>There’s no UI (well there is but it’s really just a console).<br>So if you need any level of monitoring beyond that you need to build it using cloudwatch by yourself.</p>\n"},{"title":"You Don't Know JS - Equal or Not Equal","date":"2016-10-04T02:40:48.000Z","_content":"\n<!-- toc -->\n\n## == and ===\n\nLikely you know the difference between `==` and `===`: basically, `===` means strict equality where no implicit conversion is allowed whereas `==` is loose equality.\n\n{% codeblock lang:js %}\n\n'a' === 'a'     // true\n0 == false      // true\n\n{% endcodeblock %}\n\n## Dig deeper\n\nOK but this is too boring since we all know that.\n\nHow about this:\n\n{% codeblock lang:js %}\n\nString('a') === 'a'\nnew String('a') === 'a'\n\n{% endcodeblock %}\n\nWell the answers are `true` and `false` because `String()` returns a primitive string while `new String()` returns a string object. Surely `new String('a') == 'a'` yields `true`. No surprise.\n\n### What about arrays?\n\n    [] === []\n\nWell this returns `false` because for non-primitive objects, they are compared by reference. This always returns `false` because they are different in terms of memory location.\n\nHowever surprisingly you can compare arrays like this:\n\n    [1, 2, 3] < [2, 3]      // true\n    [2, 1, 3] > [1, 2, 3]   // true\n\n{% rage_face 'Blonde hmmm' style:width:125px %}\n\n(Wait a sec. I think I have an idea.)\n\nHow about this:\n\n    function arrEquals(arr1, arr2) {\n        return !(arr1 < arr2) && !(arr2 < arr1);\n    }\n\n{% rage_face 'Fuck yeah smile' style:width:125px %}\n\nWell this is wrong because arrays will be flattened when compared, like this\n\n    [[1, 2], 1] < [1, 2, 3]     // true\n\n### What about objects\n\nWhat's the result of this expression?\n\n    {} === {}\n\nWell it's neither `true` nor `false` but you get `SyntaxError` because in this case `{}` is not an object literal but a code block and thus it cannot be followed with `=`. Anyway we are drifting away from the original topic...\n\n## Implicit conversions\n\nWell that's just warm-up. Let's see something serious.\n\nIf you read something about \"best practices\", you would probably be told not to use `==` because of the evil conversion. However chances are you've used it here and there and most likely that's also part of the \"best practices\".\n\nFor example:\n\n    var foo = bar();\n    if (foo) {\n        doSomething();\n    }\n\nThis works because in JavaScript, only 6 object/literals are evaluated to `false`. They are `0`, `''`, `NaN`, `undefined`, `null` and of course `false`. Rest of the world evaluates to `true`, including `{}` and `[]`.\n\nHmm here's something wacky:\n\n{% codeblock lang:js %}\n\nvar a = {\n    valueOf: function () {\n        return -1;\n    }\n};\n\nif (!(1 + a)) {\n    alert('boom');\n}\n\n{% endcodeblock %}\n\nYour code does go boom because `1 + a` gets implicitly converted to `1 + a.valueOf()` and hence yields `0`.\n\nThe actual behavior is documented in ECMA standard - http://www.ecma-international.org/ecma-262/6.0/#sec-abstract-equality-comparison\n\nIn most cases, implicit conversion would cause `valueOf()` to be called or falls back to `toString()` if not defined.\n\nFor example:\n\n{% codeblock lang:js %}\n\nvar foo = {\n    valueOf: function () {\n        return 'value';\n    },\n    toString: function () {\n        return 'toString';\n    }\n};\n\n'foo' + foo             // foovalue\n\n{% endcodeblock %}\n\nThis is because according to standard, when `toPrimitive` is invoked for implicit conversion with no *hint* provided (e.g. in the case of concatenation, or when `==` is used between different types), it by default prefers `valueOf`. There are a few exceptions though, including but not limited to `Array.prototype.join` and `alert`. They would call `toPrimitive` with `string` as the hint so `toString()` will be favored.\n\n## Conclusion\n\nIn general, you probably want to avoid using `==` and use `===` most of the time if not always to avoid worrying about wonky implicit conversion magic.\n\nHowever, you can't be wary enough. For example:\n\n    isNaN('1') === true\n\nYou might think that `'1'` is a string and hence this should be `false` but unfortunately `isNaN` always calls `toNumber` internally ([spec](http://www.ecma-international.org/ecma-262/6.0/#sec-tonumber)) and hence this is `true`.\n\n{% rage_face 'Computer stare' style:width:200px %}\n","source":"_posts/You-Don-t-Know-JS-Equal-or-Not-Equal.md","raw":"---\ntitle: You Don't Know JS - Equal or Not Equal\ndate: 2016-10-03 19:40:48\ncategories:\n- You Don't Know JS\ntags:\n- javascript\n- frontend\n---\n\n<!-- toc -->\n\n## == and ===\n\nLikely you know the difference between `==` and `===`: basically, `===` means strict equality where no implicit conversion is allowed whereas `==` is loose equality.\n\n{% codeblock lang:js %}\n\n'a' === 'a'     // true\n0 == false      // true\n\n{% endcodeblock %}\n\n## Dig deeper\n\nOK but this is too boring since we all know that.\n\nHow about this:\n\n{% codeblock lang:js %}\n\nString('a') === 'a'\nnew String('a') === 'a'\n\n{% endcodeblock %}\n\nWell the answers are `true` and `false` because `String()` returns a primitive string while `new String()` returns a string object. Surely `new String('a') == 'a'` yields `true`. No surprise.\n\n### What about arrays?\n\n    [] === []\n\nWell this returns `false` because for non-primitive objects, they are compared by reference. This always returns `false` because they are different in terms of memory location.\n\nHowever surprisingly you can compare arrays like this:\n\n    [1, 2, 3] < [2, 3]      // true\n    [2, 1, 3] > [1, 2, 3]   // true\n\n{% rage_face 'Blonde hmmm' style:width:125px %}\n\n(Wait a sec. I think I have an idea.)\n\nHow about this:\n\n    function arrEquals(arr1, arr2) {\n        return !(arr1 < arr2) && !(arr2 < arr1);\n    }\n\n{% rage_face 'Fuck yeah smile' style:width:125px %}\n\nWell this is wrong because arrays will be flattened when compared, like this\n\n    [[1, 2], 1] < [1, 2, 3]     // true\n\n### What about objects\n\nWhat's the result of this expression?\n\n    {} === {}\n\nWell it's neither `true` nor `false` but you get `SyntaxError` because in this case `{}` is not an object literal but a code block and thus it cannot be followed with `=`. Anyway we are drifting away from the original topic...\n\n## Implicit conversions\n\nWell that's just warm-up. Let's see something serious.\n\nIf you read something about \"best practices\", you would probably be told not to use `==` because of the evil conversion. However chances are you've used it here and there and most likely that's also part of the \"best practices\".\n\nFor example:\n\n    var foo = bar();\n    if (foo) {\n        doSomething();\n    }\n\nThis works because in JavaScript, only 6 object/literals are evaluated to `false`. They are `0`, `''`, `NaN`, `undefined`, `null` and of course `false`. Rest of the world evaluates to `true`, including `{}` and `[]`.\n\nHmm here's something wacky:\n\n{% codeblock lang:js %}\n\nvar a = {\n    valueOf: function () {\n        return -1;\n    }\n};\n\nif (!(1 + a)) {\n    alert('boom');\n}\n\n{% endcodeblock %}\n\nYour code does go boom because `1 + a` gets implicitly converted to `1 + a.valueOf()` and hence yields `0`.\n\nThe actual behavior is documented in ECMA standard - http://www.ecma-international.org/ecma-262/6.0/#sec-abstract-equality-comparison\n\nIn most cases, implicit conversion would cause `valueOf()` to be called or falls back to `toString()` if not defined.\n\nFor example:\n\n{% codeblock lang:js %}\n\nvar foo = {\n    valueOf: function () {\n        return 'value';\n    },\n    toString: function () {\n        return 'toString';\n    }\n};\n\n'foo' + foo             // foovalue\n\n{% endcodeblock %}\n\nThis is because according to standard, when `toPrimitive` is invoked for implicit conversion with no *hint* provided (e.g. in the case of concatenation, or when `==` is used between different types), it by default prefers `valueOf`. There are a few exceptions though, including but not limited to `Array.prototype.join` and `alert`. They would call `toPrimitive` with `string` as the hint so `toString()` will be favored.\n\n## Conclusion\n\nIn general, you probably want to avoid using `==` and use `===` most of the time if not always to avoid worrying about wonky implicit conversion magic.\n\nHowever, you can't be wary enough. For example:\n\n    isNaN('1') === true\n\nYou might think that `'1'` is a string and hence this should be `false` but unfortunately `isNaN` always calls `toNumber` internally ([spec](http://www.ecma-international.org/ecma-262/6.0/#sec-tonumber)) and hence this is `true`.\n\n{% rage_face 'Computer stare' style:width:200px %}\n","slug":"You-Don-t-Know-JS-Equal-or-Not-Equal","published":1,"updated":"2025-09-01T22:26:51.293Z","comments":1,"layout":"post","photos":[],"_id":"cmf1pfvsf001h8mmg7t9s0r0a","content":"<!-- toc -->\n\n<h2 id=\"and\"><a href=\"#and\" class=\"headerlink\" title=\"== and ===\"></a>== and ===</h2><p>Likely you know the difference between <code>==</code> and <code>===</code>: basically, <code>===</code> means strict equality where no implicit conversion is allowed whereas <code>==</code> is loose equality.</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;a&#x27;</span> === <span class=\"string\">&#x27;a&#x27;</span>     <span class=\"comment\">// true</span></span><br><span class=\"line\"><span class=\"number\">0</span> == <span class=\"literal\">false</span>      <span class=\"comment\">// true</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Dig-deeper\"><a href=\"#Dig-deeper\" class=\"headerlink\" title=\"Dig deeper\"></a>Dig deeper</h2><p>OK but this is too boring since we all know that.</p>\n<p>How about this:</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"title class_\">String</span>(<span class=\"string\">&#x27;a&#x27;</span>) === <span class=\"string\">&#x27;a&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">new</span> <span class=\"title class_\">String</span>(<span class=\"string\">&#x27;a&#x27;</span>) === <span class=\"string\">&#x27;a&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>Well the answers are <code>true</code> and <code>false</code> because <code>String()</code> returns a primitive string while <code>new String()</code> returns a string object. Surely <code>new String(&#39;a&#39;) == &#39;a&#39;</code> yields <code>true</code>. No surprise.</p>\n<h3 id=\"What-about-arrays\"><a href=\"#What-about-arrays\" class=\"headerlink\" title=\"What about arrays?\"></a>What about arrays?</h3><pre><code>[] === []</code></pre><p>Well this returns <code>false</code> because for non-primitive objects, they are compared by reference. This always returns <code>false</code> because they are different in terms of memory location.</p>\n<p>However surprisingly you can compare arrays like this:</p>\n<pre><code>[1, 2, 3] &lt; [2, 3]      // true\n[2, 1, 3] &gt; [1, 2, 3]   // true</code></pre><img src=\"http://www.memes.at/faces/blonde_hmmm.jpg\" alt=\"Blonde hmmm\" style=\"width:125px\">\n\n<p>(Wait a sec. I think I have an idea.)</p>\n<p>How about this:</p>\n<pre><code>function arrEquals(arr1, arr2) &#123;\n    return !(arr1 &lt; arr2) &amp;&amp; !(arr2 &lt; arr1);\n&#125;</code></pre><img src=\"http://www.memes.at/faces/fuck_yeah_smile.jpg\" alt=\"Fuck yeah smile\" style=\"width:125px\">\n\n<p>Well this is wrong because arrays will be flattened when compared, like this</p>\n<pre><code>[[1, 2], 1] &lt; [1, 2, 3]     // true</code></pre><h3 id=\"What-about-objects\"><a href=\"#What-about-objects\" class=\"headerlink\" title=\"What about objects\"></a>What about objects</h3><p>What’s the result of this expression?</p>\n<pre><code>&#123;&#125; === &#123;&#125;</code></pre><p>Well it’s neither <code>true</code> nor <code>false</code> but you get <code>SyntaxError</code> because in this case <code>&#123;&#125;</code> is not an object literal but a code block and thus it cannot be followed with <code>=</code>. Anyway we are drifting away from the original topic…</p>\n<h2 id=\"Implicit-conversions\"><a href=\"#Implicit-conversions\" class=\"headerlink\" title=\"Implicit conversions\"></a>Implicit conversions</h2><p>Well that’s just warm-up. Let’s see something serious.</p>\n<p>If you read something about “best practices”, you would probably be told not to use <code>==</code> because of the evil conversion. However chances are you’ve used it here and there and most likely that’s also part of the “best practices”.</p>\n<p>For example:</p>\n<pre><code>var foo = bar();\nif (foo) &#123;\n    doSomething();\n&#125;</code></pre><p>This works because in JavaScript, only 6 object/literals are evaluated to <code>false</code>. They are <code>0</code>, <code>&#39;&#39;</code>, <code>NaN</code>, <code>undefined</code>, <code>null</code> and of course <code>false</code>. Rest of the world evaluates to <code>true</code>, including <code>&#123;&#125;</code> and <code>[]</code>.</p>\n<p>Hmm here’s something wacky:</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> a = &#123;</span><br><span class=\"line\">    <span class=\"attr\">valueOf</span>: <span class=\"keyword\">function</span> (<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> -<span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> (!(<span class=\"number\">1</span> + a)) &#123;</span><br><span class=\"line\">    <span class=\"title function_\">alert</span>(<span class=\"string\">&#x27;boom&#x27;</span>);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>Your code does go boom because <code>1 + a</code> gets implicitly converted to <code>1 + a.valueOf()</code> and hence yields <code>0</code>.</p>\n<p>The actual behavior is documented in ECMA standard - <a href=\"http://www.ecma-international.org/ecma-262/6.0/#sec-abstract-equality-comparison\">http://www.ecma-international.org/ecma-262/6.0/#sec-abstract-equality-comparison</a></p>\n<p>In most cases, implicit conversion would cause <code>valueOf()</code> to be called or falls back to <code>toString()</code> if not defined.</p>\n<p>For example:</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> foo = &#123;</span><br><span class=\"line\">    <span class=\"attr\">valueOf</span>: <span class=\"keyword\">function</span> (<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&#x27;value&#x27;</span>;</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">toString</span>: <span class=\"keyword\">function</span> (<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&#x27;toString&#x27;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;foo&#x27;</span> + foo             <span class=\"comment\">// foovalue</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>This is because according to standard, when <code>toPrimitive</code> is invoked for implicit conversion with no <em>hint</em> provided (e.g. in the case of concatenation, or when <code>==</code> is used between different types), it by default prefers <code>valueOf</code>. There are a few exceptions though, including but not limited to <code>Array.prototype.join</code> and <code>alert</code>. They would call <code>toPrimitive</code> with <code>string</code> as the hint so <code>toString()</code> will be favored.</p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>In general, you probably want to avoid using <code>==</code> and use <code>===</code> most of the time if not always to avoid worrying about wonky implicit conversion magic.</p>\n<p>However, you can’t be wary enough. For example:</p>\n<pre><code>isNaN(&apos;1&apos;) === true</code></pre><p>You might think that <code>&#39;1&#39;</code> is a string and hence this should be <code>false</code> but unfortunately <code>isNaN</code> always calls <code>toNumber</code> internally (<a href=\"http://www.ecma-international.org/ecma-262/6.0/#sec-tonumber\">spec</a>) and hence this is <code>true</code>.</p>\n<img src=\"http://www.memes.at/faces/computer_stare.jpg\" alt=\"Computer stare\" style=\"width:200px\">\n","thumbnailImageUrl":null,"excerpt":"","more":"<!-- toc -->\n\n<h2 id=\"and\"><a href=\"#and\" class=\"headerlink\" title=\"== and ===\"></a>== and ===</h2><p>Likely you know the difference between <code>==</code> and <code>===</code>: basically, <code>===</code> means strict equality where no implicit conversion is allowed whereas <code>==</code> is loose equality.</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;a&#x27;</span> === <span class=\"string\">&#x27;a&#x27;</span>     <span class=\"comment\">// true</span></span><br><span class=\"line\"><span class=\"number\">0</span> == <span class=\"literal\">false</span>      <span class=\"comment\">// true</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Dig-deeper\"><a href=\"#Dig-deeper\" class=\"headerlink\" title=\"Dig deeper\"></a>Dig deeper</h2><p>OK but this is too boring since we all know that.</p>\n<p>How about this:</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"title class_\">String</span>(<span class=\"string\">&#x27;a&#x27;</span>) === <span class=\"string\">&#x27;a&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">new</span> <span class=\"title class_\">String</span>(<span class=\"string\">&#x27;a&#x27;</span>) === <span class=\"string\">&#x27;a&#x27;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>Well the answers are <code>true</code> and <code>false</code> because <code>String()</code> returns a primitive string while <code>new String()</code> returns a string object. Surely <code>new String(&#39;a&#39;) == &#39;a&#39;</code> yields <code>true</code>. No surprise.</p>\n<h3 id=\"What-about-arrays\"><a href=\"#What-about-arrays\" class=\"headerlink\" title=\"What about arrays?\"></a>What about arrays?</h3><pre><code>[] === []</code></pre><p>Well this returns <code>false</code> because for non-primitive objects, they are compared by reference. This always returns <code>false</code> because they are different in terms of memory location.</p>\n<p>However surprisingly you can compare arrays like this:</p>\n<pre><code>[1, 2, 3] &lt; [2, 3]      // true\n[2, 1, 3] &gt; [1, 2, 3]   // true</code></pre><img src=\"http://www.memes.at/faces/blonde_hmmm.jpg\" alt=\"Blonde hmmm\" style=\"width:125px\">\n\n<p>(Wait a sec. I think I have an idea.)</p>\n<p>How about this:</p>\n<pre><code>function arrEquals(arr1, arr2) &#123;\n    return !(arr1 &lt; arr2) &amp;&amp; !(arr2 &lt; arr1);\n&#125;</code></pre><img src=\"http://www.memes.at/faces/fuck_yeah_smile.jpg\" alt=\"Fuck yeah smile\" style=\"width:125px\">\n\n<p>Well this is wrong because arrays will be flattened when compared, like this</p>\n<pre><code>[[1, 2], 1] &lt; [1, 2, 3]     // true</code></pre><h3 id=\"What-about-objects\"><a href=\"#What-about-objects\" class=\"headerlink\" title=\"What about objects\"></a>What about objects</h3><p>What’s the result of this expression?</p>\n<pre><code>&#123;&#125; === &#123;&#125;</code></pre><p>Well it’s neither <code>true</code> nor <code>false</code> but you get <code>SyntaxError</code> because in this case <code>&#123;&#125;</code> is not an object literal but a code block and thus it cannot be followed with <code>=</code>. Anyway we are drifting away from the original topic…</p>\n<h2 id=\"Implicit-conversions\"><a href=\"#Implicit-conversions\" class=\"headerlink\" title=\"Implicit conversions\"></a>Implicit conversions</h2><p>Well that’s just warm-up. Let’s see something serious.</p>\n<p>If you read something about “best practices”, you would probably be told not to use <code>==</code> because of the evil conversion. However chances are you’ve used it here and there and most likely that’s also part of the “best practices”.</p>\n<p>For example:</p>\n<pre><code>var foo = bar();\nif (foo) &#123;\n    doSomething();\n&#125;</code></pre><p>This works because in JavaScript, only 6 object/literals are evaluated to <code>false</code>. They are <code>0</code>, <code>&#39;&#39;</code>, <code>NaN</code>, <code>undefined</code>, <code>null</code> and of course <code>false</code>. Rest of the world evaluates to <code>true</code>, including <code>&#123;&#125;</code> and <code>[]</code>.</p>\n<p>Hmm here’s something wacky:</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> a = &#123;</span><br><span class=\"line\">    <span class=\"attr\">valueOf</span>: <span class=\"keyword\">function</span> (<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> -<span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> (!(<span class=\"number\">1</span> + a)) &#123;</span><br><span class=\"line\">    <span class=\"title function_\">alert</span>(<span class=\"string\">&#x27;boom&#x27;</span>);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>Your code does go boom because <code>1 + a</code> gets implicitly converted to <code>1 + a.valueOf()</code> and hence yields <code>0</code>.</p>\n<p>The actual behavior is documented in ECMA standard - <a href=\"http://www.ecma-international.org/ecma-262/6.0/#sec-abstract-equality-comparison\">http://www.ecma-international.org/ecma-262/6.0/#sec-abstract-equality-comparison</a></p>\n<p>In most cases, implicit conversion would cause <code>valueOf()</code> to be called or falls back to <code>toString()</code> if not defined.</p>\n<p>For example:</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">var</span> foo = &#123;</span><br><span class=\"line\">    <span class=\"attr\">valueOf</span>: <span class=\"keyword\">function</span> (<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&#x27;value&#x27;</span>;</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    <span class=\"attr\">toString</span>: <span class=\"keyword\">function</span> (<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&#x27;toString&#x27;</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;foo&#x27;</span> + foo             <span class=\"comment\">// foovalue</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>This is because according to standard, when <code>toPrimitive</code> is invoked for implicit conversion with no <em>hint</em> provided (e.g. in the case of concatenation, or when <code>==</code> is used between different types), it by default prefers <code>valueOf</code>. There are a few exceptions though, including but not limited to <code>Array.prototype.join</code> and <code>alert</code>. They would call <code>toPrimitive</code> with <code>string</code> as the hint so <code>toString()</code> will be favored.</p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>In general, you probably want to avoid using <code>==</code> and use <code>===</code> most of the time if not always to avoid worrying about wonky implicit conversion magic.</p>\n<p>However, you can’t be wary enough. For example:</p>\n<pre><code>isNaN(&apos;1&apos;) === true</code></pre><p>You might think that <code>&#39;1&#39;</code> is a string and hence this should be <code>false</code> but unfortunately <code>isNaN</code> always calls <code>toNumber</code> internally (<a href=\"http://www.ecma-international.org/ecma-262/6.0/#sec-tonumber\">spec</a>) and hence this is <code>true</code>.</p>\n<img src=\"http://www.memes.at/faces/computer_stare.jpg\" alt=\"Computer stare\" style=\"width:200px\">\n"},{"title":"You Don't Know JS - Eval","date":"2016-10-01T23:03:31.000Z","_content":"\n<!-- toc -->\n\nRecently I've been writing quite a bit of front-end stuff and seen quite a few tricks from other people's libraries. It turns out JavaScript is a pretty ~~wonky and fked up~~ interesting language, which tempts me to write a series about it and this is the first one. This is by no means supposed to show how to write JS but just to show some \"wacky\" stuff.\n\n## Have you seen eval() written like this?\n\n    (0, eval)('something');\n\n{% rage_face 'Are you fucking kidding me' style:width:200px %}\n\n## Regular eval\n\nEval basically allows you to execute any script within the given context.\n\nFor example:\n\n{% codeblock lang:js %}\neval('console.log(\"123\");');            // prints out 123\n\n(function A() {\n    this.a = 1;\n\n    eval('console.log(this.a);');            // 1\n})();\n\n{% endcodeblock %}\n\nSo far everything is normal: eval runs inside the current scope. `this` is pointed to the instance of A.\n\n## Global eval\n\nThings get interesting when you do this:\n\n{% codeblock lang:js %}\nvar someVar = 'outer';\n\n(function A() {\n    this.someVar = 'inner';\n\n    eval('console.log(someVar);');       // you may want 'outer' but this says 'inner'\n})();\n\n{% endcodeblock %}\n\nWell in this scenario eval cannot get the value of someVar in the global scope.\n\nHowever ECMA5 says, if you change `eval()` call to indirect, in other words, if you use it as a value rather than a function reference, then it will evaluate the input in the global scope.\n\nSo this would work:\n\n{% codeblock lang:js %}\nvar someVar = 'outer';\n\n(function A() {\n    var geval = eval;\n    this.someVar = 'inner';\n\n    geval('console.log(someVar);');       // 'outer'\n})();\n\n{% endcodeblock %}\n\nAlthough `geval` and `eval` call the exact same function, `geval` is a value and thus it becomes an indirect call according to ECMA5.\n\n## Back to the original topic\n\nSo what the hell is `(0, eval)` then? Well a comma separated expression list evaluates to the last value, so it essentially is a shortcut to\n\n    var geval = eval;\n    geval(...);\n\n0 is only a puppet here. It could be any value.\n\n{% rage_face 'So much win' style:width:200px %}","source":"_posts/You-Don-t-Know-JS-Eval.md","raw":"---\ntitle: You Don't Know JS - Eval\ndate: 2016-10-01 16:03:31\ncategories:\n- You Don't Know JS\ntags:\n- javascript\n---\n\n<!-- toc -->\n\nRecently I've been writing quite a bit of front-end stuff and seen quite a few tricks from other people's libraries. It turns out JavaScript is a pretty ~~wonky and fked up~~ interesting language, which tempts me to write a series about it and this is the first one. This is by no means supposed to show how to write JS but just to show some \"wacky\" stuff.\n\n## Have you seen eval() written like this?\n\n    (0, eval)('something');\n\n{% rage_face 'Are you fucking kidding me' style:width:200px %}\n\n## Regular eval\n\nEval basically allows you to execute any script within the given context.\n\nFor example:\n\n{% codeblock lang:js %}\neval('console.log(\"123\");');            // prints out 123\n\n(function A() {\n    this.a = 1;\n\n    eval('console.log(this.a);');            // 1\n})();\n\n{% endcodeblock %}\n\nSo far everything is normal: eval runs inside the current scope. `this` is pointed to the instance of A.\n\n## Global eval\n\nThings get interesting when you do this:\n\n{% codeblock lang:js %}\nvar someVar = 'outer';\n\n(function A() {\n    this.someVar = 'inner';\n\n    eval('console.log(someVar);');       // you may want 'outer' but this says 'inner'\n})();\n\n{% endcodeblock %}\n\nWell in this scenario eval cannot get the value of someVar in the global scope.\n\nHowever ECMA5 says, if you change `eval()` call to indirect, in other words, if you use it as a value rather than a function reference, then it will evaluate the input in the global scope.\n\nSo this would work:\n\n{% codeblock lang:js %}\nvar someVar = 'outer';\n\n(function A() {\n    var geval = eval;\n    this.someVar = 'inner';\n\n    geval('console.log(someVar);');       // 'outer'\n})();\n\n{% endcodeblock %}\n\nAlthough `geval` and `eval` call the exact same function, `geval` is a value and thus it becomes an indirect call according to ECMA5.\n\n## Back to the original topic\n\nSo what the hell is `(0, eval)` then? Well a comma separated expression list evaluates to the last value, so it essentially is a shortcut to\n\n    var geval = eval;\n    geval(...);\n\n0 is only a puppet here. It could be any value.\n\n{% rage_face 'So much win' style:width:200px %}","slug":"You-Don-t-Know-JS-Eval","published":1,"updated":"2025-09-01T22:26:51.294Z","comments":1,"layout":"post","photos":[],"_id":"cmf1pfvsg001j8mmgfnkf35hi","content":"<!-- toc -->\n\n<p>Recently I’ve been writing quite a bit of front-end stuff and seen quite a few tricks from other people’s libraries. It turns out JavaScript is a pretty <del>wonky and fked up</del> interesting language, which tempts me to write a series about it and this is the first one. This is by no means supposed to show how to write JS but just to show some “wacky” stuff.</p>\n<h2 id=\"Have-you-seen-eval-written-like-this\"><a href=\"#Have-you-seen-eval-written-like-this\" class=\"headerlink\" title=\"Have you seen eval() written like this?\"></a>Have you seen eval() written like this?</h2><pre><code>(0, eval)(&apos;something&apos;);</code></pre><img src=\"http://www.memes.at/faces/are_you_fucking_kidding_me.jpg\" alt=\"Are you fucking kidding me\" style=\"width:200px\">\n\n<h2 id=\"Regular-eval\"><a href=\"#Regular-eval\" class=\"headerlink\" title=\"Regular eval\"></a>Regular eval</h2><p>Eval basically allows you to execute any script within the given context.</p>\n<p>For example:</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">eval</span>(<span class=\"string\">&#x27;console.log(&quot;123&quot;);&#x27;</span>);            <span class=\"comment\">// prints out 123</span></span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"keyword\">function</span> <span class=\"title function_\">A</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">this</span>.<span class=\"property\">a</span> = <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">eval</span>(<span class=\"string\">&#x27;console.log(this.a);&#x27;</span>);            <span class=\"comment\">// 1</span></span><br><span class=\"line\">&#125;)();</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>So far everything is normal: eval runs inside the current scope. <code>this</code> is pointed to the instance of A.</p>\n<h2 id=\"Global-eval\"><a href=\"#Global-eval\" class=\"headerlink\" title=\"Global eval\"></a>Global eval</h2><p>Things get interesting when you do this:</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> someVar = <span class=\"string\">&#x27;outer&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"keyword\">function</span> <span class=\"title function_\">A</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">this</span>.<span class=\"property\">someVar</span> = <span class=\"string\">&#x27;inner&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">eval</span>(<span class=\"string\">&#x27;console.log(someVar);&#x27;</span>);       <span class=\"comment\">// you may want &#x27;outer&#x27; but this says &#x27;inner&#x27;</span></span><br><span class=\"line\">&#125;)();</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>Well in this scenario eval cannot get the value of someVar in the global scope.</p>\n<p>However ECMA5 says, if you change <code>eval()</code> call to indirect, in other words, if you use it as a value rather than a function reference, then it will evaluate the input in the global scope.</p>\n<p>So this would work:</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> someVar = <span class=\"string\">&#x27;outer&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"keyword\">function</span> <span class=\"title function_\">A</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> geval = <span class=\"built_in\">eval</span>;</span><br><span class=\"line\">    <span class=\"variable language_\">this</span>.<span class=\"property\">someVar</span> = <span class=\"string\">&#x27;inner&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"title function_\">geval</span>(<span class=\"string\">&#x27;console.log(someVar);&#x27;</span>);       <span class=\"comment\">// &#x27;outer&#x27;</span></span><br><span class=\"line\">&#125;)();</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>Although <code>geval</code> and <code>eval</code> call the exact same function, <code>geval</code> is a value and thus it becomes an indirect call according to ECMA5.</p>\n<h2 id=\"Back-to-the-original-topic\"><a href=\"#Back-to-the-original-topic\" class=\"headerlink\" title=\"Back to the original topic\"></a>Back to the original topic</h2><p>So what the hell is <code>(0, eval)</code> then? Well a comma separated expression list evaluates to the last value, so it essentially is a shortcut to</p>\n<pre><code>var geval = eval;\ngeval(...);</code></pre><p>0 is only a puppet here. It could be any value.</p>\n<img src=\"http://www.memes.at/faces/so_much_win.jpg\" alt=\"So much win\" style=\"width:200px\">","thumbnailImageUrl":null,"excerpt":"","more":"<!-- toc -->\n\n<p>Recently I’ve been writing quite a bit of front-end stuff and seen quite a few tricks from other people’s libraries. It turns out JavaScript is a pretty <del>wonky and fked up</del> interesting language, which tempts me to write a series about it and this is the first one. This is by no means supposed to show how to write JS but just to show some “wacky” stuff.</p>\n<h2 id=\"Have-you-seen-eval-written-like-this\"><a href=\"#Have-you-seen-eval-written-like-this\" class=\"headerlink\" title=\"Have you seen eval() written like this?\"></a>Have you seen eval() written like this?</h2><pre><code>(0, eval)(&apos;something&apos;);</code></pre><img src=\"http://www.memes.at/faces/are_you_fucking_kidding_me.jpg\" alt=\"Are you fucking kidding me\" style=\"width:200px\">\n\n<h2 id=\"Regular-eval\"><a href=\"#Regular-eval\" class=\"headerlink\" title=\"Regular eval\"></a>Regular eval</h2><p>Eval basically allows you to execute any script within the given context.</p>\n<p>For example:</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">eval</span>(<span class=\"string\">&#x27;console.log(&quot;123&quot;);&#x27;</span>);            <span class=\"comment\">// prints out 123</span></span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"keyword\">function</span> <span class=\"title function_\">A</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">this</span>.<span class=\"property\">a</span> = <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">eval</span>(<span class=\"string\">&#x27;console.log(this.a);&#x27;</span>);            <span class=\"comment\">// 1</span></span><br><span class=\"line\">&#125;)();</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>So far everything is normal: eval runs inside the current scope. <code>this</code> is pointed to the instance of A.</p>\n<h2 id=\"Global-eval\"><a href=\"#Global-eval\" class=\"headerlink\" title=\"Global eval\"></a>Global eval</h2><p>Things get interesting when you do this:</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> someVar = <span class=\"string\">&#x27;outer&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"keyword\">function</span> <span class=\"title function_\">A</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    <span class=\"variable language_\">this</span>.<span class=\"property\">someVar</span> = <span class=\"string\">&#x27;inner&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">eval</span>(<span class=\"string\">&#x27;console.log(someVar);&#x27;</span>);       <span class=\"comment\">// you may want &#x27;outer&#x27; but this says &#x27;inner&#x27;</span></span><br><span class=\"line\">&#125;)();</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>Well in this scenario eval cannot get the value of someVar in the global scope.</p>\n<p>However ECMA5 says, if you change <code>eval()</code> call to indirect, in other words, if you use it as a value rather than a function reference, then it will evaluate the input in the global scope.</p>\n<p>So this would work:</p>\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> someVar = <span class=\"string\">&#x27;outer&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"keyword\">function</span> <span class=\"title function_\">A</span>(<span class=\"params\"></span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> geval = <span class=\"built_in\">eval</span>;</span><br><span class=\"line\">    <span class=\"variable language_\">this</span>.<span class=\"property\">someVar</span> = <span class=\"string\">&#x27;inner&#x27;</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"title function_\">geval</span>(<span class=\"string\">&#x27;console.log(someVar);&#x27;</span>);       <span class=\"comment\">// &#x27;outer&#x27;</span></span><br><span class=\"line\">&#125;)();</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>Although <code>geval</code> and <code>eval</code> call the exact same function, <code>geval</code> is a value and thus it becomes an indirect call according to ECMA5.</p>\n<h2 id=\"Back-to-the-original-topic\"><a href=\"#Back-to-the-original-topic\" class=\"headerlink\" title=\"Back to the original topic\"></a>Back to the original topic</h2><p>So what the hell is <code>(0, eval)</code> then? Well a comma separated expression list evaluates to the last value, so it essentially is a shortcut to</p>\n<pre><code>var geval = eval;\ngeval(...);</code></pre><p>0 is only a puppet here. It could be any value.</p>\n<img src=\"http://www.memes.at/faces/so_much_win.jpg\" alt=\"So much win\" style=\"width:200px\">"},{"title":"Get Started with Hexo Blogging System","date":"2016-08-14T21:32:53.000Z","_content":"\n# You should read this if\n\n- You want to set up a personal blog\n- You know what [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) is\n- You don't want to set up a heavy Wordpress environment\n- You don't want to set up any database just for the blog\n- You either don't have a VPS or want to host blog content in some easy-to-reach place.\n- You still want a template/theme system.\n\n# Solution\n\nGithub Pages + Hexo (what this site uses)\n\n<!-- more -->\n\n# What is Hexo\n\n\"[Hexo](https://hexo.io/docs/index.html) is a fast, simple and powerful blog framework. You write posts in Markdown (or other languages) and Hexo generates static files with a beautiful theme in seconds.\"\n\nIn other words, Hexo is a **static** blogging system. This means there is no need for database, node/php code to maintain like Wordpress or other dynamic blogging systems require. Awesome.\n\n# How on earth do things work\n\n- You write posts using Markdown language and preview them locally\n- Hexo generates the static htmls locally\n- You commit all static assets to Github\n- All content will be public via Github IO\n    - When you hit path that ends with /, Github will attempt to read index.html under that directory, i.e. for /foo/, the corresponding file is /foo/index.html\n    - For URLs don't end with / (except root), Github will try to read the corresponding file, i.e. for /foo, if file foo doesn't exist, 404 will be returned.\n    - Generally, Hexo's path layout should be fine but if you use certain themes (like the default landscape), you might need to fix the URLs manually if they don't end with / while they should.\n\n# What this solution cannot do\n\nYou can think of Hexo as a lightweight templating system so most dynamic features that Wordpress offer don't exist, including searching (provided by simple google search instead), comment system (you can use Disqus instead), dynamic widgets like top posts, permalink backend forwarding and so on.\n\nHowever, do you really need these?\n\n# Get started\n\nIf you've read this far, chances are you want to give it a try. Here are the steps:\n\n1. Install node (required by Hexo): https://nodejs.org/en/download/\n\n2. Install Hexo\n\n        npm install -g hexo-cli\n        \n        // this downloads the starter pack and sets up node modules under the directory <name> for you so it may take a while\n        hexo init <name>\n\n3. Open `<name>/_config.yml` and there may be a few things you want to set such as the name and author. The most important things are the `url` and `root`. The details can be found [here](https://hexo.io/docs/configuration.html)\n\n4. By default Hexo will generate the Hello world post for you. You can preview it by starting the local server:\n\n         hexo server\n\n5. Generate actual assets:\n\n         hexo generate\n\n6. Set up Github page repo: https://pages.github.com/\n\nDepending on if you have a VPS, things maybe a bit different. Follow 7a if you do and 7b if not. The major difference is the directory structure in the repo.\n\n7a-1. Hexo by default publishes all static assets to `public`. However, Github by default expects stuff to be under root. That means, with this setup, the repo needs to be rooted at `public`:\n\n        cd public\n        git init\n        git remote add origin <repo url>\n        git pull\n\n7a-2. Commit all assets:\n\n        git add --all\n        git commit -m \"...\"\n        git push\n\n7a-3. If you have a custom domain, set CNAME pointed to your.github.io and remember to disable https enforcement in Github (since SSL will verify host name). Also don't forget to set the custom url in Github.\n\n---\n\n\n7b-1. If you have a VPS then you can commit all the files and just use github as the storage area since you can do backend url forwarding.\n\n{% codeblock nginx.conf lang:nginx %}\nserver {\n    listen       80;\n    server_name  xcorpion.tech;\n\n    location / {\n        proxy_set_header Host xcorpion.tech;\n        proxy_pass http://your.github.io/public/;\n    }\n}\n{% endcodeblock %}\n\n7b-2. If you have a custom domain, set A record pointed to your server IP and similarily, remember to turn off https enforcement. Also don't forget to set the custom url in Github.\n\n# And that's it! Let's start writing!\n\n- In your hexo root directory, start a new post:\n\n        hexo new post\n\n- Preview your post with\n\n        hexo server\n\n- In preview mode, Hexo will watch your files and automatically update the html. However, it **will not** publish any static assets (those eventually shown in your site). So run this to let hexo do it:\n\n        hexo generate\n\n- Then commit the changes to your repo (You can automate this with a shell script.)\n","source":"_posts/get-started-with-hexo-blogging-system.md","raw":"---\ntitle: Get Started with Hexo Blogging System\ndate: 2016-08-14 14:32:53\ncategories:\n- Hexo\ntags:\n- blog\n- hexo\n---\n\n# You should read this if\n\n- You want to set up a personal blog\n- You know what [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) is\n- You don't want to set up a heavy Wordpress environment\n- You don't want to set up any database just for the blog\n- You either don't have a VPS or want to host blog content in some easy-to-reach place.\n- You still want a template/theme system.\n\n# Solution\n\nGithub Pages + Hexo (what this site uses)\n\n<!-- more -->\n\n# What is Hexo\n\n\"[Hexo](https://hexo.io/docs/index.html) is a fast, simple and powerful blog framework. You write posts in Markdown (or other languages) and Hexo generates static files with a beautiful theme in seconds.\"\n\nIn other words, Hexo is a **static** blogging system. This means there is no need for database, node/php code to maintain like Wordpress or other dynamic blogging systems require. Awesome.\n\n# How on earth do things work\n\n- You write posts using Markdown language and preview them locally\n- Hexo generates the static htmls locally\n- You commit all static assets to Github\n- All content will be public via Github IO\n    - When you hit path that ends with /, Github will attempt to read index.html under that directory, i.e. for /foo/, the corresponding file is /foo/index.html\n    - For URLs don't end with / (except root), Github will try to read the corresponding file, i.e. for /foo, if file foo doesn't exist, 404 will be returned.\n    - Generally, Hexo's path layout should be fine but if you use certain themes (like the default landscape), you might need to fix the URLs manually if they don't end with / while they should.\n\n# What this solution cannot do\n\nYou can think of Hexo as a lightweight templating system so most dynamic features that Wordpress offer don't exist, including searching (provided by simple google search instead), comment system (you can use Disqus instead), dynamic widgets like top posts, permalink backend forwarding and so on.\n\nHowever, do you really need these?\n\n# Get started\n\nIf you've read this far, chances are you want to give it a try. Here are the steps:\n\n1. Install node (required by Hexo): https://nodejs.org/en/download/\n\n2. Install Hexo\n\n        npm install -g hexo-cli\n        \n        // this downloads the starter pack and sets up node modules under the directory <name> for you so it may take a while\n        hexo init <name>\n\n3. Open `<name>/_config.yml` and there may be a few things you want to set such as the name and author. The most important things are the `url` and `root`. The details can be found [here](https://hexo.io/docs/configuration.html)\n\n4. By default Hexo will generate the Hello world post for you. You can preview it by starting the local server:\n\n         hexo server\n\n5. Generate actual assets:\n\n         hexo generate\n\n6. Set up Github page repo: https://pages.github.com/\n\nDepending on if you have a VPS, things maybe a bit different. Follow 7a if you do and 7b if not. The major difference is the directory structure in the repo.\n\n7a-1. Hexo by default publishes all static assets to `public`. However, Github by default expects stuff to be under root. That means, with this setup, the repo needs to be rooted at `public`:\n\n        cd public\n        git init\n        git remote add origin <repo url>\n        git pull\n\n7a-2. Commit all assets:\n\n        git add --all\n        git commit -m \"...\"\n        git push\n\n7a-3. If you have a custom domain, set CNAME pointed to your.github.io and remember to disable https enforcement in Github (since SSL will verify host name). Also don't forget to set the custom url in Github.\n\n---\n\n\n7b-1. If you have a VPS then you can commit all the files and just use github as the storage area since you can do backend url forwarding.\n\n{% codeblock nginx.conf lang:nginx %}\nserver {\n    listen       80;\n    server_name  xcorpion.tech;\n\n    location / {\n        proxy_set_header Host xcorpion.tech;\n        proxy_pass http://your.github.io/public/;\n    }\n}\n{% endcodeblock %}\n\n7b-2. If you have a custom domain, set A record pointed to your server IP and similarily, remember to turn off https enforcement. Also don't forget to set the custom url in Github.\n\n# And that's it! Let's start writing!\n\n- In your hexo root directory, start a new post:\n\n        hexo new post\n\n- Preview your post with\n\n        hexo server\n\n- In preview mode, Hexo will watch your files and automatically update the html. However, it **will not** publish any static assets (those eventually shown in your site). So run this to let hexo do it:\n\n        hexo generate\n\n- Then commit the changes to your repo (You can automate this with a shell script.)\n","slug":"get-started-with-hexo-blogging-system","published":1,"updated":"2025-09-01T22:26:51.294Z","comments":1,"layout":"post","photos":[],"_id":"cmf1pfvsg001m8mmg9iab8lbd","content":"<h1 id=\"You-should-read-this-if\"><a href=\"#You-should-read-this-if\" class=\"headerlink\" title=\"You should read this if\"></a>You should read this if</h1><ul>\n<li>You want to set up a personal blog</li>\n<li>You know what <a href=\"https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\">Markdown</a> is</li>\n<li>You don’t want to set up a heavy Wordpress environment</li>\n<li>You don’t want to set up any database just for the blog</li>\n<li>You either don’t have a VPS or want to host blog content in some easy-to-reach place.</li>\n<li>You still want a template/theme system.</li>\n</ul>\n<h1 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h1><p>Github Pages + Hexo (what this site uses)</p>\n<span id=\"more\"></span>\n\n<h1 id=\"What-is-Hexo\"><a href=\"#What-is-Hexo\" class=\"headerlink\" title=\"What is Hexo\"></a>What is Hexo</h1><p>“<a href=\"https://hexo.io/docs/index.html\">Hexo</a> is a fast, simple and powerful blog framework. You write posts in Markdown (or other languages) and Hexo generates static files with a beautiful theme in seconds.”</p>\n<p>In other words, Hexo is a <strong>static</strong> blogging system. This means there is no need for database, node/php code to maintain like Wordpress or other dynamic blogging systems require. Awesome.</p>\n<h1 id=\"How-on-earth-do-things-work\"><a href=\"#How-on-earth-do-things-work\" class=\"headerlink\" title=\"How on earth do things work\"></a>How on earth do things work</h1><ul>\n<li>You write posts using Markdown language and preview them locally</li>\n<li>Hexo generates the static htmls locally</li>\n<li>You commit all static assets to Github</li>\n<li>All content will be public via Github IO<ul>\n<li>When you hit path that ends with /, Github will attempt to read index.html under that directory, i.e. for /foo/, the corresponding file is /foo/index.html</li>\n<li>For URLs don’t end with / (except root), Github will try to read the corresponding file, i.e. for /foo, if file foo doesn’t exist, 404 will be returned.</li>\n<li>Generally, Hexo’s path layout should be fine but if you use certain themes (like the default landscape), you might need to fix the URLs manually if they don’t end with / while they should.</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"What-this-solution-cannot-do\"><a href=\"#What-this-solution-cannot-do\" class=\"headerlink\" title=\"What this solution cannot do\"></a>What this solution cannot do</h1><p>You can think of Hexo as a lightweight templating system so most dynamic features that Wordpress offer don’t exist, including searching (provided by simple google search instead), comment system (you can use Disqus instead), dynamic widgets like top posts, permalink backend forwarding and so on.</p>\n<p>However, do you really need these?</p>\n<h1 id=\"Get-started\"><a href=\"#Get-started\" class=\"headerlink\" title=\"Get started\"></a>Get started</h1><p>If you’ve read this far, chances are you want to give it a try. Here are the steps:</p>\n<ol>\n<li><p>Install node (required by Hexo): <a href=\"https://nodejs.org/en/download/\">https://nodejs.org/en/download/</a></p>\n</li>\n<li><p>Install Hexo</p>\n<pre><code>npm install -g hexo-cli\n\n// this downloads the starter pack and sets up node modules under the directory &lt;name&gt; for you so it may take a while\nhexo init &lt;name&gt;</code></pre></li>\n<li><p>Open <code>&lt;name&gt;/_config.yml</code> and there may be a few things you want to set such as the name and author. The most important things are the <code>url</code> and <code>root</code>. The details can be found <a href=\"https://hexo.io/docs/configuration.html\">here</a></p>\n</li>\n<li><p>By default Hexo will generate the Hello world post for you. You can preview it by starting the local server:</p>\n<pre><code>hexo server</code></pre></li>\n<li><p>Generate actual assets:</p>\n<pre><code>hexo generate</code></pre></li>\n<li><p>Set up Github page repo: <a href=\"https://pages.github.com/\">https://pages.github.com/</a></p>\n</li>\n</ol>\n<p>Depending on if you have a VPS, things maybe a bit different. Follow 7a if you do and 7b if not. The major difference is the directory structure in the repo.</p>\n<p>7a-1. Hexo by default publishes all static assets to <code>public</code>. However, Github by default expects stuff to be under root. That means, with this setup, the repo needs to be rooted at <code>public</code>:</p>\n<pre><code>cd public\ngit init\ngit remote add origin &lt;repo url&gt;\ngit pull</code></pre><p>7a-2. Commit all assets:</p>\n<pre><code>git add --all\ngit commit -m &quot;...&quot;\ngit push</code></pre><p>7a-3. If you have a custom domain, set CNAME pointed to your.github.io and remember to disable https enforcement in Github (since SSL will verify host name). Also don’t forget to set the custom url in Github.</p>\n<hr>\n<p>7b-1. If you have a VPS then you can commit all the files and just use github as the storage area since you can do backend url forwarding.</p>\n<figure class=\"highlight nginx\"><figcaption><span>nginx.conf</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"section\">server</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">listen</span>       <span class=\"number\">80</span>;</span><br><span class=\"line\">    <span class=\"attribute\">server_name</span>  xcorpion.tech;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"section\">location</span> / &#123;</span><br><span class=\"line\">        <span class=\"attribute\">proxy_set_header</span> Host xcorpion.tech;</span><br><span class=\"line\">        <span class=\"attribute\">proxy_pass</span> http://your.github.io/public/;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>7b-2. If you have a custom domain, set A record pointed to your server IP and similarily, remember to turn off https enforcement. Also don’t forget to set the custom url in Github.</p>\n<h1 id=\"And-that’s-it-Let’s-start-writing\"><a href=\"#And-that’s-it-Let’s-start-writing\" class=\"headerlink\" title=\"And that’s it! Let’s start writing!\"></a>And that’s it! Let’s start writing!</h1><ul>\n<li><p>In your hexo root directory, start a new post:</p>\n<pre><code>hexo new post</code></pre></li>\n<li><p>Preview your post with</p>\n<pre><code>hexo server</code></pre></li>\n<li><p>In preview mode, Hexo will watch your files and automatically update the html. However, it <strong>will not</strong> publish any static assets (those eventually shown in your site). So run this to let hexo do it:</p>\n<pre><code>hexo generate</code></pre></li>\n<li><p>Then commit the changes to your repo (You can automate this with a shell script.)</p>\n</li>\n</ul>\n","thumbnailImageUrl":null,"excerpt":"<h1 id=\"You-should-read-this-if\"><a href=\"#You-should-read-this-if\" class=\"headerlink\" title=\"You should read this if\"></a>You should read this if</h1><ul>\n<li>You want to set up a personal blog</li>\n<li>You know what <a href=\"https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\">Markdown</a> is</li>\n<li>You don’t want to set up a heavy Wordpress environment</li>\n<li>You don’t want to set up any database just for the blog</li>\n<li>You either don’t have a VPS or want to host blog content in some easy-to-reach place.</li>\n<li>You still want a template/theme system.</li>\n</ul>\n<h1 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h1><p>Github Pages + Hexo (what this site uses)</p>","more":"<h1 id=\"What-is-Hexo\"><a href=\"#What-is-Hexo\" class=\"headerlink\" title=\"What is Hexo\"></a>What is Hexo</h1><p>“<a href=\"https://hexo.io/docs/index.html\">Hexo</a> is a fast, simple and powerful blog framework. You write posts in Markdown (or other languages) and Hexo generates static files with a beautiful theme in seconds.”</p>\n<p>In other words, Hexo is a <strong>static</strong> blogging system. This means there is no need for database, node/php code to maintain like Wordpress or other dynamic blogging systems require. Awesome.</p>\n<h1 id=\"How-on-earth-do-things-work\"><a href=\"#How-on-earth-do-things-work\" class=\"headerlink\" title=\"How on earth do things work\"></a>How on earth do things work</h1><ul>\n<li>You write posts using Markdown language and preview them locally</li>\n<li>Hexo generates the static htmls locally</li>\n<li>You commit all static assets to Github</li>\n<li>All content will be public via Github IO<ul>\n<li>When you hit path that ends with /, Github will attempt to read index.html under that directory, i.e. for /foo/, the corresponding file is /foo/index.html</li>\n<li>For URLs don’t end with / (except root), Github will try to read the corresponding file, i.e. for /foo, if file foo doesn’t exist, 404 will be returned.</li>\n<li>Generally, Hexo’s path layout should be fine but if you use certain themes (like the default landscape), you might need to fix the URLs manually if they don’t end with / while they should.</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"What-this-solution-cannot-do\"><a href=\"#What-this-solution-cannot-do\" class=\"headerlink\" title=\"What this solution cannot do\"></a>What this solution cannot do</h1><p>You can think of Hexo as a lightweight templating system so most dynamic features that Wordpress offer don’t exist, including searching (provided by simple google search instead), comment system (you can use Disqus instead), dynamic widgets like top posts, permalink backend forwarding and so on.</p>\n<p>However, do you really need these?</p>\n<h1 id=\"Get-started\"><a href=\"#Get-started\" class=\"headerlink\" title=\"Get started\"></a>Get started</h1><p>If you’ve read this far, chances are you want to give it a try. Here are the steps:</p>\n<ol>\n<li><p>Install node (required by Hexo): <a href=\"https://nodejs.org/en/download/\">https://nodejs.org/en/download/</a></p>\n</li>\n<li><p>Install Hexo</p>\n<pre><code>npm install -g hexo-cli\n\n// this downloads the starter pack and sets up node modules under the directory &lt;name&gt; for you so it may take a while\nhexo init &lt;name&gt;</code></pre></li>\n<li><p>Open <code>&lt;name&gt;/_config.yml</code> and there may be a few things you want to set such as the name and author. The most important things are the <code>url</code> and <code>root</code>. The details can be found <a href=\"https://hexo.io/docs/configuration.html\">here</a></p>\n</li>\n<li><p>By default Hexo will generate the Hello world post for you. You can preview it by starting the local server:</p>\n<pre><code>hexo server</code></pre></li>\n<li><p>Generate actual assets:</p>\n<pre><code>hexo generate</code></pre></li>\n<li><p>Set up Github page repo: <a href=\"https://pages.github.com/\">https://pages.github.com/</a></p>\n</li>\n</ol>\n<p>Depending on if you have a VPS, things maybe a bit different. Follow 7a if you do and 7b if not. The major difference is the directory structure in the repo.</p>\n<p>7a-1. Hexo by default publishes all static assets to <code>public</code>. However, Github by default expects stuff to be under root. That means, with this setup, the repo needs to be rooted at <code>public</code>:</p>\n<pre><code>cd public\ngit init\ngit remote add origin &lt;repo url&gt;\ngit pull</code></pre><p>7a-2. Commit all assets:</p>\n<pre><code>git add --all\ngit commit -m &quot;...&quot;\ngit push</code></pre><p>7a-3. If you have a custom domain, set CNAME pointed to your.github.io and remember to disable https enforcement in Github (since SSL will verify host name). Also don’t forget to set the custom url in Github.</p>\n<hr>\n<p>7b-1. If you have a VPS then you can commit all the files and just use github as the storage area since you can do backend url forwarding.</p>\n<figure class=\"highlight nginx\"><figcaption><span>nginx.conf</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"section\">server</span> &#123;</span><br><span class=\"line\">    <span class=\"attribute\">listen</span>       <span class=\"number\">80</span>;</span><br><span class=\"line\">    <span class=\"attribute\">server_name</span>  xcorpion.tech;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"section\">location</span> / &#123;</span><br><span class=\"line\">        <span class=\"attribute\">proxy_set_header</span> Host xcorpion.tech;</span><br><span class=\"line\">        <span class=\"attribute\">proxy_pass</span> http://your.github.io/public/;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>7b-2. If you have a custom domain, set A record pointed to your server IP and similarily, remember to turn off https enforcement. Also don’t forget to set the custom url in Github.</p>\n<h1 id=\"And-that’s-it-Let’s-start-writing\"><a href=\"#And-that’s-it-Let’s-start-writing\" class=\"headerlink\" title=\"And that’s it! Let’s start writing!\"></a>And that’s it! Let’s start writing!</h1><ul>\n<li><p>In your hexo root directory, start a new post:</p>\n<pre><code>hexo new post</code></pre></li>\n<li><p>Preview your post with</p>\n<pre><code>hexo server</code></pre></li>\n<li><p>In preview mode, Hexo will watch your files and automatically update the html. However, it <strong>will not</strong> publish any static assets (those eventually shown in your site). So run this to let hexo do it:</p>\n<pre><code>hexo generate</code></pre></li>\n<li><p>Then commit the changes to your repo (You can automate this with a shell script.)</p>\n</li>\n</ul>"},{"title":"Implementing Regex from Scratch: 1 - The Basics of Regex and Finite Automata","date":"2018-05-13T00:32:25.000Z","_content":"\n# Intro\n\nImplementing a regular expression engine is a fun topic and it can be quite complex.\nUnfortunately most of the tutorials are either too complex to follow,\nor impractical, meaning you can't just read it and build one yourself.\n\nWe are going to fix it in this series.\nOur goal here is not to build a fully-fledged engine\nthat can performantly handle all cases since that's already provided by popular languages,\nbut rather we will try to build a usable one from ground up\nthat can handle a clearly defined set of features.\nThrough this we can get a better understanding\nof how it works and where it can be optimized.\n\n# Index\n\nThere's no concrete plan as of now but I'll update the list as we move on.\n\nThe topics we are going to cover includes basics, lexing, parsing, processing and basic optimization.\n\n# Prerequisites\n\nAs the title says, readers aren't supposed to be equipped with much knowledge\nabout this before reading this as they will be explained and discussed in this post.\n\nHowever since we are building an engine from scratch, this post assumes you:\n\n- Know what a regex is and how to read/write regex\n- Have heard about finite automata/NFA/DFA\n- Know what lexing and parsing mean\n- Basic knowledge about algorithm - e.g. BFS and DFS\n\nIf you don't, you might want to check Wikipedia first to\nfamilarize yourself with those topics.\nIt wouldn't hurt if you don't have deep understanding of those\nbut basic knowledge would help.\n\n# Goal\n\nOur goal is to implement a regex processor that understands\n\n- Basic literal and escaping - `abc123\\?`\n- Alternation - `ab|cd`\n- Quantification - `a?`, `a+`, `a*`, `{1,2}`, `{2}`\n- Grouping - `(ab)+`, `a(b|c)d`\n- Wildcard - `.`\n- Anchors - `^`, `$`\n- Extended characters - `\\d`, `\\w`, `\\s`\n- Character classes - `[a-z]`, `[^a]`\n\nSo it should understand that `^a(b+|[c-z]?)+\\?d.+$` would match `abbcw?ds`\nbut not `bbcw?ds` or `abbcds`.\n\nNotice that there is a lot of features missing here:\nwe don't support non-capturing group `(?:)`,\ncapturing group replacement `$1`,\nnon-greedy matching like `.*?`,\nor any other basic/advanced regex syntax.\nIt's possible to cover those topics but that might make the post too\ncomplex to follow for first-time readers. So we will keep the scope minimal\nif possible and only include topics if time permits.\n\n# The Basics\n\nRegular expression is a typical context-free language,\nwhich means there a finite number of predefined replacement rules\n(or more formally, production rules) that can be applied regardless of context,\nyielding a stable \"converged\" state from the original one.\n\nWhile one can possibly search in text through basic regex using DFS and backtracking,\nthe actual implementation can be very complicated once more features are added in.\nSo in practice, they are typically implemented through state machines, or finite automata.\n\nFor example this would be the finite automata that checks if a string is \"ab\".\n{% asset_img sm.png %}\nAnd this is how it is evaluated in practice.\n\n|        Current State            |            Input Char                  |\n|:-------------------------------:|:--------------------------------------:|\n|          `<Start>`              |                `a`                     |\n|            `s1`                 |                `b`                     |\n|          `<End>`                |              `<EOS>`                   |\n\nIf we reach state `<End>` then the input string is a match, otherwise it's not.\n\nThis one is a deterministic finite automata, or DFA.\nThe nice thing about DFA is that each state transition is determined based on input\nso there's no need to backtrack.\nThis implies you could implement that in code with nothing but\na two dimensional array with on representing the possible states (nodes)\nand the other representing transitions (edges).\n\nHere's how that one will be represented (row -> state, column -> input).\nThe cell value represents the next state (row id) with -1 being invalid.\n\n|     |  a  |  b  |\n|:---:|:---:|:---:|\n|  S  |  1  | -1  |\n|  1  | -1  |  2  |\n|  E  | -1  | -1  |\n\n```python\ntrans_mat = [\n    [1, -1],\n    [-1, 2],\n    [-1, -1]\n]\n```\n\nThen the actual implementation is just a for loop and checks if we are in state E.\n\n# Using NFA to Represent Advanced Regex Syntax\n\nSo if DFA is easy on the implementation side, can we actually implement regex in that?\n\nThe answer is YES. However, it's not intuitive to write down the DFA directly.\nSo let's first take an intermediate step.\n\nSay we need to implement `a|bc`. One intuitive thought is to write down a graph like this:\n{% asset_img example2.png %}\nNotice that we actually introduced epsilon (ε) transition here.\nAn epsilon transition is one that allows for spontanous transition (without consuming input).\nYou might wonder how is that different from just connecting the nodes directly,\nand you are totally right - they are effectively the same. That is called compression, but in\nthis post we are going to focus only on the basics and we'll talk about optimization later.\n\nHowever, this branching causes that this is no longer a DFA but rather a non-deterministic\nfinite automata, or NFA, because the transition from start to the next state is no longer\nuniquely determined. This would make the implementation trickier. There are basically two\nways to simulate an NFA: DFS with backtracking, or Thompson's algorithm, which is somewhat\nlike a BFS.\n\nThere's proof that every NFA has a corresponding DFA. The conversion can be done through\nalgorithm called \"powerset construction\", which we will talk about in later optimization topic.\n\nEven though NFA is not as performant as DFA in terms of implementation, it greatly reduces\nbrainwork to abstract the regex. Below we can see how some of the common syntaxes can be\nrepresented through NFA with the help of ε transition and additional pseudo states.\n","source":"_drafts/Implementing-Regex-from-Scratch-1-The-Basics-of-Regex-and-Finite-Automata.md","raw":"---\ntitle: 'Implementing Regex from Scratch: 1 - The Basics of Regex and Finite Automata'\ndate: 2018-05-12 17:32:25\ntags:\n- regex\n- parsing\n- lexing\n- nfa\n- dfa\n- algorithm\n- python\n---\n\n# Intro\n\nImplementing a regular expression engine is a fun topic and it can be quite complex.\nUnfortunately most of the tutorials are either too complex to follow,\nor impractical, meaning you can't just read it and build one yourself.\n\nWe are going to fix it in this series.\nOur goal here is not to build a fully-fledged engine\nthat can performantly handle all cases since that's already provided by popular languages,\nbut rather we will try to build a usable one from ground up\nthat can handle a clearly defined set of features.\nThrough this we can get a better understanding\nof how it works and where it can be optimized.\n\n# Index\n\nThere's no concrete plan as of now but I'll update the list as we move on.\n\nThe topics we are going to cover includes basics, lexing, parsing, processing and basic optimization.\n\n# Prerequisites\n\nAs the title says, readers aren't supposed to be equipped with much knowledge\nabout this before reading this as they will be explained and discussed in this post.\n\nHowever since we are building an engine from scratch, this post assumes you:\n\n- Know what a regex is and how to read/write regex\n- Have heard about finite automata/NFA/DFA\n- Know what lexing and parsing mean\n- Basic knowledge about algorithm - e.g. BFS and DFS\n\nIf you don't, you might want to check Wikipedia first to\nfamilarize yourself with those topics.\nIt wouldn't hurt if you don't have deep understanding of those\nbut basic knowledge would help.\n\n# Goal\n\nOur goal is to implement a regex processor that understands\n\n- Basic literal and escaping - `abc123\\?`\n- Alternation - `ab|cd`\n- Quantification - `a?`, `a+`, `a*`, `{1,2}`, `{2}`\n- Grouping - `(ab)+`, `a(b|c)d`\n- Wildcard - `.`\n- Anchors - `^`, `$`\n- Extended characters - `\\d`, `\\w`, `\\s`\n- Character classes - `[a-z]`, `[^a]`\n\nSo it should understand that `^a(b+|[c-z]?)+\\?d.+$` would match `abbcw?ds`\nbut not `bbcw?ds` or `abbcds`.\n\nNotice that there is a lot of features missing here:\nwe don't support non-capturing group `(?:)`,\ncapturing group replacement `$1`,\nnon-greedy matching like `.*?`,\nor any other basic/advanced regex syntax.\nIt's possible to cover those topics but that might make the post too\ncomplex to follow for first-time readers. So we will keep the scope minimal\nif possible and only include topics if time permits.\n\n# The Basics\n\nRegular expression is a typical context-free language,\nwhich means there a finite number of predefined replacement rules\n(or more formally, production rules) that can be applied regardless of context,\nyielding a stable \"converged\" state from the original one.\n\nWhile one can possibly search in text through basic regex using DFS and backtracking,\nthe actual implementation can be very complicated once more features are added in.\nSo in practice, they are typically implemented through state machines, or finite automata.\n\nFor example this would be the finite automata that checks if a string is \"ab\".\n{% asset_img sm.png %}\nAnd this is how it is evaluated in practice.\n\n|        Current State            |            Input Char                  |\n|:-------------------------------:|:--------------------------------------:|\n|          `<Start>`              |                `a`                     |\n|            `s1`                 |                `b`                     |\n|          `<End>`                |              `<EOS>`                   |\n\nIf we reach state `<End>` then the input string is a match, otherwise it's not.\n\nThis one is a deterministic finite automata, or DFA.\nThe nice thing about DFA is that each state transition is determined based on input\nso there's no need to backtrack.\nThis implies you could implement that in code with nothing but\na two dimensional array with on representing the possible states (nodes)\nand the other representing transitions (edges).\n\nHere's how that one will be represented (row -> state, column -> input).\nThe cell value represents the next state (row id) with -1 being invalid.\n\n|     |  a  |  b  |\n|:---:|:---:|:---:|\n|  S  |  1  | -1  |\n|  1  | -1  |  2  |\n|  E  | -1  | -1  |\n\n```python\ntrans_mat = [\n    [1, -1],\n    [-1, 2],\n    [-1, -1]\n]\n```\n\nThen the actual implementation is just a for loop and checks if we are in state E.\n\n# Using NFA to Represent Advanced Regex Syntax\n\nSo if DFA is easy on the implementation side, can we actually implement regex in that?\n\nThe answer is YES. However, it's not intuitive to write down the DFA directly.\nSo let's first take an intermediate step.\n\nSay we need to implement `a|bc`. One intuitive thought is to write down a graph like this:\n{% asset_img example2.png %}\nNotice that we actually introduced epsilon (ε) transition here.\nAn epsilon transition is one that allows for spontanous transition (without consuming input).\nYou might wonder how is that different from just connecting the nodes directly,\nand you are totally right - they are effectively the same. That is called compression, but in\nthis post we are going to focus only on the basics and we'll talk about optimization later.\n\nHowever, this branching causes that this is no longer a DFA but rather a non-deterministic\nfinite automata, or NFA, because the transition from start to the next state is no longer\nuniquely determined. This would make the implementation trickier. There are basically two\nways to simulate an NFA: DFS with backtracking, or Thompson's algorithm, which is somewhat\nlike a BFS.\n\nThere's proof that every NFA has a corresponding DFA. The conversion can be done through\nalgorithm called \"powerset construction\", which we will talk about in later optimization topic.\n\nEven though NFA is not as performant as DFA in terms of implementation, it greatly reduces\nbrainwork to abstract the regex. Below we can see how some of the common syntaxes can be\nrepresented through NFA with the help of ε transition and additional pseudo states.\n","slug":"Implementing-Regex-from-Scratch-1-The-Basics-of-Regex-and-Finite-Automata","published":0,"updated":"2025-09-01T22:26:51.284Z","comments":1,"layout":"post","photos":[],"_id":"cmf1pfvsg001o8mmg2cka9po4","content":"<h1 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h1><p>Implementing a regular expression engine is a fun topic and it can be quite complex.<br>Unfortunately most of the tutorials are either too complex to follow,<br>or impractical, meaning you can’t just read it and build one yourself.</p>\n<p>We are going to fix it in this series.<br>Our goal here is not to build a fully-fledged engine<br>that can performantly handle all cases since that’s already provided by popular languages,<br>but rather we will try to build a usable one from ground up<br>that can handle a clearly defined set of features.<br>Through this we can get a better understanding<br>of how it works and where it can be optimized.</p>\n<h1 id=\"Index\"><a href=\"#Index\" class=\"headerlink\" title=\"Index\"></a>Index</h1><p>There’s no concrete plan as of now but I’ll update the list as we move on.</p>\n<p>The topics we are going to cover includes basics, lexing, parsing, processing and basic optimization.</p>\n<h1 id=\"Prerequisites\"><a href=\"#Prerequisites\" class=\"headerlink\" title=\"Prerequisites\"></a>Prerequisites</h1><p>As the title says, readers aren’t supposed to be equipped with much knowledge<br>about this before reading this as they will be explained and discussed in this post.</p>\n<p>However since we are building an engine from scratch, this post assumes you:</p>\n<ul>\n<li>Know what a regex is and how to read/write regex</li>\n<li>Have heard about finite automata/NFA/DFA</li>\n<li>Know what lexing and parsing mean</li>\n<li>Basic knowledge about algorithm - e.g. BFS and DFS</li>\n</ul>\n<p>If you don’t, you might want to check Wikipedia first to<br>familarize yourself with those topics.<br>It wouldn’t hurt if you don’t have deep understanding of those<br>but basic knowledge would help.</p>\n<h1 id=\"Goal\"><a href=\"#Goal\" class=\"headerlink\" title=\"Goal\"></a>Goal</h1><p>Our goal is to implement a regex processor that understands</p>\n<ul>\n<li>Basic literal and escaping - <code>abc123\\?</code></li>\n<li>Alternation - <code>ab|cd</code></li>\n<li>Quantification - <code>a?</code>, <code>a+</code>, <code>a*</code>, <code>&#123;1,2&#125;</code>, <code>&#123;2&#125;</code></li>\n<li>Grouping - <code>(ab)+</code>, <code>a(b|c)d</code></li>\n<li>Wildcard - <code>.</code></li>\n<li>Anchors - <code>^</code>, <code>$</code></li>\n<li>Extended characters - <code>\\d</code>, <code>\\w</code>, <code>\\s</code></li>\n<li>Character classes - <code>[a-z]</code>, <code>[^a]</code></li>\n</ul>\n<p>So it should understand that <code>^a(b+|[c-z]?)+\\?d.+$</code> would match <code>abbcw?ds</code><br>but not <code>bbcw?ds</code> or <code>abbcds</code>.</p>\n<p>Notice that there is a lot of features missing here:<br>we don’t support non-capturing group <code>(?:)</code>,<br>capturing group replacement <code>$1</code>,<br>non-greedy matching like <code>.*?</code>,<br>or any other basic/advanced regex syntax.<br>It’s possible to cover those topics but that might make the post too<br>complex to follow for first-time readers. So we will keep the scope minimal<br>if possible and only include topics if time permits.</p>\n<h1 id=\"The-Basics\"><a href=\"#The-Basics\" class=\"headerlink\" title=\"The Basics\"></a>The Basics</h1><p>Regular expression is a typical context-free language,<br>which means there a finite number of predefined replacement rules<br>(or more formally, production rules) that can be applied regardless of context,<br>yielding a stable “converged” state from the original one.</p>\n<p>While one can possibly search in text through basic regex using DFS and backtracking,<br>the actual implementation can be very complicated once more features are added in.<br>So in practice, they are typically implemented through state machines, or finite automata.</p>\n<p>For example this would be the finite automata that checks if a string is “ab”.</p>\n\n<p>And this is how it is evaluated in practice.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Current State</th>\n<th align=\"center\">Input Char</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"><code>&lt;Start&gt;</code></td>\n<td align=\"center\"><code>a</code></td>\n</tr>\n<tr>\n<td align=\"center\"><code>s1</code></td>\n<td align=\"center\"><code>b</code></td>\n</tr>\n<tr>\n<td align=\"center\"><code>&lt;End&gt;</code></td>\n<td align=\"center\"><code>&lt;EOS&gt;</code></td>\n</tr>\n</tbody></table>\n<p>If we reach state <code>&lt;End&gt;</code> then the input string is a match, otherwise it’s not.</p>\n<p>This one is a deterministic finite automata, or DFA.<br>The nice thing about DFA is that each state transition is determined based on input<br>so there’s no need to backtrack.<br>This implies you could implement that in code with nothing but<br>a two dimensional array with on representing the possible states (nodes)<br>and the other representing transitions (edges).</p>\n<p>Here’s how that one will be represented (row -&gt; state, column -&gt; input).<br>The cell value represents the next state (row id) with -1 being invalid.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">a</th>\n<th align=\"center\">b</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">S</td>\n<td align=\"center\">1</td>\n<td align=\"center\">-1</td>\n</tr>\n<tr>\n<td align=\"center\">1</td>\n<td align=\"center\">-1</td>\n<td align=\"center\">2</td>\n</tr>\n<tr>\n<td align=\"center\">E</td>\n<td align=\"center\">-1</td>\n<td align=\"center\">-1</td>\n</tr>\n</tbody></table>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trans_mat = [</span><br><span class=\"line\">    [<span class=\"number\">1</span>, -<span class=\"number\">1</span>],</span><br><span class=\"line\">    [-<span class=\"number\">1</span>, <span class=\"number\">2</span>],</span><br><span class=\"line\">    [-<span class=\"number\">1</span>, -<span class=\"number\">1</span>]</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n\n<p>Then the actual implementation is just a for loop and checks if we are in state E.</p>\n<h1 id=\"Using-NFA-to-Represent-Advanced-Regex-Syntax\"><a href=\"#Using-NFA-to-Represent-Advanced-Regex-Syntax\" class=\"headerlink\" title=\"Using NFA to Represent Advanced Regex Syntax\"></a>Using NFA to Represent Advanced Regex Syntax</h1><p>So if DFA is easy on the implementation side, can we actually implement regex in that?</p>\n<p>The answer is YES. However, it’s not intuitive to write down the DFA directly.<br>So let’s first take an intermediate step.</p>\n<p>Say we need to implement <code>a|bc</code>. One intuitive thought is to write down a graph like this:</p>\n\n<p>Notice that we actually introduced epsilon (ε) transition here.<br>An epsilon transition is one that allows for spontanous transition (without consuming input).<br>You might wonder how is that different from just connecting the nodes directly,<br>and you are totally right - they are effectively the same. That is called compression, but in<br>this post we are going to focus only on the basics and we’ll talk about optimization later.</p>\n<p>However, this branching causes that this is no longer a DFA but rather a non-deterministic<br>finite automata, or NFA, because the transition from start to the next state is no longer<br>uniquely determined. This would make the implementation trickier. There are basically two<br>ways to simulate an NFA: DFS with backtracking, or Thompson’s algorithm, which is somewhat<br>like a BFS.</p>\n<p>There’s proof that every NFA has a corresponding DFA. The conversion can be done through<br>algorithm called “powerset construction”, which we will talk about in later optimization topic.</p>\n<p>Even though NFA is not as performant as DFA in terms of implementation, it greatly reduces<br>brainwork to abstract the regex. Below we can see how some of the common syntaxes can be<br>represented through NFA with the help of ε transition and additional pseudo states.</p>\n","thumbnailImageUrl":null,"excerpt":"","more":"<h1 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h1><p>Implementing a regular expression engine is a fun topic and it can be quite complex.<br>Unfortunately most of the tutorials are either too complex to follow,<br>or impractical, meaning you can’t just read it and build one yourself.</p>\n<p>We are going to fix it in this series.<br>Our goal here is not to build a fully-fledged engine<br>that can performantly handle all cases since that’s already provided by popular languages,<br>but rather we will try to build a usable one from ground up<br>that can handle a clearly defined set of features.<br>Through this we can get a better understanding<br>of how it works and where it can be optimized.</p>\n<h1 id=\"Index\"><a href=\"#Index\" class=\"headerlink\" title=\"Index\"></a>Index</h1><p>There’s no concrete plan as of now but I’ll update the list as we move on.</p>\n<p>The topics we are going to cover includes basics, lexing, parsing, processing and basic optimization.</p>\n<h1 id=\"Prerequisites\"><a href=\"#Prerequisites\" class=\"headerlink\" title=\"Prerequisites\"></a>Prerequisites</h1><p>As the title says, readers aren’t supposed to be equipped with much knowledge<br>about this before reading this as they will be explained and discussed in this post.</p>\n<p>However since we are building an engine from scratch, this post assumes you:</p>\n<ul>\n<li>Know what a regex is and how to read/write regex</li>\n<li>Have heard about finite automata/NFA/DFA</li>\n<li>Know what lexing and parsing mean</li>\n<li>Basic knowledge about algorithm - e.g. BFS and DFS</li>\n</ul>\n<p>If you don’t, you might want to check Wikipedia first to<br>familarize yourself with those topics.<br>It wouldn’t hurt if you don’t have deep understanding of those<br>but basic knowledge would help.</p>\n<h1 id=\"Goal\"><a href=\"#Goal\" class=\"headerlink\" title=\"Goal\"></a>Goal</h1><p>Our goal is to implement a regex processor that understands</p>\n<ul>\n<li>Basic literal and escaping - <code>abc123\\?</code></li>\n<li>Alternation - <code>ab|cd</code></li>\n<li>Quantification - <code>a?</code>, <code>a+</code>, <code>a*</code>, <code>&#123;1,2&#125;</code>, <code>&#123;2&#125;</code></li>\n<li>Grouping - <code>(ab)+</code>, <code>a(b|c)d</code></li>\n<li>Wildcard - <code>.</code></li>\n<li>Anchors - <code>^</code>, <code>$</code></li>\n<li>Extended characters - <code>\\d</code>, <code>\\w</code>, <code>\\s</code></li>\n<li>Character classes - <code>[a-z]</code>, <code>[^a]</code></li>\n</ul>\n<p>So it should understand that <code>^a(b+|[c-z]?)+\\?d.+$</code> would match <code>abbcw?ds</code><br>but not <code>bbcw?ds</code> or <code>abbcds</code>.</p>\n<p>Notice that there is a lot of features missing here:<br>we don’t support non-capturing group <code>(?:)</code>,<br>capturing group replacement <code>$1</code>,<br>non-greedy matching like <code>.*?</code>,<br>or any other basic/advanced regex syntax.<br>It’s possible to cover those topics but that might make the post too<br>complex to follow for first-time readers. So we will keep the scope minimal<br>if possible and only include topics if time permits.</p>\n<h1 id=\"The-Basics\"><a href=\"#The-Basics\" class=\"headerlink\" title=\"The Basics\"></a>The Basics</h1><p>Regular expression is a typical context-free language,<br>which means there a finite number of predefined replacement rules<br>(or more formally, production rules) that can be applied regardless of context,<br>yielding a stable “converged” state from the original one.</p>\n<p>While one can possibly search in text through basic regex using DFS and backtracking,<br>the actual implementation can be very complicated once more features are added in.<br>So in practice, they are typically implemented through state machines, or finite automata.</p>\n<p>For example this would be the finite automata that checks if a string is “ab”.</p>\n\n<p>And this is how it is evaluated in practice.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Current State</th>\n<th align=\"center\">Input Char</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"><code>&lt;Start&gt;</code></td>\n<td align=\"center\"><code>a</code></td>\n</tr>\n<tr>\n<td align=\"center\"><code>s1</code></td>\n<td align=\"center\"><code>b</code></td>\n</tr>\n<tr>\n<td align=\"center\"><code>&lt;End&gt;</code></td>\n<td align=\"center\"><code>&lt;EOS&gt;</code></td>\n</tr>\n</tbody></table>\n<p>If we reach state <code>&lt;End&gt;</code> then the input string is a match, otherwise it’s not.</p>\n<p>This one is a deterministic finite automata, or DFA.<br>The nice thing about DFA is that each state transition is determined based on input<br>so there’s no need to backtrack.<br>This implies you could implement that in code with nothing but<br>a two dimensional array with on representing the possible states (nodes)<br>and the other representing transitions (edges).</p>\n<p>Here’s how that one will be represented (row -&gt; state, column -&gt; input).<br>The cell value represents the next state (row id) with -1 being invalid.</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\"></th>\n<th align=\"center\">a</th>\n<th align=\"center\">b</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">S</td>\n<td align=\"center\">1</td>\n<td align=\"center\">-1</td>\n</tr>\n<tr>\n<td align=\"center\">1</td>\n<td align=\"center\">-1</td>\n<td align=\"center\">2</td>\n</tr>\n<tr>\n<td align=\"center\">E</td>\n<td align=\"center\">-1</td>\n<td align=\"center\">-1</td>\n</tr>\n</tbody></table>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trans_mat = [</span><br><span class=\"line\">    [<span class=\"number\">1</span>, -<span class=\"number\">1</span>],</span><br><span class=\"line\">    [-<span class=\"number\">1</span>, <span class=\"number\">2</span>],</span><br><span class=\"line\">    [-<span class=\"number\">1</span>, -<span class=\"number\">1</span>]</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n\n<p>Then the actual implementation is just a for loop and checks if we are in state E.</p>\n<h1 id=\"Using-NFA-to-Represent-Advanced-Regex-Syntax\"><a href=\"#Using-NFA-to-Represent-Advanced-Regex-Syntax\" class=\"headerlink\" title=\"Using NFA to Represent Advanced Regex Syntax\"></a>Using NFA to Represent Advanced Regex Syntax</h1><p>So if DFA is easy on the implementation side, can we actually implement regex in that?</p>\n<p>The answer is YES. However, it’s not intuitive to write down the DFA directly.<br>So let’s first take an intermediate step.</p>\n<p>Say we need to implement <code>a|bc</code>. One intuitive thought is to write down a graph like this:</p>\n\n<p>Notice that we actually introduced epsilon (ε) transition here.<br>An epsilon transition is one that allows for spontanous transition (without consuming input).<br>You might wonder how is that different from just connecting the nodes directly,<br>and you are totally right - they are effectively the same. That is called compression, but in<br>this post we are going to focus only on the basics and we’ll talk about optimization later.</p>\n<p>However, this branching causes that this is no longer a DFA but rather a non-deterministic<br>finite automata, or NFA, because the transition from start to the next state is no longer<br>uniquely determined. This would make the implementation trickier. There are basically two<br>ways to simulate an NFA: DFS with backtracking, or Thompson’s algorithm, which is somewhat<br>like a BFS.</p>\n<p>There’s proof that every NFA has a corresponding DFA. The conversion can be done through<br>algorithm called “powerset construction”, which we will talk about in later optimization topic.</p>\n<p>Even though NFA is not as performant as DFA in terms of implementation, it greatly reduces<br>brainwork to abstract the regex. Below we can see how some of the common syntaxes can be<br>represented through NFA with the help of ε transition and additional pseudo states.</p>\n"},{"title":"The Spec Mess: Journey to Enable M1 Pro + 2 2K-monitors @ 100Hz over One Single Thunderbolt Cable","date":"2025-09-01T23:44:36.000Z","prompts":"Write a blog post with the title\n\n\"The Spec Mess: Journey to Enable M1 Pro + 2 2K-monitors @ 100Hz w/ One Single Thunderbolt Cable\"\n\nStart with the intro:\n\nRecently I've been looking into giving my WFH setup some upgrade. I already have a 38inch AW3821DW monitor with a spec of 3800x1600 @ 144Hz, connected to the M1 Pro Macbook Pro via a TB3 dock (StarTech TB3DOCK2DPPD that I bought years ago pre-pandemic). I'm trying to add a side monitor to it with likely a QHD spec (2560x1440) @ 100Hz.\nThen I suddenly realized one thing, if I want to keep my connection topology as is (wire up all devices to my dock and connect it to the Mac using a single TB cable) - would that work?\n\nThen breakdown the sections as follows:\n1. RGB 444 vs YCbCr422\n2. The bandwidth calculation\n3. DP 1.4 vs 1.2 and relationship with Thunderbolt version\n4. M1 Pro limitation and the need for downstream thunderbolt port\n5. Summary Put the final output into a code block with raw markdown to avoid it getting formatted on the fly.\n\nList all the references in the final section like\n```\n## References\n[1] [XX](https://yy)\n[2] ...\n```\n","_content":"\nRecently I've been looking into giving my WFH setup some upgrade.\nI already have a 38inch AW3821DW monitor running **3800x1600 @ 144Hz**, connected to the **M1 Pro Macbook Pro** via a **TB3 dock** (StarTech TB3DOCK2DPPD that I bought years ago pre-pandemic). I'm trying to add a side monitor to it with likely a QHD spec **(2560x1440) @ 100Hz**.\n\n{% mermaid %}\ngraph TD;\n    M1[M1 Pro]-->TBDock[TB Dock]\n    TBDock-->AW3821DW[AW3821DW 3800x1600 @ 144Hz]\n    TBDock-->QHD[QHD 2560x1440 @ 100Hz]\n{% endmermaid %}\n\nThen I suddenly realized one thing, if I want to keep my connection topology as is (wire up all devices to my dock and connect it to the Mac using a single TB cable) - would that work?\n\nIt turns out it's not that straightforward to average users. And here's why.\n\n---\n\n## 1) RGB 4:4:4 vs YCbCr 4:2:2 Chroma Subsampling\n\nBefore doing math, we first need to check the *signal* we want to drive:\n\n- **RGB/YCbCr 4:4:4, 8-bit per channel (bpc)** → **24 bits/pixel**. This is the “no-compromise” desktop signal—full chroma on every pixel. Text looks crisp.\n- **YCbCr 4:2:2, 8-bpc** → the chroma (Cb/Cr) is shared between two horizontal pixels, averaging **16 bits/pixel** (≈ 33% less data than 4:4:4). Saves bandwidth, but fine colored edges and UI text soften.\n- **YCbCr 4:2:0, 8-bpc** → **12 bits/pixel** (≈ 50% less than 4:4:4). Great for video, not for desktop use.\n- **HDR / 10-bpc** pushes each of those up by 25% (e.g., RGB 4:4:4 becomes **30 bits/pixel**).\n\nFor SDR productivity monitors, we should **ALWAYS** prefer **RGB 4:4:4** (or YCbCr 4:4:4) at 8-bpc for clear text.\nAnything doing chroma subsampling (422) is a no-go as text would have blurred/colored edges. Period.\n\n---\n\n## 2) The bandwidth calculation\n\nFor uncompressed links (DisplayPort/HDMI without DSC), a solid back-of-the-envelope is:\n\n$$\n\\textbf{Signal rate} \\approx \\text{(Horizontal px)} \\times \\text{(Vertical px)} \\times \\text{(Refresh Hz)} \\times \\text{(bits/pixel)}.\n$$\n\nThis yields the **active video** payload. Real links also carry blanking intervals and link-layer overhead. Using modern reduced-blanking timings, a **5–10% headroom** usually suffices for comparisons; standards bodies quote separate “link rates” that already include line coding overhead.\n\nLet’s compute the key cases (all at 8-bpc unless noted):\n\n- **One QHD (2560×1440) @ 100 Hz, 4:4:4**  \n  $$2560 \\times 1440 \\times 100 \\times 24 \\;=\\; \\mathbf{8.85\\ Gbps}$$\n  With 10-bpc (30 bpp): **11.06 Gbps**.  \n  With 4:2:2 (16 bpp): **5.90 Gbps**.\n\n- **Your ultrawide (3840×1600), 4:4:4**  \n  - @ **100 Hz**\n    $$3840 \\times 1600 \\times 100 \\times 24 = \\mathbf{14.75\\ Gbps} $$\n  - @ **120 Hz** **17.69 Gbps** (already nudging past DP 1.2’s effective limit once overhead is considered).  \n  - @ **144 Hz**: **21.23 Gbps** (firmly beyond DP 1.2; needs DP 1.4/HBR3 and/or DSC support end-to-end).\n\n- **Two monitors together**:  \n  - **3840×1600 @ 100 Hz** + **2560×1440 @ 100 Hz** (both 8-bpc 4:4:4) →\n    $$14.75 + 8.85 \\approx \\mathbf{23.6\\ Gbps}$$\n    This is in the same ballpark as **dual 4K60 8-bpc** (≈ 23.9 Gbps active), which many Thunderbolt 3 docks explicitly support.\n\n**Interpreting against link limits**\n\n- **DisplayPort 1.2 (HBR2)**: Max **17.28 Gbps** payload per DP link. One 2560×1440@100 (8-bpc 4:4:4) fits comfortably. One 3840×1600@100 fits. **3840×1600@120 does not**, and @144 certainly does not.  \n- **DisplayPort 1.4 (HBR3)**: Max **25.92 Gbps** payload per DP link. Enough for 3840×1600@120/144 uncompressed in many timing modes, or with DSC when needed.\n\n---\n\n## 3) DP 1.4 vs DP 1.2—and how that relates to Thunderbolt versions\n\n- **DP 1.2 (HBR2)** tops out at 5.4Gbps/lane x 4 - **21.6 Gbps raw / 17.28 Gbps effective** payload.  \n- **DP 1.4 (HBR3)** increases the lane rate to 8.1Gbps/lane -> **32.4 Gbps raw / 25.92 Gbps effective** and adds features like **DSC 1.2** for higher-than-link-rate modes.\n- **Thunderbolt 3** originally shipped on **Alpine Ridge** controllers that **tunnel DP 1.2** only. Later **Titan Ridge** TB3 silicon can **tunnel DP 1.4**.  \n- **Thunderbolt 4** is built on **USB4 v1**; it **mandates** support for **two 4K displays or one 8K display**, functionally aligning with **DP 1.4/HBR3** tunneling end-to-end. (TB4 does not raise the nominal 40 Gbps link rate; it raises the *minimums* OEMs must deliver.)\n\nHowever, the **StarTech TB3DOCK2DPPD** is an older **Alpine Ridge (JHL6540)** design—i.e., **DP 1.2-only** on its display paths. It still handles **dual-display** by presenting **two DP sinks** to the host TB controller (one on the rear DisplayPort jack, and a **downstream TB3 port**). But each DP hop is capped at **DP 1.2** timings.\n\n---\n\n## 4) M1 Pro limitations and why a **downstream Thunderbolt port** matters\n\nAt this moment, it's roughly clear that a TB3 dock is not going to be sufficient bandwidth wise.\n\nHowever there's a bit more nuance here (and why we actually need a higher-end dock with a **downstream** TB4 port):\n\n- **M1 Pro MacBook Pro (2021)** supports **up to two external displays** over Thunderbolt (each up to 6K60). That’s a *hard* GPU/display-engine limit on count, not just raw bandwidth.\n- MacOS does not support DisplayPort MST for extended desktops. In plain English: you can’t split one DP lane group into two independent desktops via an MST hub and expect macOS to see two displays—it will mirror them.  \n  **Implication:** to run **two** external monitors over **one cable**, we need a **Thunderbolt dock** that exposes **two independent DP tunnels** (e.g., one physical DP output **plus** a **downstream TB** port for the second display via USB-C→DP).\n\nThe **TB3DOCK2DPPD** does exactly that: one native DP port **and** one **downstream TB3** port intended to carry the second display. The constraint, again, is that both of those hops are **DP 1.2** only.\n\nA lot of the lower-end docks, on the other hand, **do NOT carry downstream TB ports** and hence not suited for such setup.\n\n**Putting it together:**  \n- To keep the single-cable topology, we should use the dock’s **DP** for one monitor and the **downstream TB3→USB-C/DP** for the other.\n- We also need a **DP 1.4-capable TB3/TB4 dock** (Titan Ridge, Goshen Ridge/TB4) to fulfill the bandwidth requirements for the 2 monitors at high refresh rate.\n\n---\n\n## 5) Summary\n\n- **Yes, the single-cable topology can work**—with caveats. On the **Alpine Ridge TB3 dock (DP 1.2)**, run the **AW3821DW at 100 Hz** and the new **2560×1440 at 100 Hz**, both at **8-bpc RGB 4:4:4**.  \n- The dock will not do **3840×1600 @ 144 Hz** concurrently with a high-refresh second display; DP 1.2 per-link bandwidth is the blocker.\n- Because **macOS lacks MST-Extended**, we also need to ensure the dock provides a **downstream Thunderbolt port**.\n\n---\n\n## References\n[1] Apple — *MacBook Pro (14-inch, 2021) Tech Specs* (external display support) – https://support.apple.com/en-us/111902  \n[2] Dell — *Alienware AW3821DW product page* (3840×1600 @ 144 Hz over DP) – https://www.dell.com/en-us/shop/alienware-38-curved-gaming-monitor-aw3821dw/apd/210-axvg/monitors-monitor-accessories  \n[3] RTINGS — *Chroma Subsampling: 4:4:4 vs 4:2:2 vs 4:2:0* – https://www.rtings.com/tv/learn/chroma-subsampling  \n[4] Eaton/Tripp Lite — *DisplayPort explained* (8b/10b vs 128b/132b; effective payloads) – https://www.eaton.com/us/en-us/products/backup-power-ups-surge-it-power-distribution/backup-power-ups-it-power-distribution-resources/cpdi-vertical-marketing/displayport-explained.html  \n[5] Cable Matters — *DisplayPort 1.4 vs 1.2: What’s the Difference?* (21.6 vs 32.4 Gbps raw; 17.28 vs 25.92 Gbps payload) – https://www.cablematters.com/Blog/DisplayPort/displayport-1-4-vs-1-2  \n[6] Wikipedia — *DisplayPort* (TB3 Alpine Ridge DP 1.2 vs Titan Ridge DP 1.4) – https://en.wikipedia.org/wiki/DisplayPort  \n[7] StarTech — *TB3DOCK2DPPD Manual* (identifies **JHL6540 Alpine Ridge** chipset, DP 1.2 heritage) – https://sgcdn.startech.com/005329/media/sets/tb3dock2dppd_manual/tb3dock2dppd_tb3dock2dppu_thunderbolt_3_dock.pdf  \n[8] StarTech — *TB3DOCK2DPPD Datasheet* (dual-4K60 via DP + downstream TB3) – https://media.startech.com/cms/pdfs/tb3dock2dppd_datasheet.pdf  \n[9] Plugable KB — *MST on macOS (Extended not supported)* – https://kb.plugable.com/understanding-mst-multi-display-setups-with-windows-macos-and-displayport  \n[10] Intel — *Thunderbolt 3 vs Thunderbolt 4* (TB4 mandates dual 4K or one 8K) – https://www.intel.com/content/www/us/en/architecture-and-technology/thunderbolt/thunderbolt-3-vs-4.html  \n[11] Wikipedia — *USB4* (DP 1.4a tunneling in USB4 v1; DP 2.1 in USB4 v2) – https://en.wikipedia.org/wiki/USB4\n","source":"_posts/Journey-to-enable-M1-pro-single-thunderbolt.md","raw":"---\ntitle: \"The Spec Mess: Journey to Enable M1 Pro + 2 2K-monitors @ 100Hz over One Single Thunderbolt Cable\"\ndate: 2025-09-01 16:44:36\ntags:\n    - pcparts\n    - setup\nprompts: |\n    Write a blog post with the title\n\n    \"The Spec Mess: Journey to Enable M1 Pro + 2 2K-monitors @ 100Hz w/ One Single Thunderbolt Cable\"\n    \n    Start with the intro:\n    \n    Recently I've been looking into giving my WFH setup some upgrade. I already have a 38inch AW3821DW monitor with a spec of 3800x1600 @ 144Hz, connected to the M1 Pro Macbook Pro via a TB3 dock (StarTech TB3DOCK2DPPD that I bought years ago pre-pandemic). I'm trying to add a side monitor to it with likely a QHD spec (2560x1440) @ 100Hz.\n    Then I suddenly realized one thing, if I want to keep my connection topology as is (wire up all devices to my dock and connect it to the Mac using a single TB cable) - would that work?\n    \n    Then breakdown the sections as follows:\n    1. RGB 444 vs YCbCr422\n    2. The bandwidth calculation\n    3. DP 1.4 vs 1.2 and relationship with Thunderbolt version\n    4. M1 Pro limitation and the need for downstream thunderbolt port\n    5. Summary Put the final output into a code block with raw markdown to avoid it getting formatted on the fly.\n    \n    List all the references in the final section like\n    ```\n    ## References\n    [1] [XX](https://yy)\n    [2] ...\n    ```\n---\n\nRecently I've been looking into giving my WFH setup some upgrade.\nI already have a 38inch AW3821DW monitor running **3800x1600 @ 144Hz**, connected to the **M1 Pro Macbook Pro** via a **TB3 dock** (StarTech TB3DOCK2DPPD that I bought years ago pre-pandemic). I'm trying to add a side monitor to it with likely a QHD spec **(2560x1440) @ 100Hz**.\n\n{% mermaid %}\ngraph TD;\n    M1[M1 Pro]-->TBDock[TB Dock]\n    TBDock-->AW3821DW[AW3821DW 3800x1600 @ 144Hz]\n    TBDock-->QHD[QHD 2560x1440 @ 100Hz]\n{% endmermaid %}\n\nThen I suddenly realized one thing, if I want to keep my connection topology as is (wire up all devices to my dock and connect it to the Mac using a single TB cable) - would that work?\n\nIt turns out it's not that straightforward to average users. And here's why.\n\n---\n\n## 1) RGB 4:4:4 vs YCbCr 4:2:2 Chroma Subsampling\n\nBefore doing math, we first need to check the *signal* we want to drive:\n\n- **RGB/YCbCr 4:4:4, 8-bit per channel (bpc)** → **24 bits/pixel**. This is the “no-compromise” desktop signal—full chroma on every pixel. Text looks crisp.\n- **YCbCr 4:2:2, 8-bpc** → the chroma (Cb/Cr) is shared between two horizontal pixels, averaging **16 bits/pixel** (≈ 33% less data than 4:4:4). Saves bandwidth, but fine colored edges and UI text soften.\n- **YCbCr 4:2:0, 8-bpc** → **12 bits/pixel** (≈ 50% less than 4:4:4). Great for video, not for desktop use.\n- **HDR / 10-bpc** pushes each of those up by 25% (e.g., RGB 4:4:4 becomes **30 bits/pixel**).\n\nFor SDR productivity monitors, we should **ALWAYS** prefer **RGB 4:4:4** (or YCbCr 4:4:4) at 8-bpc for clear text.\nAnything doing chroma subsampling (422) is a no-go as text would have blurred/colored edges. Period.\n\n---\n\n## 2) The bandwidth calculation\n\nFor uncompressed links (DisplayPort/HDMI without DSC), a solid back-of-the-envelope is:\n\n$$\n\\textbf{Signal rate} \\approx \\text{(Horizontal px)} \\times \\text{(Vertical px)} \\times \\text{(Refresh Hz)} \\times \\text{(bits/pixel)}.\n$$\n\nThis yields the **active video** payload. Real links also carry blanking intervals and link-layer overhead. Using modern reduced-blanking timings, a **5–10% headroom** usually suffices for comparisons; standards bodies quote separate “link rates” that already include line coding overhead.\n\nLet’s compute the key cases (all at 8-bpc unless noted):\n\n- **One QHD (2560×1440) @ 100 Hz, 4:4:4**  \n  $$2560 \\times 1440 \\times 100 \\times 24 \\;=\\; \\mathbf{8.85\\ Gbps}$$\n  With 10-bpc (30 bpp): **11.06 Gbps**.  \n  With 4:2:2 (16 bpp): **5.90 Gbps**.\n\n- **Your ultrawide (3840×1600), 4:4:4**  \n  - @ **100 Hz**\n    $$3840 \\times 1600 \\times 100 \\times 24 = \\mathbf{14.75\\ Gbps} $$\n  - @ **120 Hz** **17.69 Gbps** (already nudging past DP 1.2’s effective limit once overhead is considered).  \n  - @ **144 Hz**: **21.23 Gbps** (firmly beyond DP 1.2; needs DP 1.4/HBR3 and/or DSC support end-to-end).\n\n- **Two monitors together**:  \n  - **3840×1600 @ 100 Hz** + **2560×1440 @ 100 Hz** (both 8-bpc 4:4:4) →\n    $$14.75 + 8.85 \\approx \\mathbf{23.6\\ Gbps}$$\n    This is in the same ballpark as **dual 4K60 8-bpc** (≈ 23.9 Gbps active), which many Thunderbolt 3 docks explicitly support.\n\n**Interpreting against link limits**\n\n- **DisplayPort 1.2 (HBR2)**: Max **17.28 Gbps** payload per DP link. One 2560×1440@100 (8-bpc 4:4:4) fits comfortably. One 3840×1600@100 fits. **3840×1600@120 does not**, and @144 certainly does not.  \n- **DisplayPort 1.4 (HBR3)**: Max **25.92 Gbps** payload per DP link. Enough for 3840×1600@120/144 uncompressed in many timing modes, or with DSC when needed.\n\n---\n\n## 3) DP 1.4 vs DP 1.2—and how that relates to Thunderbolt versions\n\n- **DP 1.2 (HBR2)** tops out at 5.4Gbps/lane x 4 - **21.6 Gbps raw / 17.28 Gbps effective** payload.  \n- **DP 1.4 (HBR3)** increases the lane rate to 8.1Gbps/lane -> **32.4 Gbps raw / 25.92 Gbps effective** and adds features like **DSC 1.2** for higher-than-link-rate modes.\n- **Thunderbolt 3** originally shipped on **Alpine Ridge** controllers that **tunnel DP 1.2** only. Later **Titan Ridge** TB3 silicon can **tunnel DP 1.4**.  \n- **Thunderbolt 4** is built on **USB4 v1**; it **mandates** support for **two 4K displays or one 8K display**, functionally aligning with **DP 1.4/HBR3** tunneling end-to-end. (TB4 does not raise the nominal 40 Gbps link rate; it raises the *minimums* OEMs must deliver.)\n\nHowever, the **StarTech TB3DOCK2DPPD** is an older **Alpine Ridge (JHL6540)** design—i.e., **DP 1.2-only** on its display paths. It still handles **dual-display** by presenting **two DP sinks** to the host TB controller (one on the rear DisplayPort jack, and a **downstream TB3 port**). But each DP hop is capped at **DP 1.2** timings.\n\n---\n\n## 4) M1 Pro limitations and why a **downstream Thunderbolt port** matters\n\nAt this moment, it's roughly clear that a TB3 dock is not going to be sufficient bandwidth wise.\n\nHowever there's a bit more nuance here (and why we actually need a higher-end dock with a **downstream** TB4 port):\n\n- **M1 Pro MacBook Pro (2021)** supports **up to two external displays** over Thunderbolt (each up to 6K60). That’s a *hard* GPU/display-engine limit on count, not just raw bandwidth.\n- MacOS does not support DisplayPort MST for extended desktops. In plain English: you can’t split one DP lane group into two independent desktops via an MST hub and expect macOS to see two displays—it will mirror them.  \n  **Implication:** to run **two** external monitors over **one cable**, we need a **Thunderbolt dock** that exposes **two independent DP tunnels** (e.g., one physical DP output **plus** a **downstream TB** port for the second display via USB-C→DP).\n\nThe **TB3DOCK2DPPD** does exactly that: one native DP port **and** one **downstream TB3** port intended to carry the second display. The constraint, again, is that both of those hops are **DP 1.2** only.\n\nA lot of the lower-end docks, on the other hand, **do NOT carry downstream TB ports** and hence not suited for such setup.\n\n**Putting it together:**  \n- To keep the single-cable topology, we should use the dock’s **DP** for one monitor and the **downstream TB3→USB-C/DP** for the other.\n- We also need a **DP 1.4-capable TB3/TB4 dock** (Titan Ridge, Goshen Ridge/TB4) to fulfill the bandwidth requirements for the 2 monitors at high refresh rate.\n\n---\n\n## 5) Summary\n\n- **Yes, the single-cable topology can work**—with caveats. On the **Alpine Ridge TB3 dock (DP 1.2)**, run the **AW3821DW at 100 Hz** and the new **2560×1440 at 100 Hz**, both at **8-bpc RGB 4:4:4**.  \n- The dock will not do **3840×1600 @ 144 Hz** concurrently with a high-refresh second display; DP 1.2 per-link bandwidth is the blocker.\n- Because **macOS lacks MST-Extended**, we also need to ensure the dock provides a **downstream Thunderbolt port**.\n\n---\n\n## References\n[1] Apple — *MacBook Pro (14-inch, 2021) Tech Specs* (external display support) – https://support.apple.com/en-us/111902  \n[2] Dell — *Alienware AW3821DW product page* (3840×1600 @ 144 Hz over DP) – https://www.dell.com/en-us/shop/alienware-38-curved-gaming-monitor-aw3821dw/apd/210-axvg/monitors-monitor-accessories  \n[3] RTINGS — *Chroma Subsampling: 4:4:4 vs 4:2:2 vs 4:2:0* – https://www.rtings.com/tv/learn/chroma-subsampling  \n[4] Eaton/Tripp Lite — *DisplayPort explained* (8b/10b vs 128b/132b; effective payloads) – https://www.eaton.com/us/en-us/products/backup-power-ups-surge-it-power-distribution/backup-power-ups-it-power-distribution-resources/cpdi-vertical-marketing/displayport-explained.html  \n[5] Cable Matters — *DisplayPort 1.4 vs 1.2: What’s the Difference?* (21.6 vs 32.4 Gbps raw; 17.28 vs 25.92 Gbps payload) – https://www.cablematters.com/Blog/DisplayPort/displayport-1-4-vs-1-2  \n[6] Wikipedia — *DisplayPort* (TB3 Alpine Ridge DP 1.2 vs Titan Ridge DP 1.4) – https://en.wikipedia.org/wiki/DisplayPort  \n[7] StarTech — *TB3DOCK2DPPD Manual* (identifies **JHL6540 Alpine Ridge** chipset, DP 1.2 heritage) – https://sgcdn.startech.com/005329/media/sets/tb3dock2dppd_manual/tb3dock2dppd_tb3dock2dppu_thunderbolt_3_dock.pdf  \n[8] StarTech — *TB3DOCK2DPPD Datasheet* (dual-4K60 via DP + downstream TB3) – https://media.startech.com/cms/pdfs/tb3dock2dppd_datasheet.pdf  \n[9] Plugable KB — *MST on macOS (Extended not supported)* – https://kb.plugable.com/understanding-mst-multi-display-setups-with-windows-macos-and-displayport  \n[10] Intel — *Thunderbolt 3 vs Thunderbolt 4* (TB4 mandates dual 4K or one 8K) – https://www.intel.com/content/www/us/en/architecture-and-technology/thunderbolt/thunderbolt-3-vs-4.html  \n[11] Wikipedia — *USB4* (DP 1.4a tunneling in USB4 v1; DP 2.1 in USB4 v2) – https://en.wikipedia.org/wiki/USB4\n","slug":"Journey-to-enable-M1-pro-single-thunderbolt","published":1,"updated":"2025-09-02T00:57:29.629Z","_id":"cmf1uau9v00009lmgb49h8mzu","comments":1,"layout":"post","photos":[],"content":"<p>Recently I’ve been looking into giving my WFH setup some upgrade.<br>\nI already have a 38inch AW3821DW monitor running <strong>3800x1600 @ 144Hz</strong>, connected to the <strong>M1 Pro Macbook Pro</strong> via a <strong>TB3 dock</strong> (StarTech TB3DOCK2DPPD that I bought years ago pre-pandemic). I’m trying to add a side monitor to it with likely a QHD spec <strong>(2560x1440) @ 100Hz</strong>.</p>\n<div class=\"mermaid\">\n  graph TD;\n    M1[M1 Pro]-->TBDock[TB Dock]\n    TBDock-->AW3821DW[AW3821DW 3800x1600 @ 144Hz]\n    TBDock-->QHD[QHD 2560x1440 @ 100Hz]\n</div>\n\n<p>Then I suddenly realized one thing, if I want to keep my connection topology as is (wire up all devices to my dock and connect it to the Mac using a single TB cable) - would that work?</p>\n<p>It turns out it’s not that straightforward to average users. And here’s why.</p>\n<hr>\n<h2 id=\"1)-rgb-4%3A4%3A4-vs-ycbcr-4%3A2%3A2-chroma-subsampling\" tabindex=\"-1\">1) RGB 4:4:4 vs YCbCr 4:2:2 Chroma Subsampling</h2>\n<p>Before doing math, we first need to check the <em>signal</em> we want to drive:</p>\n<ul>\n<li><strong>RGB/YCbCr 4:4:4, 8-bit per channel (bpc)</strong> → <strong>24 bits/pixel</strong>. This is the “no-compromise” desktop signal—full chroma on every pixel. Text looks crisp.</li>\n<li><strong>YCbCr 4:2:2, 8-bpc</strong> → the chroma (Cb/Cr) is shared between two horizontal pixels, averaging <strong>16 bits/pixel</strong> (≈ 33% less data than 4:4:4). Saves bandwidth, but fine colored edges and UI text soften.</li>\n<li><strong>YCbCr 4:2:0, 8-bpc</strong> → <strong>12 bits/pixel</strong> (≈ 50% less than 4:4:4). Great for video, not for desktop use.</li>\n<li><strong>HDR / 10-bpc</strong> pushes each of those up by 25% (e.g., RGB 4:4:4 becomes <strong>30 bits/pixel</strong>).</li>\n</ul>\n<p>For SDR productivity monitors, we should <strong>ALWAYS</strong> prefer <strong>RGB 4:4:4</strong> (or YCbCr 4:4:4) at 8-bpc for clear text.<br>\nAnything doing chroma subsampling (422) is a no-go as text would have blurred/colored edges. Period.</p>\n<hr>\n<h2 id=\"2)-the-bandwidth-calculation\" tabindex=\"-1\">2) The bandwidth calculation</h2>\n<p>For uncompressed links (DisplayPort/HDMI without DSC), a solid back-of-the-envelope is:</p>\n<section><eqn><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mrow><mtext mathvariant=\"bold\">Signal</mtext><mtext> </mtext><mtext mathvariant=\"bold\">rate</mtext></mrow><mo>≈</mo><mtext>(Horizontal px)</mtext><mo>×</mo><mtext>(Vertical px)</mtext><mo>×</mo><mtext>(Refresh Hz)</mtext><mo>×</mo><mtext>(bits/pixel)</mtext><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">\n\\textbf{Signal rate} \\approx \\text{(Horizontal px)} \\times \\text{(Vertical px)} \\times \\text{(Refresh Hz)} \\times \\text{(bits/pixel)}.\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord text\"><span class=\"mord textbf\">Signal rate</span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">(Horizontal px)</span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">(Vertical px)</span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">(Refresh Hz)</span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">(bits/pixel)</span></span><span class=\"mord\">.</span></span></span></span></span></eqn></section><p>This yields the <strong>active video</strong> payload. Real links also carry blanking intervals and link-layer overhead. Using modern reduced-blanking timings, a <strong>5–10% headroom</strong> usually suffices for comparisons; standards bodies quote separate “link rates” that already include line coding overhead.</p>\n<p>Let’s compute the key cases (all at 8-bpc unless noted):</p>\n<ul>\n<li>\n<p><strong>One QHD (2560×1440) @ 100 Hz, 4:4:4</strong><br>\n<section><eqn><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mn>2560</mn><mo>×</mo><mn>1440</mn><mo>×</mo><mn>100</mn><mo>×</mo><mn>24</mn><mtext>  </mtext><mo>=</mo><mtext>  </mtext><mrow><mn mathvariant=\"bold\">8.85</mn><mtext> </mtext><mi mathvariant=\"bold\">G</mi><mi mathvariant=\"bold\">b</mi><mi mathvariant=\"bold\">p</mi><mi mathvariant=\"bold\">s</mi></mrow></mrow><annotation encoding=\"application/x-tex\">2560 \\times 1440 \\times 100 \\times 24 \\;=\\; \\mathbf{8.85\\ Gbps}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">2560</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">1440</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">100</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">24</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">8.85</span><span class=\"mspace\"> </span><span class=\"mord mathbf\">Gbps</span></span></span></span></span></span></eqn></section><br>\nWith 10-bpc (30 bpp): <strong>11.06 Gbps</strong>.<br>\nWith 4:2:2 (16 bpp): <strong>5.90 Gbps</strong>.</p>\n</li>\n<li>\n<p><strong>Your ultrawide (3840×1600), 4:4:4</strong></p>\n<ul>\n<li>@ <strong>100 Hz</strong><br>\n<section><eqn><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mn>3840</mn><mo>×</mo><mn>1600</mn><mo>×</mo><mn>100</mn><mo>×</mo><mn>24</mn><mo>=</mo><mrow><mn mathvariant=\"bold\">14.75</mn><mtext> </mtext><mi mathvariant=\"bold\">G</mi><mi mathvariant=\"bold\">b</mi><mi mathvariant=\"bold\">p</mi><mi mathvariant=\"bold\">s</mi></mrow></mrow><annotation encoding=\"application/x-tex\">3840 \\times 1600 \\times 100 \\times 24 = \\mathbf{14.75\\ Gbps} </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">3840</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">1600</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">100</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">24</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">14.75</span><span class=\"mspace\"> </span><span class=\"mord mathbf\">Gbps</span></span></span></span></span></span></eqn></section></li>\n<li>@ <strong>120 Hz</strong> <strong>17.69 Gbps</strong> (already nudging past DP 1.2’s effective limit once overhead is considered).</li>\n<li>@ <strong>144 Hz</strong>: <strong>21.23 Gbps</strong> (firmly beyond DP 1.2; needs DP 1.4/HBR3 and/or DSC support end-to-end).</li>\n</ul>\n</li>\n<li>\n<p><strong>Two monitors together</strong>:</p>\n<ul>\n<li><strong>3840×1600 @ 100 Hz</strong> + <strong>2560×1440 @ 100 Hz</strong> (both 8-bpc 4:4:4) →<br>\n<section><eqn><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mn>14.75</mn><mo>+</mo><mn>8.85</mn><mo>≈</mo><mrow><mn mathvariant=\"bold\">23.6</mn><mtext> </mtext><mi mathvariant=\"bold\">G</mi><mi mathvariant=\"bold\">b</mi><mi mathvariant=\"bold\">p</mi><mi mathvariant=\"bold\">s</mi></mrow></mrow><annotation encoding=\"application/x-tex\">14.75 + 8.85 \\approx \\mathbf{23.6\\ Gbps}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">14.75</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">8.85</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">23.6</span><span class=\"mspace\"> </span><span class=\"mord mathbf\">Gbps</span></span></span></span></span></span></eqn></section><br>\nThis is in the same ballpark as <strong>dual 4K60 8-bpc</strong> (≈ 23.9 Gbps active), which many Thunderbolt 3 docks explicitly support.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Interpreting against link limits</strong></p>\n<ul>\n<li><strong>DisplayPort 1.2 (HBR2)</strong>: Max <strong>17.28 Gbps</strong> payload per DP link. One 2560×1440@100 (8-bpc 4:4:4) fits comfortably. One 3840×1600@100 fits. <strong>3840×1600@120 does not</strong>, and @144 certainly does not.</li>\n<li><strong>DisplayPort 1.4 (HBR3)</strong>: Max <strong>25.92 Gbps</strong> payload per DP link. Enough for 3840×1600@120/144 uncompressed in many timing modes, or with DSC when needed.</li>\n</ul>\n<hr>\n<h2 id=\"3)-dp-1.4-vs-dp-1.2%E2%80%94and-how-that-relates-to-thunderbolt-versions\" tabindex=\"-1\">3) DP 1.4 vs DP 1.2—and how that relates to Thunderbolt versions</h2>\n<ul>\n<li><strong>DP 1.2 (HBR2)</strong> tops out at 5.4Gbps/lane x 4 - <strong>21.6 Gbps raw / 17.28 Gbps effective</strong> payload.</li>\n<li><strong>DP 1.4 (HBR3)</strong> increases the lane rate to 8.1Gbps/lane -&gt; <strong>32.4 Gbps raw / 25.92 Gbps effective</strong> and adds features like <strong>DSC 1.2</strong> for higher-than-link-rate modes.</li>\n<li><strong>Thunderbolt 3</strong> originally shipped on <strong>Alpine Ridge</strong> controllers that <strong>tunnel DP 1.2</strong> only. Later <strong>Titan Ridge</strong> TB3 silicon can <strong>tunnel DP 1.4</strong>.</li>\n<li><strong>Thunderbolt 4</strong> is built on <strong>USB4 v1</strong>; it <strong>mandates</strong> support for <strong>two 4K displays or one 8K display</strong>, functionally aligning with <strong>DP 1.4/HBR3</strong> tunneling end-to-end. (TB4 does not raise the nominal 40 Gbps link rate; it raises the <em>minimums</em> OEMs must deliver.)</li>\n</ul>\n<p>However, the <strong>StarTech TB3DOCK2DPPD</strong> is an older <strong>Alpine Ridge (JHL6540)</strong> design—i.e., <strong>DP 1.2-only</strong> on its display paths. It still handles <strong>dual-display</strong> by presenting <strong>two DP sinks</strong> to the host TB controller (one on the rear DisplayPort jack, and a <strong>downstream TB3 port</strong>). But each DP hop is capped at <strong>DP 1.2</strong> timings.</p>\n<hr>\n<h2 id=\"4)-m1-pro-limitations-and-why-a-downstream-thunderbolt-port-matters\" tabindex=\"-1\">4) M1 Pro limitations and why a <strong>downstream Thunderbolt port</strong> matters</h2>\n<p>At this moment, it’s roughly clear that a TB3 dock is not going to be sufficient bandwidth wise.</p>\n<p>However there’s a bit more nuance here (and why we actually need a higher-end dock with a <strong>downstream</strong> TB4 port):</p>\n<ul>\n<li><strong>M1 Pro MacBook Pro (2021)</strong> supports <strong>up to two external displays</strong> over Thunderbolt (each up to 6K60). That’s a <em>hard</em> GPU/display-engine limit on count, not just raw bandwidth.</li>\n<li>MacOS does not support DisplayPort MST for extended desktops. In plain English: you can’t split one DP lane group into two independent desktops via an MST hub and expect macOS to see two displays—it will mirror them.<br>\n<strong>Implication:</strong> to run <strong>two</strong> external monitors over <strong>one cable</strong>, we need a <strong>Thunderbolt dock</strong> that exposes <strong>two independent DP tunnels</strong> (e.g., one physical DP output <strong>plus</strong> a <strong>downstream TB</strong> port for the second display via USB-C→DP).</li>\n</ul>\n<p>The <strong>TB3DOCK2DPPD</strong> does exactly that: one native DP port <strong>and</strong> one <strong>downstream TB3</strong> port intended to carry the second display. The constraint, again, is that both of those hops are <strong>DP 1.2</strong> only.</p>\n<p>A lot of the lower-end docks, on the other hand, <strong>do NOT carry downstream TB ports</strong> and hence not suited for such setup.</p>\n<p><strong>Putting it together:</strong></p>\n<ul>\n<li>To keep the single-cable topology, we should use the dock’s <strong>DP</strong> for one monitor and the <strong>downstream TB3→USB-C/DP</strong> for the other.</li>\n<li>We also need a <strong>DP 1.4-capable TB3/TB4 dock</strong> (Titan Ridge, Goshen Ridge/TB4) to fulfill the bandwidth requirements for the 2 monitors at high refresh rate.</li>\n</ul>\n<hr>\n<h2 id=\"5)-summary\" tabindex=\"-1\">5) Summary</h2>\n<ul>\n<li><strong>Yes, the single-cable topology can work</strong>—with caveats. On the <strong>Alpine Ridge TB3 dock (DP 1.2)</strong>, run the <strong>AW3821DW at 100 Hz</strong> and the new <strong>2560×1440 at 100 Hz</strong>, both at <strong>8-bpc RGB 4:4:4</strong>.</li>\n<li>The dock will not do <strong>3840×1600 @ 144 Hz</strong> concurrently with a high-refresh second display; DP 1.2 per-link bandwidth is the blocker.</li>\n<li>Because <strong>macOS lacks MST-Extended</strong>, we also need to ensure the dock provides a <strong>downstream Thunderbolt port</strong>.</li>\n</ul>\n<hr>\n<h2 id=\"references\" tabindex=\"-1\">References</h2>\n<p>[1] Apple — <em>MacBook Pro (14-inch, 2021) Tech Specs</em> (external display support) – <a href=\"https://support.apple.com/en-us/111902\">https://support.apple.com/en-us/111902</a><br>\n[2] Dell — <em>Alienware AW3821DW product page</em> (3840×1600 @ 144 Hz over DP) – <a href=\"https://www.dell.com/en-us/shop/alienware-38-curved-gaming-monitor-aw3821dw/apd/210-axvg/monitors-monitor-accessories\">https://www.dell.com/en-us/shop/alienware-38-curved-gaming-monitor-aw3821dw/apd/210-axvg/monitors-monitor-accessories</a><br>\n[3] RTINGS — <em>Chroma Subsampling: 4:4:4 vs 4:2:2 vs 4:2:0</em> – <a href=\"https://www.rtings.com/tv/learn/chroma-subsampling\">https://www.rtings.com/tv/learn/chroma-subsampling</a><br>\n[4] Eaton/Tripp Lite — <em>DisplayPort explained</em> (8b/10b vs 128b/132b; effective payloads) – <a href=\"https://www.eaton.com/us/en-us/products/backup-power-ups-surge-it-power-distribution/backup-power-ups-it-power-distribution-resources/cpdi-vertical-marketing/displayport-explained.html\">https://www.eaton.com/us/en-us/products/backup-power-ups-surge-it-power-distribution/backup-power-ups-it-power-distribution-resources/cpdi-vertical-marketing/displayport-explained.html</a><br>\n[5] Cable Matters — <em>DisplayPort 1.4 vs 1.2: What’s the Difference?</em> (21.6 vs 32.4 Gbps raw; 17.28 vs 25.92 Gbps payload) – <a href=\"https://www.cablematters.com/Blog/DisplayPort/displayport-1-4-vs-1-2\">https://www.cablematters.com/Blog/DisplayPort/displayport-1-4-vs-1-2</a><br>\n[6] Wikipedia — <em>DisplayPort</em> (TB3 Alpine Ridge DP 1.2 vs Titan Ridge DP 1.4) – <a href=\"https://en.wikipedia.org/wiki/DisplayPort\">https://en.wikipedia.org/wiki/DisplayPort</a><br>\n[7] StarTech — <em>TB3DOCK2DPPD Manual</em> (identifies <strong>JHL6540 Alpine Ridge</strong> chipset, DP 1.2 heritage) – <a href=\"https://sgcdn.startech.com/005329/media/sets/tb3dock2dppd_manual/tb3dock2dppd_tb3dock2dppu_thunderbolt_3_dock.pdf\">https://sgcdn.startech.com/005329/media/sets/tb3dock2dppd_manual/tb3dock2dppd_tb3dock2dppu_thunderbolt_3_dock.pdf</a><br>\n[8] StarTech — <em>TB3DOCK2DPPD Datasheet</em> (dual-4K60 via DP + downstream TB3) – <a href=\"https://media.startech.com/cms/pdfs/tb3dock2dppd_datasheet.pdf\">https://media.startech.com/cms/pdfs/tb3dock2dppd_datasheet.pdf</a><br>\n[9] Plugable KB — <em>MST on macOS (Extended not supported)</em> – <a href=\"https://kb.plugable.com/understanding-mst-multi-display-setups-with-windows-macos-and-displayport\">https://kb.plugable.com/understanding-mst-multi-display-setups-with-windows-macos-and-displayport</a><br>\n[10] Intel — <em>Thunderbolt 3 vs Thunderbolt 4</em> (TB4 mandates dual 4K or one 8K) – <a href=\"https://www.intel.com/content/www/us/en/architecture-and-technology/thunderbolt/thunderbolt-3-vs-4.html\">https://www.intel.com/content/www/us/en/architecture-and-technology/thunderbolt/thunderbolt-3-vs-4.html</a><br>\n[11] Wikipedia — <em>USB4</em> (DP 1.4a tunneling in USB4 v1; DP 2.1 in USB4 v2) – <a href=\"https://en.wikipedia.org/wiki/USB4\">https://en.wikipedia.org/wiki/USB4</a></p>\n<link rel=\"stylesheet\" href=\"//cdn.jsdelivr.net/npm/katex/dist/katex.min.css\">\n<link rel=\"stylesheet\" href=\"//cdn.jsdelivr.net/npm/markdown-it-texmath/css/texmath.min.css\">\n","thumbnailImageUrl":null,"excerpt":"","more":"<p>Recently I’ve been looking into giving my WFH setup some upgrade.<br>\nI already have a 38inch AW3821DW monitor running <strong>3800x1600 @ 144Hz</strong>, connected to the <strong>M1 Pro Macbook Pro</strong> via a <strong>TB3 dock</strong> (StarTech TB3DOCK2DPPD that I bought years ago pre-pandemic). I’m trying to add a side monitor to it with likely a QHD spec <strong>(2560x1440) @ 100Hz</strong>.</p>\n<div class=\"mermaid\">\n  graph TD;\n    M1[M1 Pro]-->TBDock[TB Dock]\n    TBDock-->AW3821DW[AW3821DW 3800x1600 @ 144Hz]\n    TBDock-->QHD[QHD 2560x1440 @ 100Hz]\n</div>\n\n<p>Then I suddenly realized one thing, if I want to keep my connection topology as is (wire up all devices to my dock and connect it to the Mac using a single TB cable) - would that work?</p>\n<p>It turns out it’s not that straightforward to average users. And here’s why.</p>\n<hr>\n<h2 id=\"1)-rgb-4%3A4%3A4-vs-ycbcr-4%3A2%3A2-chroma-subsampling\" tabindex=\"-1\">1) RGB 4:4:4 vs YCbCr 4:2:2 Chroma Subsampling</h2>\n<p>Before doing math, we first need to check the <em>signal</em> we want to drive:</p>\n<ul>\n<li><strong>RGB/YCbCr 4:4:4, 8-bit per channel (bpc)</strong> → <strong>24 bits/pixel</strong>. This is the “no-compromise” desktop signal—full chroma on every pixel. Text looks crisp.</li>\n<li><strong>YCbCr 4:2:2, 8-bpc</strong> → the chroma (Cb/Cr) is shared between two horizontal pixels, averaging <strong>16 bits/pixel</strong> (≈ 33% less data than 4:4:4). Saves bandwidth, but fine colored edges and UI text soften.</li>\n<li><strong>YCbCr 4:2:0, 8-bpc</strong> → <strong>12 bits/pixel</strong> (≈ 50% less than 4:4:4). Great for video, not for desktop use.</li>\n<li><strong>HDR / 10-bpc</strong> pushes each of those up by 25% (e.g., RGB 4:4:4 becomes <strong>30 bits/pixel</strong>).</li>\n</ul>\n<p>For SDR productivity monitors, we should <strong>ALWAYS</strong> prefer <strong>RGB 4:4:4</strong> (or YCbCr 4:4:4) at 8-bpc for clear text.<br>\nAnything doing chroma subsampling (422) is a no-go as text would have blurred/colored edges. Period.</p>\n<hr>\n<h2 id=\"2)-the-bandwidth-calculation\" tabindex=\"-1\">2) The bandwidth calculation</h2>\n<p>For uncompressed links (DisplayPort/HDMI without DSC), a solid back-of-the-envelope is:</p>\n<section><eqn><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mrow><mtext mathvariant=\"bold\">Signal</mtext><mtext> </mtext><mtext mathvariant=\"bold\">rate</mtext></mrow><mo>≈</mo><mtext>(Horizontal px)</mtext><mo>×</mo><mtext>(Vertical px)</mtext><mo>×</mo><mtext>(Refresh Hz)</mtext><mo>×</mo><mtext>(bits/pixel)</mtext><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">\n\\textbf{Signal rate} \\approx \\text{(Horizontal px)} \\times \\text{(Vertical px)} \\times \\text{(Refresh Hz)} \\times \\text{(bits/pixel)}.\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord text\"><span class=\"mord textbf\">Signal rate</span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">(Horizontal px)</span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">(Vertical px)</span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">(Refresh Hz)</span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">(bits/pixel)</span></span><span class=\"mord\">.</span></span></span></span></span></eqn></section><p>This yields the <strong>active video</strong> payload. Real links also carry blanking intervals and link-layer overhead. Using modern reduced-blanking timings, a <strong>5–10% headroom</strong> usually suffices for comparisons; standards bodies quote separate “link rates” that already include line coding overhead.</p>\n<p>Let’s compute the key cases (all at 8-bpc unless noted):</p>\n<ul>\n<li>\n<p><strong>One QHD (2560×1440) @ 100 Hz, 4:4:4</strong><br>\n<section><eqn><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mn>2560</mn><mo>×</mo><mn>1440</mn><mo>×</mo><mn>100</mn><mo>×</mo><mn>24</mn><mtext>  </mtext><mo>=</mo><mtext>  </mtext><mrow><mn mathvariant=\"bold\">8.85</mn><mtext> </mtext><mi mathvariant=\"bold\">G</mi><mi mathvariant=\"bold\">b</mi><mi mathvariant=\"bold\">p</mi><mi mathvariant=\"bold\">s</mi></mrow></mrow><annotation encoding=\"application/x-tex\">2560 \\times 1440 \\times 100 \\times 24 \\;=\\; \\mathbf{8.85\\ Gbps}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">2560</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">1440</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">100</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">24</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">8.85</span><span class=\"mspace\"> </span><span class=\"mord mathbf\">Gbps</span></span></span></span></span></span></eqn></section><br>\nWith 10-bpc (30 bpp): <strong>11.06 Gbps</strong>.<br>\nWith 4:2:2 (16 bpp): <strong>5.90 Gbps</strong>.</p>\n</li>\n<li>\n<p><strong>Your ultrawide (3840×1600), 4:4:4</strong></p>\n<ul>\n<li>@ <strong>100 Hz</strong><br>\n<section><eqn><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mn>3840</mn><mo>×</mo><mn>1600</mn><mo>×</mo><mn>100</mn><mo>×</mo><mn>24</mn><mo>=</mo><mrow><mn mathvariant=\"bold\">14.75</mn><mtext> </mtext><mi mathvariant=\"bold\">G</mi><mi mathvariant=\"bold\">b</mi><mi mathvariant=\"bold\">p</mi><mi mathvariant=\"bold\">s</mi></mrow></mrow><annotation encoding=\"application/x-tex\">3840 \\times 1600 \\times 100 \\times 24 = \\mathbf{14.75\\ Gbps} </annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">3840</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">1600</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">100</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">24</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">14.75</span><span class=\"mspace\"> </span><span class=\"mord mathbf\">Gbps</span></span></span></span></span></span></eqn></section></li>\n<li>@ <strong>120 Hz</strong> <strong>17.69 Gbps</strong> (already nudging past DP 1.2’s effective limit once overhead is considered).</li>\n<li>@ <strong>144 Hz</strong>: <strong>21.23 Gbps</strong> (firmly beyond DP 1.2; needs DP 1.4/HBR3 and/or DSC support end-to-end).</li>\n</ul>\n</li>\n<li>\n<p><strong>Two monitors together</strong>:</p>\n<ul>\n<li><strong>3840×1600 @ 100 Hz</strong> + <strong>2560×1440 @ 100 Hz</strong> (both 8-bpc 4:4:4) →<br>\n<section><eqn><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mn>14.75</mn><mo>+</mo><mn>8.85</mn><mo>≈</mo><mrow><mn mathvariant=\"bold\">23.6</mn><mtext> </mtext><mi mathvariant=\"bold\">G</mi><mi mathvariant=\"bold\">b</mi><mi mathvariant=\"bold\">p</mi><mi mathvariant=\"bold\">s</mi></mrow></mrow><annotation encoding=\"application/x-tex\">14.75 + 8.85 \\approx \\mathbf{23.6\\ Gbps}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">14.75</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">8.85</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">≈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">23.6</span><span class=\"mspace\"> </span><span class=\"mord mathbf\">Gbps</span></span></span></span></span></span></eqn></section><br>\nThis is in the same ballpark as <strong>dual 4K60 8-bpc</strong> (≈ 23.9 Gbps active), which many Thunderbolt 3 docks explicitly support.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Interpreting against link limits</strong></p>\n<ul>\n<li><strong>DisplayPort 1.2 (HBR2)</strong>: Max <strong>17.28 Gbps</strong> payload per DP link. One 2560×1440@100 (8-bpc 4:4:4) fits comfortably. One 3840×1600@100 fits. <strong>3840×1600@120 does not</strong>, and @144 certainly does not.</li>\n<li><strong>DisplayPort 1.4 (HBR3)</strong>: Max <strong>25.92 Gbps</strong> payload per DP link. Enough for 3840×1600@120/144 uncompressed in many timing modes, or with DSC when needed.</li>\n</ul>\n<hr>\n<h2 id=\"3)-dp-1.4-vs-dp-1.2%E2%80%94and-how-that-relates-to-thunderbolt-versions\" tabindex=\"-1\">3) DP 1.4 vs DP 1.2—and how that relates to Thunderbolt versions</h2>\n<ul>\n<li><strong>DP 1.2 (HBR2)</strong> tops out at 5.4Gbps/lane x 4 - <strong>21.6 Gbps raw / 17.28 Gbps effective</strong> payload.</li>\n<li><strong>DP 1.4 (HBR3)</strong> increases the lane rate to 8.1Gbps/lane -&gt; <strong>32.4 Gbps raw / 25.92 Gbps effective</strong> and adds features like <strong>DSC 1.2</strong> for higher-than-link-rate modes.</li>\n<li><strong>Thunderbolt 3</strong> originally shipped on <strong>Alpine Ridge</strong> controllers that <strong>tunnel DP 1.2</strong> only. Later <strong>Titan Ridge</strong> TB3 silicon can <strong>tunnel DP 1.4</strong>.</li>\n<li><strong>Thunderbolt 4</strong> is built on <strong>USB4 v1</strong>; it <strong>mandates</strong> support for <strong>two 4K displays or one 8K display</strong>, functionally aligning with <strong>DP 1.4/HBR3</strong> tunneling end-to-end. (TB4 does not raise the nominal 40 Gbps link rate; it raises the <em>minimums</em> OEMs must deliver.)</li>\n</ul>\n<p>However, the <strong>StarTech TB3DOCK2DPPD</strong> is an older <strong>Alpine Ridge (JHL6540)</strong> design—i.e., <strong>DP 1.2-only</strong> on its display paths. It still handles <strong>dual-display</strong> by presenting <strong>two DP sinks</strong> to the host TB controller (one on the rear DisplayPort jack, and a <strong>downstream TB3 port</strong>). But each DP hop is capped at <strong>DP 1.2</strong> timings.</p>\n<hr>\n<h2 id=\"4)-m1-pro-limitations-and-why-a-downstream-thunderbolt-port-matters\" tabindex=\"-1\">4) M1 Pro limitations and why a <strong>downstream Thunderbolt port</strong> matters</h2>\n<p>At this moment, it’s roughly clear that a TB3 dock is not going to be sufficient bandwidth wise.</p>\n<p>However there’s a bit more nuance here (and why we actually need a higher-end dock with a <strong>downstream</strong> TB4 port):</p>\n<ul>\n<li><strong>M1 Pro MacBook Pro (2021)</strong> supports <strong>up to two external displays</strong> over Thunderbolt (each up to 6K60). That’s a <em>hard</em> GPU/display-engine limit on count, not just raw bandwidth.</li>\n<li>MacOS does not support DisplayPort MST for extended desktops. In plain English: you can’t split one DP lane group into two independent desktops via an MST hub and expect macOS to see two displays—it will mirror them.<br>\n<strong>Implication:</strong> to run <strong>two</strong> external monitors over <strong>one cable</strong>, we need a <strong>Thunderbolt dock</strong> that exposes <strong>two independent DP tunnels</strong> (e.g., one physical DP output <strong>plus</strong> a <strong>downstream TB</strong> port for the second display via USB-C→DP).</li>\n</ul>\n<p>The <strong>TB3DOCK2DPPD</strong> does exactly that: one native DP port <strong>and</strong> one <strong>downstream TB3</strong> port intended to carry the second display. The constraint, again, is that both of those hops are <strong>DP 1.2</strong> only.</p>\n<p>A lot of the lower-end docks, on the other hand, <strong>do NOT carry downstream TB ports</strong> and hence not suited for such setup.</p>\n<p><strong>Putting it together:</strong></p>\n<ul>\n<li>To keep the single-cable topology, we should use the dock’s <strong>DP</strong> for one monitor and the <strong>downstream TB3→USB-C/DP</strong> for the other.</li>\n<li>We also need a <strong>DP 1.4-capable TB3/TB4 dock</strong> (Titan Ridge, Goshen Ridge/TB4) to fulfill the bandwidth requirements for the 2 monitors at high refresh rate.</li>\n</ul>\n<hr>\n<h2 id=\"5)-summary\" tabindex=\"-1\">5) Summary</h2>\n<ul>\n<li><strong>Yes, the single-cable topology can work</strong>—with caveats. On the <strong>Alpine Ridge TB3 dock (DP 1.2)</strong>, run the <strong>AW3821DW at 100 Hz</strong> and the new <strong>2560×1440 at 100 Hz</strong>, both at <strong>8-bpc RGB 4:4:4</strong>.</li>\n<li>The dock will not do <strong>3840×1600 @ 144 Hz</strong> concurrently with a high-refresh second display; DP 1.2 per-link bandwidth is the blocker.</li>\n<li>Because <strong>macOS lacks MST-Extended</strong>, we also need to ensure the dock provides a <strong>downstream Thunderbolt port</strong>.</li>\n</ul>\n<hr>\n<h2 id=\"references\" tabindex=\"-1\">References</h2>\n<p>[1] Apple — <em>MacBook Pro (14-inch, 2021) Tech Specs</em> (external display support) – <a href=\"https://support.apple.com/en-us/111902\">https://support.apple.com/en-us/111902</a><br>\n[2] Dell — <em>Alienware AW3821DW product page</em> (3840×1600 @ 144 Hz over DP) – <a href=\"https://www.dell.com/en-us/shop/alienware-38-curved-gaming-monitor-aw3821dw/apd/210-axvg/monitors-monitor-accessories\">https://www.dell.com/en-us/shop/alienware-38-curved-gaming-monitor-aw3821dw/apd/210-axvg/monitors-monitor-accessories</a><br>\n[3] RTINGS — <em>Chroma Subsampling: 4:4:4 vs 4:2:2 vs 4:2:0</em> – <a href=\"https://www.rtings.com/tv/learn/chroma-subsampling\">https://www.rtings.com/tv/learn/chroma-subsampling</a><br>\n[4] Eaton/Tripp Lite — <em>DisplayPort explained</em> (8b/10b vs 128b/132b; effective payloads) – <a href=\"https://www.eaton.com/us/en-us/products/backup-power-ups-surge-it-power-distribution/backup-power-ups-it-power-distribution-resources/cpdi-vertical-marketing/displayport-explained.html\">https://www.eaton.com/us/en-us/products/backup-power-ups-surge-it-power-distribution/backup-power-ups-it-power-distribution-resources/cpdi-vertical-marketing/displayport-explained.html</a><br>\n[5] Cable Matters — <em>DisplayPort 1.4 vs 1.2: What’s the Difference?</em> (21.6 vs 32.4 Gbps raw; 17.28 vs 25.92 Gbps payload) – <a href=\"https://www.cablematters.com/Blog/DisplayPort/displayport-1-4-vs-1-2\">https://www.cablematters.com/Blog/DisplayPort/displayport-1-4-vs-1-2</a><br>\n[6] Wikipedia — <em>DisplayPort</em> (TB3 Alpine Ridge DP 1.2 vs Titan Ridge DP 1.4) – <a href=\"https://en.wikipedia.org/wiki/DisplayPort\">https://en.wikipedia.org/wiki/DisplayPort</a><br>\n[7] StarTech — <em>TB3DOCK2DPPD Manual</em> (identifies <strong>JHL6540 Alpine Ridge</strong> chipset, DP 1.2 heritage) – <a href=\"https://sgcdn.startech.com/005329/media/sets/tb3dock2dppd_manual/tb3dock2dppd_tb3dock2dppu_thunderbolt_3_dock.pdf\">https://sgcdn.startech.com/005329/media/sets/tb3dock2dppd_manual/tb3dock2dppd_tb3dock2dppu_thunderbolt_3_dock.pdf</a><br>\n[8] StarTech — <em>TB3DOCK2DPPD Datasheet</em> (dual-4K60 via DP + downstream TB3) – <a href=\"https://media.startech.com/cms/pdfs/tb3dock2dppd_datasheet.pdf\">https://media.startech.com/cms/pdfs/tb3dock2dppd_datasheet.pdf</a><br>\n[9] Plugable KB — <em>MST on macOS (Extended not supported)</em> – <a href=\"https://kb.plugable.com/understanding-mst-multi-display-setups-with-windows-macos-and-displayport\">https://kb.plugable.com/understanding-mst-multi-display-setups-with-windows-macos-and-displayport</a><br>\n[10] Intel — <em>Thunderbolt 3 vs Thunderbolt 4</em> (TB4 mandates dual 4K or one 8K) – <a href=\"https://www.intel.com/content/www/us/en/architecture-and-technology/thunderbolt/thunderbolt-3-vs-4.html\">https://www.intel.com/content/www/us/en/architecture-and-technology/thunderbolt/thunderbolt-3-vs-4.html</a><br>\n[11] Wikipedia — <em>USB4</em> (DP 1.4a tunneling in USB4 v1; DP 2.1 in USB4 v2) – <a href=\"https://en.wikipedia.org/wiki/USB4\">https://en.wikipedia.org/wiki/USB4</a></p>\n<link rel=\"stylesheet\" href=\"//cdn.jsdelivr.net/npm/katex/dist/katex.min.css\">\n<link rel=\"stylesheet\" href=\"//cdn.jsdelivr.net/npm/markdown-it-texmath/css/texmath.min.css\">\n"}],"PostAsset":[{"_id":"source/_posts/Building-Linux-Workspace-on-Windows-10-via-WSL/WSL.png","slug":"WSL.png","post":"cmf1pfvs500038mmg08ibhuon","modified":0,"renderable":0},{"_id":"source/_posts/Building-Linux-Workspace-on-Windows-10-via-WSL/title.png","slug":"title.png","post":"cmf1pfvs500038mmg08ibhuon","modified":0,"renderable":0},{"_id":"source/_posts/Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul/RBK20.png","slug":"RBK20.png","post":"cmf1pfvs600078mmg825bdt5d","modified":0,"renderable":0},{"_id":"source/_posts/Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul/slowspeed.png","slug":"slowspeed.png","post":"cmf1pfvs600078mmg825bdt5d","modified":0,"renderable":0},{"_id":"source/_posts/Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul/speed.png","slug":"speed.png","post":"cmf1pfvs600078mmg825bdt5d","modified":0,"renderable":0},{"_id":"source/_posts/Make-Wireless-BackHaul-Great-Again-Disable-Orbi-2-4G-Backhaul/telnet.png","slug":"telnet.png","post":"cmf1pfvs600078mmg825bdt5d","modified":0,"renderable":0},{"_id":"source/_posts/It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO/mmap.png","slug":"mmap.png","post":"cmf1pfvs700088mmg6i3n26h6","modified":0,"renderable":0},{"_id":"source/_posts/It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO/non_zero_copy.png","slug":"non_zero_copy.png","post":"cmf1pfvs700088mmg6i3n26h6","modified":0,"renderable":0},{"_id":"source/_posts/It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO/scattergather.png","slug":"scattergather.png","post":"cmf1pfvs700088mmg6i3n26h6","modified":0,"renderable":0},{"_id":"source/_posts/It-s-all-about-buffers-zero-copy-mmap-and-Java-NIO/zero_copy.png","slug":"zero_copy.png","post":"cmf1pfvs700088mmg6i3n26h6","modified":0,"renderable":0},{"_id":"source/_posts/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray/iam.png","slug":"iam.png","post":"cmf1pfvs700098mmg3sccdwdw","modified":0,"renderable":0},{"_id":"source/_posts/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray/ms-failure.png","slug":"ms-failure.png","post":"cmf1pfvs700098mmg3sccdwdw","modified":0,"renderable":0},{"_id":"source/_posts/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray/tracing.png","slug":"tracing.png","post":"cmf1pfvs700098mmg3sccdwdw","modified":0,"renderable":0},{"_id":"source/_posts/Monitor-gRPC-Microservices-in-Kubernetes-with-Amazon-X-Ray/xray.png","slug":"xray.png","post":"cmf1pfvs700098mmg3sccdwdw","modified":0,"renderable":0},{"_id":"source/_posts/PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-1-DHCP-Setup/net-topo.png","slug":"net-topo.png","post":"cmf1pfvse001a8mmg6z7y8nyy","modified":0,"renderable":0},{"_id":"source/_posts/PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-2-Config-TFTP-and-NFS-mounts/nfs-permission.png","slug":"nfs-permission.png","post":"cmf1pfvse001b8mmg961s6fw4","modified":0,"renderable":0},{"_id":"source/_posts/PXE-Boot-Diskless-Raspberry-Pi-4-With-Ubuntu-Ubiquiti-and-Synology-2-Config-TFTP-and-NFS-mounts/rainbow.jpg","slug":"rainbow.jpg","post":"cmf1pfvse001b8mmg961s6fw4","modified":0,"renderable":0},{"_id":"source/_posts/Why-you-should-ditch-browserify-and-commonjs-in-the-http-2-world/comparison.gif","slug":"comparison.gif","post":"cmf1pfvse001d8mmgfwy7006u","modified":0,"renderable":0},{"_id":"source/_drafts/Implementing-Regex-from-Scratch-1-The-Basics-of-Regex-and-Finite-Automata/example2.png","slug":"-Automata/example2.png","post":"cmf1pfvs300018mmg9u4qb1pk","modified":0,"renderable":0},{"_id":"source/_drafts/Implementing-Regex-from-Scratch-1-The-Basics-of-Regex-and-Finite-Automata/sm.png","slug":"-Automata/sm.png","post":"cmf1pfvs300018mmg9u4qb1pk","modified":0,"renderable":0}],"PostCategory":[{"post_id":"cmf1pfvs300018mmg9u4qb1pk","category_id":"cmf1pfvs600048mmghto64uv5","_id":"cmf1pfvs8000d8mmg03gtbaru"},{"post_id":"cmf1pfvs700088mmg6i3n26h6","category_id":"cmf1pfvs8000b8mmghjro4mrq","_id":"cmf1pfvs9000h8mmg1r5ifyyv"},{"post_id":"cmf1pfvs700098mmg3sccdwdw","category_id":"cmf1pfvs8000f8mmg61901fo3","_id":"cmf1pfvs9000o8mmg4ng57lm4"},{"post_id":"cmf1pfvs700098mmg3sccdwdw","category_id":"cmf1pfvs9000i8mmg1fvk39uv","_id":"cmf1pfvs9000r8mmggbi63eiu"},{"post_id":"cmf1pfvse001d8mmgfwy7006u","category_id":"cmf1pfvs600048mmghto64uv5","_id":"cmf1pfvsf001i8mmg33iu92j7"},{"post_id":"cmf1pfvsf001h8mmg7t9s0r0a","category_id":"cmf1pfvsg001l8mmgd7u0emmh","_id":"cmf1pfvsh001v8mmgdk1i1pci"},{"post_id":"cmf1pfvsg001j8mmgfnkf35hi","category_id":"cmf1pfvsg001l8mmgd7u0emmh","_id":"cmf1pfvsh001z8mmggtqubhp3"},{"post_id":"cmf1pfvsg001m8mmg9iab8lbd","category_id":"cmf1pfvsh001u8mmgcsa0clxa","_id":"cmf1pfvsh00228mmggqm62thq"}],"PostTag":[{"post_id":"cmf1pfvs300018mmg9u4qb1pk","tag_id":"cmf1pfvs600058mmg7p576ks7","_id":"cmf1pfvs8000c8mmgex4c6ewo"},{"post_id":"cmf1pfvs500038mmg08ibhuon","tag_id":"cmf1pfvs8000a8mmgan65682x","_id":"cmf1pfvs9000k8mmg5hxo3lyi"},{"post_id":"cmf1pfvs500038mmg08ibhuon","tag_id":"cmf1pfvs8000e8mmg78qwdiig","_id":"cmf1pfvs9000l8mmghmdzeb08"},{"post_id":"cmf1pfvs500038mmg08ibhuon","tag_id":"cmf1pfvs9000g8mmg8ncnct6p","_id":"cmf1pfvs9000n8mmgee3l9i8p"},{"post_id":"cmf1pfvs600078mmg825bdt5d","tag_id":"cmf1pfvs9000j8mmg7go679ho","_id":"cmf1pfvs9000p8mmg4wc70fyr"},{"post_id":"cmf1pfvs700088mmg6i3n26h6","tag_id":"cmf1pfvs9000m8mmg0rpt173d","_id":"cmf1pfvsa000v8mmgdf3d454g"},{"post_id":"cmf1pfvs700088mmg6i3n26h6","tag_id":"cmf1pfvs9000q8mmgdiirde01","_id":"cmf1pfvsa000w8mmg28fs5yub"},{"post_id":"cmf1pfvs700088mmg6i3n26h6","tag_id":"cmf1pfvs9000s8mmgfx2q7a1t","_id":"cmf1pfvsa000y8mmgh6mwbopq"},{"post_id":"cmf1pfvs700088mmg6i3n26h6","tag_id":"cmf1pfvs9000t8mmggk0mat9z","_id":"cmf1pfvsa000z8mmg20p6hoy5"},{"post_id":"cmf1pfvs700098mmg3sccdwdw","tag_id":"cmf1pfvsa000u8mmg5po55ar1","_id":"cmf1pfvsa00148mmge5qt9rf7"},{"post_id":"cmf1pfvs700098mmg3sccdwdw","tag_id":"cmf1pfvsa000x8mmg9cjba9tk","_id":"cmf1pfvsa00158mmgbxj802qf"},{"post_id":"cmf1pfvs700098mmg3sccdwdw","tag_id":"cmf1pfvsa00108mmg835icnnq","_id":"cmf1pfvsa00168mmgbdk76zec"},{"post_id":"cmf1pfvs700098mmg3sccdwdw","tag_id":"cmf1pfvsa00118mmg3i983j18","_id":"cmf1pfvsa00178mmghkgsbmkl"},{"post_id":"cmf1pfvs700098mmg3sccdwdw","tag_id":"cmf1pfvsa00128mmg1otz87by","_id":"cmf1pfvsa00188mmga5ov8mza"},{"post_id":"cmf1pfvs700098mmg3sccdwdw","tag_id":"cmf1pfvsa00138mmg8mbn5fjd","_id":"cmf1pfvsa00198mmg3eq4hp4x"},{"post_id":"cmf1pfvse001a8mmg6z7y8nyy","tag_id":"cmf1pfvs8000a8mmgan65682x","_id":"cmf1pfvsg001n8mmg3jlkh7cr"},{"post_id":"cmf1pfvse001a8mmg6z7y8nyy","tag_id":"cmf1pfvse001c8mmg6arhbqdo","_id":"cmf1pfvsg001p8mmg2pq174ku"},{"post_id":"cmf1pfvse001a8mmg6z7y8nyy","tag_id":"cmf1pfvsf001g8mmgajyrhv49","_id":"cmf1pfvsh001s8mmg0x9b6kj2"},{"post_id":"cmf1pfvse001b8mmg961s6fw4","tag_id":"cmf1pfvs8000a8mmgan65682x","_id":"cmf1pfvsh001w8mmg0j8a5tgq"},{"post_id":"cmf1pfvse001b8mmg961s6fw4","tag_id":"cmf1pfvse001c8mmg6arhbqdo","_id":"cmf1pfvsh001x8mmghgwr3eox"},{"post_id":"cmf1pfvse001b8mmg961s6fw4","tag_id":"cmf1pfvsf001g8mmgajyrhv49","_id":"cmf1pfvsh00208mmg74fh3w82"},{"post_id":"cmf1pfvse001d8mmgfwy7006u","tag_id":"cmf1pfvsh001t8mmg5uo40lth","_id":"cmf1pfvsh00278mmg54zmcm9b"},{"post_id":"cmf1pfvse001d8mmgfwy7006u","tag_id":"cmf1pfvsh001y8mmg6u75895q","_id":"cmf1pfvsi00288mmg4cz7gwnt"},{"post_id":"cmf1pfvse001d8mmgfwy7006u","tag_id":"cmf1pfvsh00218mmg0qkg5553","_id":"cmf1pfvsi002a8mmgb2msetzj"},{"post_id":"cmf1pfvse001d8mmgfwy7006u","tag_id":"cmf1pfvsh00238mmg0t7bdgqq","_id":"cmf1pfvsi002b8mmgfp4zcom7"},{"post_id":"cmf1pfvse001d8mmgfwy7006u","tag_id":"cmf1pfvsh00248mmg280rc5q0","_id":"cmf1pfvsi002d8mmg2n7l83fv"},{"post_id":"cmf1pfvse001d8mmgfwy7006u","tag_id":"cmf1pfvsh00258mmg76vz34kq","_id":"cmf1pfvsi002e8mmgh6apg515"},{"post_id":"cmf1pfvsf001e8mmg0o1a8t9c","tag_id":"cmf1pfvsh00268mmg551i6i0b","_id":"cmf1pfvsi002g8mmghvcqb0i9"},{"post_id":"cmf1pfvsf001f8mmgeb9ad2mk","tag_id":"cmf1pfvsi00298mmggcykeiqw","_id":"cmf1pfvsi002k8mmg3dnc6zh3"},{"post_id":"cmf1pfvsf001f8mmgeb9ad2mk","tag_id":"cmf1pfvsi002c8mmg6qwlhc1t","_id":"cmf1pfvsi002l8mmg74vtgj5x"},{"post_id":"cmf1pfvsf001f8mmgeb9ad2mk","tag_id":"cmf1pfvsa00128mmg1otz87by","_id":"cmf1pfvsi002n8mmgdanq885t"},{"post_id":"cmf1pfvsf001f8mmgeb9ad2mk","tag_id":"cmf1pfvsi002f8mmg9znf4okq","_id":"cmf1pfvsi002o8mmg2et5ass1"},{"post_id":"cmf1pfvsf001f8mmgeb9ad2mk","tag_id":"cmf1pfvsi002h8mmgafkyektq","_id":"cmf1pfvsi002q8mmg4prg3gve"},{"post_id":"cmf1pfvsf001f8mmgeb9ad2mk","tag_id":"cmf1pfvsi002i8mmggr8l53cs","_id":"cmf1pfvsi002r8mmgbpou7a6h"},{"post_id":"cmf1pfvsf001h8mmg7t9s0r0a","tag_id":"cmf1pfvsh00218mmg0qkg5553","_id":"cmf1pfvsj002t8mmg24tdbk2k"},{"post_id":"cmf1pfvsf001h8mmg7t9s0r0a","tag_id":"cmf1pfvsi002m8mmgca9k24u2","_id":"cmf1pfvsj002u8mmg6t6nes3i"},{"post_id":"cmf1pfvsg001j8mmgfnkf35hi","tag_id":"cmf1pfvsh00218mmg0qkg5553","_id":"cmf1pfvsj002w8mmggyt50z3u"},{"post_id":"cmf1pfvsg001m8mmg9iab8lbd","tag_id":"cmf1pfvsi002s8mmg3lr250kc","_id":"cmf1pfvsj002x8mmg8l0w4yk4"},{"post_id":"cmf1pfvsg001m8mmg9iab8lbd","tag_id":"cmf1pfvsj002v8mmg9h9a0976","_id":"cmf1pfvsj002y8mmgcxwsf1gd"},{"post_id":"cmf1uau9v00009lmgb49h8mzu","tag_id":"cmf1sksks000002mg4fs54nqf","_id":"cmf1uau9v00019lmga3h4hzlr"},{"post_id":"cmf1uau9v00009lmgb49h8mzu","tag_id":"cmf1smytk000802mg79tt0fju","_id":"cmf1uau9v00029lmg1jx363g1"}],"Tag":[{"name":"angular","_id":"cmf1pfvs600058mmg7p576ks7"},{"name":"linux","_id":"cmf1pfvs8000a8mmgan65682x"},{"name":"windows","_id":"cmf1pfvs8000e8mmg78qwdiig"},{"name":"wsl","_id":"cmf1pfvs9000g8mmg8ncnct6p"},{"name":"wifi, openwrt","_id":"cmf1pfvs9000j8mmg7go679ho"},{"name":"io","_id":"cmf1pfvs9000m8mmg0rpt173d"},{"name":"os","_id":"cmf1pfvs9000q8mmgdiirde01"},{"name":"java","_id":"cmf1pfvs9000s8mmgfx2q7a1t"},{"name":"unix","_id":"cmf1pfvs9000t8mmggk0mat9z"},{"name":"monitoring","_id":"cmf1pfvsa000u8mmg5po55ar1"},{"name":"grpc","_id":"cmf1pfvsa000x8mmg9cjba9tk"},{"name":"microservice","_id":"cmf1pfvsa00108mmg835icnnq"},{"name":"kubernetes","_id":"cmf1pfvsa00118mmg3i983j18"},{"name":"aws","_id":"cmf1pfvsa00128mmg1otz87by"},{"name":"xray","_id":"cmf1pfvsa00138mmg8mbn5fjd"},{"name":"raspberry","_id":"cmf1pfvse001c8mmg6arhbqdo"},{"name":"network","_id":"cmf1pfvsf001g8mmgajyrhv49"},{"name":"http2","_id":"cmf1pfvsh001t8mmg5uo40lth"},{"name":"browserify","_id":"cmf1pfvsh001y8mmg6u75895q"},{"name":"javascript","_id":"cmf1pfvsh00218mmg0qkg5553"},{"name":"dependency-management","_id":"cmf1pfvsh00238mmg0t7bdgqq"},{"name":"modular-design","_id":"cmf1pfvsh00248mmg280rc5q0"},{"name":"es6","_id":"cmf1pfvsh00258mmg76vz34kq"},{"name":"software engineering","_id":"cmf1pfvsh00268mmg551i6i0b"},{"name":"opensource","_id":"cmf1pfvsi00298mmggcykeiqw"},{"name":"workflow","_id":"cmf1pfvsi002c8mmg6qwlhc1t"},{"name":"airflow","_id":"cmf1pfvsi002f8mmg9znf4okq"},{"name":"azkaban","_id":"cmf1pfvsi002h8mmgafkyektq"},{"name":"review","_id":"cmf1pfvsi002i8mmggr8l53cs"},{"name":"frontend","_id":"cmf1pfvsi002m8mmgca9k24u2"},{"name":"blog","_id":"cmf1pfvsi002s8mmg3lr250kc"},{"name":"hexo","_id":"cmf1pfvsj002v8mmg9h9a0976"},{"name":"pcparts","_id":"cmf1sksks000002mg4fs54nqf"},{"name":"pcparts,","_id":"cmf1skvmm000202mg730605ae"},{"name":"pcparts,setup","_id":"cmf1sl1ok000602mg6e6g20co"},{"name":"setup","_id":"cmf1smytk000802mg79tt0fju"}]}}